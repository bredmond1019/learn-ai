[00:03] [Music]

[00:14] Everybody excited? So, uh, what does [00:16] Kalin do? We build stuff for people. So, [00:18] people come to us with ideas and they're [00:20] like, I want to make an app or like, oh, [00:21] I want to move off of Oracle onto [00:23] Postgress, you know. And we just do that [00:24] stuff. We are builders. We uh created a [00:28] company by hiring a bunch of passionate [00:30] autodidacts with a little bit of product [00:31] ADHD. And we jump around to all these [00:34] different things and build cool things [00:35] for our customers. And we have hundreds [00:37] of customers at any given time. Everyone [00:38] from like the Fortune 500 to startups. [00:41] And it's a very fun gig. It's really [00:44] cool. You get exposed to a lot of [00:46] technology. And what we've learned is [00:48] that uh generative AI is not the the [00:52] magical pill that solves everything that [00:54] a lot of people seem to think it is. Uh [00:56] and then what your CTO read in the Wall [00:59] Street Journal is not necessarily the [01:00] latest and greatest thing. And we'll [01:02] share some concrete components of that. [01:04] Uh but I'll just point out a couple of [01:06] different customers here. One of the [01:07] ones is Brainbox AI. So they are a uh [01:12] building operating system. They help [01:15] decarbonize the built environment. So [01:17] they manage uh tens of thousands of [01:19] buildings across the United States and [01:21] Canada or North America and they manage [01:23] the HVAC systems. And we built an agent [01:26] for them for helping uh with that uh [01:30] decarbonization of the built environment [01:32] and managing those things. And that was [01:34] uh I think in the Times 100 best [01:37] inventions of the year or something [01:38] because it helps drastically reduce [01:40] greenhouse emissions. Uh and then [01:41] Simmons is uh water management [01:43] conservation which we also implemented [01:45] with AI. Uh and with that, you know, [01:47] there's a couple other customers here. [01:49] Pipes AI, virtual moving technologies, [01:51] Z5 inventory. Uh but I thought it'd be [01:53] cool to just show a demo. And one of the [01:55] things that I'm most interested in right [01:57] now is multimodal search and uh semantic [02:00] understanding of videos. So this is one [02:03] of our customers, Nature Footage. They [02:05] have uh a ton of stock footage of, you [02:07] know, lions and tigers and bears. Oh my. [02:10] and crocodiles I suppose and we needed [02:12] to index all of that and make it [02:14] searchable over uh not just a vector [02:16] index but also like a caption. So we [02:19] leverage the Nova Pro models to generate [02:22] understandings and timestamps and [02:24] features of these videos store all of [02:26] those in elastic search and then we are [02:28] able to search on them and one of the [02:30] most important things there is that we [02:31] were able to build uh a pooling [02:33] embedding. So by taking frame samples [02:36] and pulling the embeddings uh of those [02:38] frames, we can do a multimodal embedding [02:40] and search with text for the images. And [02:42] that's provided through the Titan v2 [02:44] multimodal embeddings. [02:46] So uh I thought we'd take a look at a [02:49] different architecture. [02:51] I hope no one here is from Michigan [02:53] because that's a terrible team. I hate [02:54] them. Anyway, anyone remember March [02:56] Madness? So, this is another customer of [02:59] ours that uh I'm not going to reveal [03:01] their name, but essentially we have a [03:02] ton of sports footage that we're [03:04] processing both in real time and in [03:05] batch, archival and in real time. And [03:08] what we'll do is we'll split that data [03:09] into the audio. We'll generate the [03:11] transcription. Fun fact, if you're [03:12] looking for highlights, the easiest [03:14] thing to do is just ffmpeg get an [03:15] amplitude spectrograph of the audio and [03:17] look for the audience cheering and lo [03:19] and behold, you have your highlight [03:20] reel. Um, very simple hack right there. [03:23] And we'll take that and we'll generate [03:24] embeddings from both the text and from [03:26] the video itself. And we'll be able to [03:29] uh identify certain behaviors with a [03:31] certain vector and a certain confidence. [03:33] And we'll store those then into a [03:36] database. Oh, I think I paused the video [03:38] by accident. My apologies. No, I didn't. [03:42] And then we'll use something like Amazon [03:44] end user messaging or SNS or whatever. [03:46] we'll send a push notification to our [03:48] end users and say, "Look, we found uh a [03:50] three-pointer or uh we found this other [03:53] thing." And what we found is um you [03:55] don't even have to take the the raw [03:57] video. A a tiny little bit of annotation [04:00] can do wonders um for the video [04:03] understanding models at the as they [04:05] exist right now. The soda models still [04:07] just with a little tiny bit of uh [04:09] augmentation on the video will [04:11] outperform um what you can get with an [04:14] unmodified video. And what I mean by [04:16] that is if you have static camera angles [04:19] and you annotate on the court where the [04:21] three-pointer line is with a big blue [04:22] line and then you just ask the model [04:24] questions like did the player cross the [04:26] big blue line. Lo and behold you get way [04:28] better results and it takes you know [04:29] seconds and you can even have something [04:31] like SAM 2 which is another model from [04:32] meta go and do some of those annotations [04:34] for you. So that's an architecture. [04:37] You'll notice that I've put up a couple [04:39] of different databases there. We had uh [04:41] Postgress PG vector uh which is my [04:44] favorite right now. We had open search. [04:46] That's another implementation of vector [04:47] search there. Um, but anyway, why should [04:50] you listen to me? Hi, I'm Randall. Um, I [04:54] got started out hacking and building [04:56] stuff and uh playing video games and [04:58] hacking into video games. It turns out [05:00] that's super illegal. Did not know that. [05:02] Um, and then I went on to do some [05:04] physics stuff at NASA. Uh, I joined a [05:06] small company called Tenen, which became [05:08] MongoDB. They IPOed. Um, I was an idiot [05:11] and sold all my stock before the IPO. [05:13] Uh, and then I worked at SpaceX where I [05:15] led the CI/CD team. Fun fact, we never [05:17] blew up a rocket while I was in charge [05:19] of that team. Before and after my [05:20] tenure, we blew up rockets. Um, I I [05:24] don't know what else I can say there. [05:25] Uh, and then I spent a long time at AWS [05:27] and I had a great time building a ton of [05:28] technology for a lot of customers. I [05:30] even made a video about the transformer [05:32] paper in July of 2017, not realizing [05:36] what it was going to lead to. And the [05:39] fact that we're all even here today is [05:40] is still attention is all you need. Uh [05:43] you can follow me on Twitter at JR Hunt. [05:45] Uh it's still called Twitter. It will [05:46] never be called X in my mind. And uh [05:48] this is Kalin. You know, we've won [05:50] partner of the year for AWS for a long [05:52] time. We build stuff. Like I said, I I I [05:54] like to say our motto is we build cool [05:56] stuff. Um marketing doesn't like it when [05:58] I say that. Uh because I don't always [06:00] say the word stuff. Sometimes I'll sub [06:01] in a different word. And what we build, [06:04] you know, everything from chat bots to [06:05] co-pilots to AI agents. And I'm going to [06:07] share all the lessons that we've learned [06:09] from building all of these things. You [06:11] know, this sort of stuff on the top [06:13] here, these self-service productivity [06:15] tools. Um, these are things that you can [06:18] typically buy. Uh, but certain [06:20] institutions may need a fine tune. They [06:22] may need a a particular application on [06:24] top of that self-service productivity [06:26] tool and we will often build things for [06:28] them. Uh, one of the issues that we see [06:30] organizations facing is how do they [06:32] administer and track the usage of these [06:35] third-party tools and APIs. Uh, and some [06:38] people have an on-prem network and a VPN [06:39] where they can just measure all the [06:40] traffic. They can intercept things. They [06:42] can look for PII or PHI and they can do [06:44] all the fun stuff that we're supposed to [06:45] do with network interception. There's a [06:47] great tool called Shure Path. Uh, we use [06:49] it at Kalin. I recommend them. Uh, it [06:51] does all of that for you and it can [06:52] integrate with Zcal or whatever else you [06:54] might need. Um in terms of automating [06:56] business functions, you know, this is [06:59] typically trying to get a a percentage [07:02] of time or dollars back uh end to end in [07:05] a particular business process. Uh we [07:08] work with a large logistics management [07:09] customer that does a tremendous amount [07:11] of processing of uh of receipts and [07:14] bills of laden and things like that. And [07:16] this is a typical intelligent document [07:18] processing use case leveraging [07:20] generative AI and a custom classifier [07:22] before we send it into the generative AI [07:24] models. Uh we can get far faster better [07:26] results than even their human annotators [07:28] can. Um and then there's monetization [07:31] which is adding a new skew to an [07:33] existing product. It's an existing SAS [07:35] platform. It's an existing utility and [07:37] the customer is like oh I want to add a [07:40] new skew so I can charge my users for [07:42] fancy AI because the Wall Street Journal [07:43] told me to. And that is a very fun area [07:47] to work in. But if you just build a [07:49] chatbot, you know, sayanara, like good [07:52] luck. I'll, you know, you're the [07:54] Polaroid. Um, do people still use [07:56] Polaroid? Are they doing okay? I don't [07:58] know. Anyway, I used to say Kodak. Um, [08:01] this is how we build these things and [08:02] these are the lessons that we've [08:03] learned. Um, I stole this slide. This is [08:05] not my slide. I cannot remember where it [08:08] is from. It's from Twitter somewhere. It [08:10] might have been Jason Louu. It might [08:11] have been from DSPY. But this is a great [08:13] slide that I think very strategically [08:15] identifies what the uh specifications [08:19] are to build a moat in your business and [08:22] the inputs to your system and what your [08:25] system is going to do with them. That is [08:27] the most fundamental part your inputs [08:29] and your outputs. Um, does everyone [08:31] remember Steve Balmer, uh, the former [08:34] CEO of Microsoft and how he, uh, [08:36] famously went on stage, uh, on a [08:37] tremendous amount of cocaine and just [08:39] started screaming, um, developers, [08:41] developers, developers, developers. If I [08:43] were to channel my inner balmer, what I [08:44] would say is eval. [08:47] So when we do this eval layer, this is [08:50] where we prove that the system is robust [08:54] and not just a vibe check and we're [08:56] getting a one-off on a particularly [08:58] unique uh prompt. Then we have the [09:01] system architecture and then we have the [09:03] different LLMs and tools and things we [09:05] may use. And these are all incidental to [09:07] your AI system and you should expect [09:08] them to evolve and change. What will not [09:11] evolve and change is your fundamental [09:13] definition and specification of what are [09:15] your inputs and what are your outputs. [09:17] Uh and as you know the models get better [09:20] and they improve and you can get other [09:21] like modalities of output that may [09:23] evolve. But you're always going to [09:25] figure out why am I doing this? What is [09:27] my ROI? What do I expect? [09:30] This is how we build these things in [09:32] AWS. On the bottom layer we have two [09:34] services. We have Bedrock and we have [09:35] SageMaker. Uh these are uh useful [09:39] services. SageMaker comes at a [09:41] particular compute premium. You can also [09:43] just run on EKS or EC2 if you want. Um [09:46] there's two different pieces of custom [09:47] silicon that exist within AWS. One is [09:50] trrenium, one is inferentia. Uh these [09:52] come at about a 60% price performance [09:54] improvement over using Nvidia GPUs. Now [09:56] the downside is the amount of HP RAM is [09:59] not as big as like an H200. I don't know [10:01] if anyone saw today, but it was great [10:02] news. Amazon announced that they were [10:04] reducing the prices of the P4 and P5 [10:06] instances by up to 40%. So we all get [10:08] more GPUs cheaper. Very happy about [10:10] that. Um the interesting thing with [10:13] tranium and inferentia is that you must [10:15] uh use something called the neuron SDK [10:18] to write these. So if anyone has ever [10:20] written XLA for like TensorFlow and the [10:22] good old um what were they called the [10:24] TPUs and now the new TPU7 and all that [10:26] great stuff. Uh the the neuron kernel [10:28] interface for tranium and infinia is [10:30] very similar. One level up from that we [10:32] get to pick our various models. So we [10:34] have everything from uh claude and nova [10:36] to llama and deepseeek uh and then open [10:38] source models that we can deploy. I [10:40] don't know if mistrol is ever going to [10:41] release another open source model but [10:42] who knows. Uh and then we have our [10:44] embeddings and our vector stores. So [10:46] like I said uh I do prefer Postgress [10:49] right now. If you need um persistence in [10:52] Reddus uh there's a great thing called [10:54] memory DB on AWS that also supports [10:56] vector search. Um the good news about [10:58] the reddis vector search is that it is [11:00] extremely fast. The bad news is that it [11:02] is extremely expensive because it has to [11:04] sit in RAM. Um so if you think about how [11:07] you're going to construct your indexes [11:08] and like do IVV flat or something uh be [11:10] prepared to blow up your RAM in order to [11:12] store all of that stuff. Now um within [11:14] Postgress and open search you can go to [11:16] disk and you can use things like HNSW [11:18] indexes so that you can have uh a better [11:20] allocation and search mechanism. Then we [11:23] have the prompt versioning and prompt [11:24] management. Uh all of these things are [11:26] incidental and and kind of uh you you [11:29] know not unique anymore. But this one [11:31] context management is incredibly [11:34] important. And if you are looking to [11:37] differentiate your application from [11:39] someone else's application, context is [11:41] key. So if your competitor doesn't have [11:44] the context of the user and additional [11:47] information uh but you're able to inject [11:49] oh the the user is on this page they [11:51] have a history of this browsing you know [11:53] these are the cookies that I saw this is [11:55] you know then you can go and make a much [11:57] more strategic inference on behalf of [11:59] that end user. So here are the lessons [12:02] that we learned and I I'll jump into [12:03] these but I'm also going to run out of [12:05] time so I I'll speed through a little [12:06] bit of it and I'll make the stack [12:07] available for folks. But uh it turns out [12:10] eval and embeddings are not all you [12:12] need. Uh [12:14] you know the understanding the access [12:17] patterns and understanding the way that [12:18] people will use the product uh will lead [12:20] to a much better result than just [12:22] throwing out evals and throwing out [12:23] embeddings and wishing the best of luck. [12:25] Embeddings alone do not a great query [12:27] system make. How do you do faceted [12:29] search and filters on top of embeddings [12:31] alone? That is why we love things like [12:33] open search and postgress. Um speed [12:36] matters. So if your inference is slow, [12:41] uh UX is a means of mitigating the [12:44] slowness of some of these things. [12:46] There's other techniques you can use. [12:47] You can use caching, you can use other [12:49] components. Um but if you are slower and [12:52] more expensive, you will not be used. If [12:54] you are uh slower and cheaper and you're [12:57] mitigating some of the effects by [12:58] leveraging something like a fancy UI [13:01] spinner or something that keeps your [13:02] users entertained as the inference is [13:04] being calculated uh you can uh still [13:06] win. Now uh knowing your end customer as [13:09] I said is very important. And then the [13:11] other very important thing is the number [13:13] of times I see people defining a tool [13:16] called get current date is infuriating [13:19] to me. Like it is literally like import [13:22] time.now [13:24] you know like just it's a format string [13:26] just throw it in the string like you [13:28] control the prompt. Um, so, uh, the [13:33] downside of putting some of that [13:35] information very high up in the prompt [13:36] is that your caching, uh, is not as [13:38] effective. But if you can put some of [13:40] that information at the bottom of the [13:41] prompt after the instructions, you can [13:43] often, uh, get very effective caching. [13:45] Um, then there is like I I used to say [13:51] we should fine-tune, we should do these [13:52] things. Uh, it turns out I was wrong. As [13:54] the models have improved and gotten more [13:56] and more powerful, uh, prompt [13:58] engineering has proven unreasonably [14:00] effective for us, like far more [14:02] effective than I would have predicted. [14:03] Within, uh, cloud 3.7 to claude 4, we [14:06] saw zero regressions. From cloud 35 to [14:08] 37, we did see regressions on certain [14:11] things when we moved the exact same [14:12] prompts over to some of our, uh, users [14:15] and some of our evals. But from 37 to [14:18] four, we got faster, better, cheaper, [14:21] more optimized inference in virtually [14:23] every use case. So it was like a drop in [14:25] replacement and it was amazing. Um, and [14:27] I hoping future versions will be the [14:29] same. Uh, I'm hoping we're the era of [14:31] having to adjust your prompt every time [14:33] a new model comes out is ending. Um, and [14:36] then finally, it's very important to [14:37] know your economics like is this [14:39] inference going to bankrupt my company? [14:41] Um if you think about some of the cost [14:43] of uh uh the the opus models, you know, [14:47] it may not always be the best thing to [14:49] run. [14:50] Okay, so just in the interest of time, [14:53] this is another great slide. This is uh [14:55] from anthropic actually. And when we [14:58] think about how to create our evals, the [15:00] vibe check, the very first thing that [15:01] you do when you try to create um a uh [15:08] a test, that vibe check becomes your [15:10] first eval. And then you change the data [15:12] and the stuff that you're sending in and [15:14] lo and behold, 20 minutes later, you do [15:16] have some form of eval set that you can [15:18] begin running. And then you can go for [15:20] metrics. Now metrics do not have to be a [15:22] score like a BERT or [15:25] you know a benchmark score that is [15:26] calculated. They can just be a boolean. [15:29] It can just be true or false. Was this [15:31] inference successful or not? Um that is [15:33] often easier than trying to assign a [15:35] particular value and a particular score. [15:37] Uh and then you just iterate, you know, [15:39] keep going. And like I said, speed [15:41] matters, but UX matters more. you know, [15:43] this UX orchestration, prop management, [15:46] all of this great stuff uh is why we end [15:49] up doing better than uh some of our [15:51] competitors. And then, you know, one of [15:53] our customers, Cloud Zero, uh we [15:56] originally built a chatbot for them for [15:57] you to chat with your AWS infrastructure [15:59] and get cost out of that AWS [16:01] infrastructure. Um we are now using [16:03] generative UI in order to render uh the [16:06] information that is shown in those [16:08] charts. So in just in time we will craft [16:11] a react component and inject it into the [16:14] uh the rendering of the response and [16:17] then we can cache those uh components [16:20] and describe in the prompt hey I made [16:23] this for this other user and maybe it's [16:25] helpful one day uh for some other user's [16:28] query. And so this generative UI allows [16:30] the tool to constantly evolve and [16:31] personalize to the individual end user. [16:33] Um, this is an extremely powerful [16:35] paradigm that is finally fast enough [16:37] with some of these uh models and their [16:39] lightning fast inference speed. Um, [16:42] nature footage, we covered that earlier. [16:44] Uh there's also knowing your end user [16:46] which is we had a customer uh that had [16:50] users in remote areas and so we would [16:52] give uh text summaries of these PDFs and [16:54] manuals and things and that would uh [16:59] be great and then they would get the PDF [17:00] and it would be 200 megabytes you know [17:02] and then so what we found is on the back [17:04] end on the server we could take a [17:06] screenshot essentially of the PDF and [17:08] just send that one page so that even [17:10] when they were in low connectivity areas [17:11] we could still send the text summary of [17:13] the full document mentation and [17:14] instructions but just send the relevant [17:16] parts of the PDF without them having to [17:18] download a 200 megabyte thing. So that's [17:20] know your end customer. We worked with a [17:22] hospital system for instance that uh we [17:23] originally built a voice bot for these [17:25] nurses uh and it turns out nurses hate [17:27] voice bots because hospitals are loud [17:28] and noisy and the voice transcription is [17:30] not very good and you just hear other [17:32] people yelling and they preferred a [17:33] regular old chat interface. So, we had [17:35] to know our end customers. Figure out [17:37] what exactly they were doing day-to-day. [17:40] And then [17:42] let the computer do what the computer's [17:43] good at. Don't do math in an LLM. It is [17:46] the most expensive possible way of doing [17:48] math. Um, let the the computer do its [17:52] calculations. And then prompt [17:54] engineering. I'm not going to break this [17:55] down. I'm sure you've seen hundreds of [17:57] talks over the last two days about the [18:00] uh way to engineer your prompts and [18:02] everything. Uh but one of the things [18:04] that we like to do as part of our [18:06] optimization is to think about the [18:08] output tokens and the costs that are [18:10] associated there and how we can make [18:12] that perform better. And then finally, [18:14] know your economics. There's lots of [18:16] great tools. There's things like prompt [18:17] caching. There's things like tool usage [18:19] and batch. Um batch on bedrock is a 50% [18:22] off whatever model infrance you're [18:24] trying to make across the board. And [18:26] then context management. You can [18:28] optimize your context. you can figure [18:29] out what is the minimum viable context [18:31] in order to get the correct inference [18:33] and how can I optimize that context over [18:35] time and this again requires knowing [18:37] your end user knowing what they're doing [18:38] and injecting that information into the [18:40] model and also optimizing stuff that is [18:43] irrelevant and taking it out of the [18:44] context so that the model has less to [18:46] reason over [18:50] this and you want to learn more if you [18:52] want to talk more um I'm always happy to [18:54] hop on the phone with customers you can [18:56] scan this QR code we like building cool [18:58] stuff. Uh, I got a whole bunch of [19:00] talented engineers who were just excited [19:02] to go out and build things for [19:03] customers. So, if you have a super cool [19:05] use case, come at me. All right. Thank [19:08] you very much. [19:10] [Music]