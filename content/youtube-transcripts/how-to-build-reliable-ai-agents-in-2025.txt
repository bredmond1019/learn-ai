[00:00]  [00:00] If you're a developer, then right now it [00:02] feels almost impossible to keep up with [00:04] everything that's going on within the AI [00:05] space. Everyone's talking about AI [00:07] agents. Your whole LinkedIn feeds and AX [00:10] feeds are full of it. Everyone makes it [00:12] seem super easy. Yet, you are still [00:14] trying to figure out, should I use Lang [00:16] Chain or Llama Index and trying to debug [00:19] and figure out all of these AI agent [00:21] systems that you're building and [00:22] tinkering with. All of the tutorials [00:24] that you'll find either are messy or [00:26] contradicting. And every week there is a [00:28] new fire ship video dropping something [00:30] new where you're like, "Oh do we [00:32] now also need to know this?" So all in [00:34] all, it's a complete mess. And my goal [00:37] with this video is to help calm your AI [00:40] anxiety and give you clarity on what's [00:43] currently really going on within the AI [00:46] space and why you can pretty much ignore [00:48] 99% of everything that you see online [00:51] and just focus on the core foundational [00:53] building blocks that you can use to [00:55] build reliable and effective agents. So [00:56]  [00:57] in this video, I'm going to walk you [00:59] through the seven foundational building [01:01] blocks that you need to understand when [01:03] you want to build AI agents regardless [01:05] of what tool you are using. Now I'm [01:07] going to give these code examples in the [01:09] Python programming language, but [01:10] honestly it doesn't matter what tool you [01:12] use whether that's NAN, TypeScript, Java [01:14] or any other programming language. If [01:16] you boil it down to these seven [01:18] foundational building blocks, you can [01:19] implement it in anything because they're [01:21] so simple. So, I will execute these [01:23] simple code blocks and show you the [01:25] output and walk you through everything [01:26] step by step with diagrams as well. So [01:28] that even if you've never written a [01:30] single line of Python, you can still [01:31] follow this video and I can guarantee [01:33] you that after watching this video, [01:35] you'll have a completely different [01:36] perspective on what it takes to build [01:38] effective AI agents and you'll be able [01:40] to look at almost any problem, break it [01:43] down, and know the patterns and the [01:44] building blocks you need in order to [01:46] solve it and automate it. And then you [01:47] might be thinking, okay, like Dave, but [01:48] why should I listen to you? Aren't you [01:50] just part of the 99% of noise you just [01:52] mentioned? Well, I'll let that up to [01:54] you. But I have somewhat of a unique [01:55] perspective on the AI market right now. [01:57] And that's because I have over 10 years [01:58] of experience in AI. So, I have [02:00] background in machine learning and I run [02:02] my own AI development company where over [02:04] the past years we've done over 20 full [02:06] system deployments. Next to that, I have [02:08] plenty of communities with thousands of [02:10] members combined. So, that gives me a [02:12] really good pulse on everything that's [02:14] going on, what's working and what's not. [02:16] And next to that, I also regularly [02:18] interview industry leaders like Jason [02:20] Leu or Dan Martell, for example. So, I [02:22] briefly wanted to bring that up just so [02:24] you know that I'm not just some random [02:26] guy building NAT workflows from his [02:28] bedroom. And there's nothing wrong with [02:30] that. It's just a different category. [02:32] Clients work with us and hire us to [02:33] build production ready systems that they [02:35] can rely on. And that's that knowledge [02:37] from that I want to bring to you in this [02:39] video. And the big big problem that I [02:41] see right now within the AI space and [02:43] why you feel so confused as a developer [02:45] all comes from this simple image over [02:47] here. There is simply a lot of money [02:49] flowing into the market. And every time [02:52] throughout history when there's an [02:53] opportunity like that, what happens? [02:55] People jump on it because people want to [02:57] try and capitalize on it. So even if [02:58] you're remotely interested in AI, this [03:01] is what most of your social media feeds [03:03] will look like. And they all make it [03:06] seem super easy. There are all these [03:07] tools that you can use to build full [03:09] agent armies. Yet you are still [03:11] wondering like where do I start and how [03:13] do I make this all work in really a [03:15] production ready environment. And on top [03:18] of that you have of course all of the [03:19] frameworks and libraries that follow a [03:21] similar trend right so developer tools [03:24] GitHub repositories all kinds of tools [03:26] that make it seem super easy to build [03:28] these AI agents. And then of course we [03:30] have the news, everything that's going [03:31] on and the plenty of other tools that [03:34] are built on top of that that you can [03:37] also use. And this all results in you [03:39] feeling really overwhelmed and having no [03:41] idea what's going on and what to focus [03:43] on. And now there's a clear distinction [03:45] between the top developers and teams [03:47] that are actually shipping AI systems [03:48] that make it to production versus the [03:51] developers that are still trying to [03:53] debug the latest agent frameworks. and [03:55] that is that most developers follow all [03:57] the hype you see on social media, the [03:59] frameworks, the media attention and the [04:01] plethora of AI tools that are out there [04:03] while the smart developers realize that [04:05] everything that you see over here is [04:07] simply an abstraction over the current [04:09] industry leaders the LLM model providers [04:13] and once you realize that as a developer [04:15] building AI systems and you start to [04:17] work directly with these model providers [04:19] APIs you realize that you can actually [04:21] ignore 99% of the stuff that you see [04:24] online and also O realized that [04:26] fundamentally nothing has pretty much [04:28] changed since function calling was [04:30] introduced. Yes, models get better, but [04:32] the way we work with these LLMs, it's [04:35] still the same. And every time I bring [04:37] this up, people are like, "What? How is [04:38] this possible?" But our code bases from [04:40] 2 years ago, they still run. They still [04:43] work. We only have to change the model [04:45] endpoints through the APIs because we've [04:47] engineered them in such a way to not be [04:49] reliant on frameworks that are [04:51] essentially built on Quicksand. So all [04:52] this context that I'm providing right [04:54] now is super important just like with [04:56] LLMs because otherwise the rest of this [04:58] video and the seven core building blocks [04:59] won't make a lot of sense. So the first [05:01] most important thing to understand is [05:04] that if you look at the top teams [05:05] building AI systems, they use custom [05:08] building blocks, not frameworks. And [05:09] that is because the most effective AI [05:11] agents aren't actually that agentic at [05:14] all. They're mostly deterministic [05:16] software with strategic LLM calls placed [05:18] exactly where they add value. So, the [05:20] problem with most agent frameworks and [05:21] tutorials out there is that most of them [05:23] push for giving your LLM just a bunch of [05:26] tools and let it figure out how to solve [05:28] the problem. But in reality, you don't [05:30] want your LLM making every decision. You [05:32] want it handling the one thing that's [05:34] really good at reasoning with context [05:36] while your code or application handles [05:38] everything else. And the solution is [05:40] actually quite straightforward. It's [05:42] just software engineering. So instead of [05:44] making an LLM API call with 15 tools, [05:47] you want to tactfully break down what [05:49] you're actually building into [05:50] fundamental components, solve each [05:52] problem with proper software engineering [05:54] best practices and only include an LLM [05:57] step when it's impossible to solve it [05:59] with deterministic code. Making an LLM [06:01] API call right now is the most expensive [06:04] and dangerous operation in software [06:06] engineering and it's super powerful but [06:08] you want to avoid it at all cost and [06:10] only use them when it's absolutely [06:12] necessary and this is especially true [06:14] for background automation systems. This [06:16] is a super important concept to [06:18] understand. There is a huge difference [06:20] between building personal assistants [06:22] like chat GPT or cursor where users are [06:26] in the loop versus building fully [06:28] automated systems that process [06:30] information or handle workflows without [06:32] humor in defension. And let's face it, [06:33] most of you aren't building the next [06:35] chat or cursor. You're building backend [06:37] automations to make your work or your [06:39] company more efficient. So when you are [06:41] building personal assistant like [06:43] applications, using tools and multiple [06:46] LLM calls can be more effective. But [06:48] when you're building background [06:49] automation system, you really want to [06:51] reduce them. And for example, for our [06:53] production environments for our clients, [06:55] we almost never rely on tool calls. So [06:57] you want to build your applications in [06:59] such a way where you need as little LLM [07:01] API calls as possible. Only when you [07:04] can't solve the problem anymore with [07:06] deterministic code, that's when you make [07:08] a call. And when you get to that point, [07:11] it's all about context engineering. [07:13] Because in order to get a good answer [07:14] back from an LLM, you need the right [07:17] context at the right time sent to the [07:19] right model. So you need to pre-process [07:20] all the available information, prompts, [07:22] and user inputs so the LLM can easily [07:25] and reliably solve the problem. This is [07:27] the most fundamental skill in working [07:29] with LLMs. And then the final thing that [07:31] you need to understand is that most AI [07:32] agents are simply workflows or DAGs if [07:36] you want to be precise or just graphs if [07:38] you include loops. And most steps in [07:40] these workflows should be regular code, [07:42] not LLM calls. So what I'm trying to do [07:44] in this video is really help you [07:46] understand AI agents from a foundational [07:49] level from first principles. And now [07:51] that we've set the stage, we get into [07:54] the foundational building blocks that [07:56]  [07:56] you need. And there are really only [07:58] seven or so that you use in order to [08:00] take a problem, break it down into [08:02] smaller problems, and then try and solve [08:04] each of those sub problems with these [08:06] building blocks that I will introduce to [08:08] you right now. And building block number [08:09] one is what I call the intelligence [08:11] layer. So super obvious, right? This is [08:13] the only truly AI component in there. [08:16] And this is where the magic happens. So [08:18] this is where you make the actual API [08:20] call to the large language model. [08:22] Without this, you just have regular [08:24] software. The tricky part isn't the LLM [08:27] call itself. That's super [08:28] straightforward. It's everything else [08:29] that you need to do around it. So the [08:32] pattern here is you have a user input, [08:34] you send it to the LLM, and the LLM will [08:36] send it back to you. Now we can very [08:38] easily do this in the Python programming [08:41] language using for example the open AI [08:44] Python SDK where we connect with the [08:47] client. We pretty much select which [08:49] model we want to use. We plug in a [08:51] prompt and then we wait for the [08:53] response. So simply running this can be [08:56] done in any programming language. It can [08:58] be done directly with the API. It can be [09:00] done with N8. But this is the first [09:02] foundational building block. you need a [09:04] way to communicate with these models and [09:07] get information back from it. Then [09:08]  [09:09] building block number two is the memory [09:11] building block and this ensures context [09:13] persistence across your interactions [09:15] with these models because LLMs don't [09:18] remember anything from previous [09:19] messages. They are stateless and without [09:22] memory each interaction starts from [09:24] scratch. So you need to manually pass in [09:27] the conversation history each time. This [09:29] is just storing and passing a [09:31] conversation state, something we've been [09:33] doing in web apps forever. So to build [09:35] on top of the intelligence layer that we [09:37] just saw, now next to just providing a [09:40] user input prompt, we also get the [09:42] previous context and we structure that [09:44] in a conversation like sequence where we [09:48] have a sequence of messages. Then we can [09:50] get the response and within that process [09:52] we also have to handle updating our [09:55] conversation history. And within the [09:57] second file over here called memory.py [09:58] Pi, we see an example where we ask the [10:01] AI to tell us a joke. Then we ask a [10:03] follow-up question to ask what was my [10:06] previous question, but we don't handle [10:08] the conversation history correctly. And [10:10] because LLMs are stateless, if we run [10:12] this, it will simply don't know. And [10:14] then here in this function, we have a [10:16] proper example of how to handle memory [10:18] where we pass in the conversation [10:20] history and we have an alternating [10:22] sequence between user and assistant. We [10:24] are now programming this dynamically [10:26] within our code. In a more realistic [10:28] example, you would store and retrieve [10:30] this from a database. And to demo this, [10:32] we can simply run the first joke. So why [10:35] do programmers prefer dark mode? Because [10:37] light attracts bugs. Then we ask it a [10:39] follow-up question to ask it what was my [10:42] previous question. And it says, I'm [10:44] unable to recall previous interactions. [10:46] And lastly, we do it properly where we [10:49] pass down the previous answer. And then [10:51] your previous question was asking for a [10:53] joke about programming. So now it [10:55] understands the context of the [10:56]  [10:56] conversation history. And then building [10:58] block number three is what we call tools [11:00] for external system integration [11:02] capabilities. Because most of the time [11:04] you need your LLM to actually do stuff [11:06] and not just chat because pure text [11:08] generation is limited. You want to call [11:11] APIs, update databases or read files. [11:14] tools let your LLM say I need to call [11:16] this function with these parameters and [11:18] your code handles the actual execution [11:20] of that. So if we then look at the [11:22] diagram over here, we augment the [11:25] intelligence layer, the LLM API call [11:27] potentially also with memory and we now [11:30] also provide the LLM with tools. And for [11:33] every API call with tools, the LLM [11:35] decides should I use one or more of the [11:38] tools that I have available? Yes or no? [11:41] If no, I'll give a direct response, a [11:43] text answer back. If yes, then select [11:45] the tool. Then your actual code is [11:47] responsible for catching that and [11:49] executing the tool. then pass the result [11:52] one more time to the LLM for it to [11:54] format the final response yet again in a [11:57] text answer for you. And tool calling is [11:59] also directly available in all of the [12:01] major model providers. So no need for [12:03] any external frameworks or libraries. We [12:05] just specify the function that we want [12:07] to call. We transform that into a tool [12:10] schema that we then make available to [12:12] the LLM. Our code will perform a simple [12:15] check to see if the LLM actually decided [12:17] to call the tool. then pass in the [12:19] parameters into the actual function and [12:21] run it and then pass it back to the LLM [12:23] one more time. And if we run all of [12:25] this, we can now see that we now have an [12:27] LLM that using the get weather function [12:29] is able to get the weather information [12:31] for any given city or place that you can [12:34] think of. So we augment the LLM beyond [12:37] just the text generation capabilities [12:39] based on what the model was trained on. [12:41] So through tools, we give the model a [12:43] way to integrate and to connect with [12:45] external systems. And now if you're [12:47] entirely new to tool calling, you've [12:49] never done it, this can actually be a [12:50] little tricky to understand. You need to [12:52] see a couple of examples. So for that, I [12:54] highly recommend to check out the [12:55] official documentation from OpenAI on [12:57] function calling. Or if you want to go [12:59] really deep with this, I have this full [13:00] beginner course here on YouTube, [13:02] building AI agents in pure Python, which [13:04] is actually a really good follow-up for [13:06] this video, which I will link in the [13:08]  [13:08] description as well. All right. So now [13:09] that we understand the basic operations [13:11] that you can do with a large language [13:12] model, we get to building block number [13:14] four, which is arguably the most [13:16] important one, and that is validation. [13:18] So this can help us with quality [13:21] assurance and structured data [13:22] enforcement. So if you want to build [13:24] effective applications around large [13:26] language models, you need a way to make [13:28] sure the LLM returns JSON that matches [13:31] your expected schema because LLMs are [13:34] probabilistic and can produce [13:35] inconsistent outputs. If you ask it one [13:38] question and then another user asks a [13:40] question in a slightly different way, [13:41] they can get a completely different [13:43] answer. One might be correct, the other [13:45] one might be wrong. So you validate the [13:48] JSON output against a predefined [13:50] structure. If the validation fails, you [13:52] can send it back to LLM and fix it. And [13:55] this concept is known as structured [13:57] output. And we need that structured [13:59] output, which is super crucial so we can [14:01] engineer systems around it. So rather [14:03] than just asking a question to a large [14:05] language model and getting text back, we [14:08] want a predefined JSON schema where we [14:10] are 100% sure that what we're getting [14:13] back contains the actual fields that we [14:15] can use later down the line within our [14:17] application. So the diagram looks like [14:19] this. We ask an LLM to provide us with [14:22] structured output which is simply just [14:24] JSON. We validate it against a schema [14:27] using a library like Pentic or S or [14:29] Python data classes. We check whether [14:32] it's valid. If it's valid, we have the [14:34] structured data. If it's not valid, we [14:36] take the error response and we send it [14:38] back to the LLM in order to correct it. [14:40] So, let's say you want to build some [14:42] agentic task management tool that can [14:45] transform natural language and put it [14:47] into task with due dates and priorities. [14:50] Instead of just building an agent and [14:52] let it interact with the user and then [14:54] just provide a text output back of what [14:57] it thinks is the goal of the user, we [14:59] can define a specific data structure. In [15:02] this example, I'm doing this with the [15:03] pyentic library within Python. And [15:05] getting structured output from large [15:06] language models is yet again also [15:09] supported by all of the major model [15:11] providers. So here you can see that when [15:12] we're making the LLM API call to OpenAI, [15:15] we have the parameter of text format [15:18] that we can put in here and we set that [15:20] equal to the defined task result object [15:24] that we want to get back from the LLM. [15:26] So now with our system prompt and [15:28] instructing our model to extract task [15:29] information from the user input and our [15:31] prompt I need to complete the project [15:33] presentation by Friday is high priority. [15:35] We can run this and get an actual [15:37] validated structured output data object [15:40] back where we can clearly see we have a [15:42] task completed and a priority field that [15:45] we can now also programmatically call. [15:47] So we can now actually come to our [15:49] result object. We can call the task on [15:52] it and we actually get the information [15:54] that's on there. And with techniques [15:56] like this we can validate both the [15:58] incoming data so what we send to the LLM [16:00] as well as the outcoming data of what [16:02] the LLM is sending back to us. And you [16:05] now already understand that context [16:06] engineering is one of the most important [16:08] skills when it comes to building [16:10] reliable LLM applications. So using [16:12] libraries like Pientic are really at the [16:14] core of that. And then real quick, if [16:16] you're a developer and you ever thought [16:18] about starting as a freelancer to maybe [16:19] make a little bit more money on the [16:21] side, work on fun projects, or just to [16:22] learn, but you don't really know where [16:24] to start or how to find that first [16:26] client, you might want to check out the [16:27] first link in the description. It's a [16:29] video of me going over how my company [16:30] can help you with this. We've been [16:32] running this program for over 3 years. [16:33] We have hundreds of case studies of [16:35] developers who already successfully made [16:37] the leap and are now working on exciting [16:39] projects either next to their full-time [16:41] job or even doing this full-time. And [16:43] now, if you feel like you're not [16:44] technically ready yet for freelancing or [16:46] don't feel comfortable, there's a second [16:48] link in there as well that will teach [16:49] you everything that you need to know [16:50] about how to get ready for freelancing [16:52] as an AI engineer. And that brings us to [16:53]  [16:55] building block number five, and that is [16:56] control for deterministics, decision-m, [16:59] and process flow. So you don't want your [17:02] LLM making every decision. Some things [17:04] should be handled by regular code. You [17:07] can use if else statements, switch [17:09] cases, and routing logic to direct flow [17:11] based on conditions. This is just normal [17:14] business logic and routing that you [17:15] would write in any application. So if we [17:18] look at the diagram over here, we have [17:20] incoming data. We can for example use an [17:23] LLM to classify the intent using [17:26] structured output where we tell the LLM [17:29] look here's the incoming message. It can [17:31] either be a question, it can be a [17:33] request, a complaint or the category of [17:35] other. We can now program our [17:37] application using simple if statements [17:39] where we do a quick check. If the [17:41] category equals question, we call this [17:44] specific function that handles just that [17:46] part of the application. If it's a [17:48] request, we do something else. If it's a [17:50] complaint, we have a different way of [17:52] handling it. We now make our workflow in [17:55] our process modular. We take a big [17:57] problem and we break it down into [17:59] smaller sub problems and categories that [18:01] we can better solve individually. And [18:03] now here's what that looks like in a [18:05] simple code example. So we again use [18:07] Pentic here to define a data model where [18:09] we can now specify the intent and we set [18:12] that equal to a literal which is pretty [18:15] much a category. It can either be a [18:17] question, a request or a complaint. If [18:18] it's set to something else, it will [18:20] throw an error. We also have the [18:21] confidence score and reasoning. We then [18:24] have a simple function that we can use [18:26] to filter for the type of intent that we [18:29] have. And then based on that, we can [18:30] call a specific function within our [18:33] application. This is nothing more than [18:34] using simple if else statements to [18:36] create the router that we see over here [18:38] in this diagram. So if I now run this, I [18:40] will run through these three examples. [18:42] So three questions. What is machine [18:43] learning? Please schedule a meeting for [18:45] tomorrow. And I'm unhappy with my [18:46] surface quality. So you'll find that the [18:48] LLM is now going through this and it's [18:51] going to determine the intent. So for [18:53] the first question, uh it determined [18:55] that this is a question. Now it's a [18:57] request in the second one and the third [18:58] one is a complaint. And based on that, [19:01] it handled it differently based on the [19:03] functions that we have sent it towards. [19:06] And now you can see that you can also [19:07] start to chain multiple LLM calls [19:09] together where if it's a question, we [19:12] simply have another function that makes [19:14] another call to OpenAI to handle it. For [19:17] a request, we might do something else. [19:19] Here we just do a simple print [19:20] statement, but this can be anything. [19:22] This is how you make modular workflows [19:24] where you implement the logic based on [19:26] certain conditions. And now remember [19:28] this is all possible because we are [19:30] first of all using structured output. So [19:33] we know that we get this data model [19:35] back. Then in our code we look at the [19:37] response from the LLM and we can say [19:39] take the intent and then we can create a [19:42] simple if else statement to do a simple [19:44] check. If the intent is equal to [19:47] question we do this. If the intent is [19:50] equal to request we do this and so on. [19:52] And now at this point I can also get [19:54] back to something I mentioned earlier in [19:56] this video and that was being very [19:58] careful with tool calls and that for our [20:00] production environments we rarely use [20:01] tool calls at all. So what do we do? [20:04] Well, we almost always prefer to use [20:07] structured output and let an LLM decide [20:10] a specific type of category and then [20:12] based on that category create simple [20:14] routers within our code that you just [20:16] saw using if else statements to decide [20:19] what function or tool if you will to [20:22] use. Now, in simple cases, the result [20:24] will be exactly the same whether you use [20:25] a tool call or the approach that I just [20:27] mentioned. But when your systems get [20:29] more complex and you need to debug [20:31] things, it can get very tricky as to [20:34] figure out why an LLM did not decide to [20:37] use a tool call. Whereas if you use a [20:40] classification step with categories and [20:42] a reasoning for why it decided to use [20:45] that category, you have a full log of [20:48] figuring out, okay, look, we have a bug [20:50] within this step of our workflow for [20:52] this particular data point where the LLM [20:55] actually thought it was this category [20:56] and it gave a reasoning for why it [20:59]  [20:59] thought it was that category. And that's [21:00] also exactly what we were doing over [21:02] here. So beyond just printing the [21:05] response and the classification, we also [21:07] list the reasoning. So you can see that [21:09] over here if I make this a little bit [21:11] bigger. So the input, what is machine [21:13] learning reasoning? The input asks for [21:15] information or explanation about the [21:16] concept etc. So this gives you an entire [21:19] log as to the decision-making process of [21:21] the LLM, which is super helpful for [21:23] debugging. All right. And that brings us [21:25] to building block number six, and that [21:27] is recovery. So things will go wrong in [21:30] production. and APIs will be down. LLMs [21:32] will return nonsense. Rate limits will [21:34] hit you. And you need try catch blocks, [21:38] retire logic with back off and fallbacks [21:40] responses when stuff breaks. This is [21:43] building reliable applications 101. This [21:45] is just standard error handling that you [21:47] would implement in any production [21:48] system. So it could look something like [21:50] this. You have a request coming in. You [21:53] check whether it's a success, yes or no, [21:55] based on either an error that is [21:57] happening or some kind of data that is [21:59] present or not. If it's a success, you [22:01] can simply return the result. All good. [22:03] But if it's not a success, there is an [22:05] error, you can for example retry that [22:08] first of all check is that even [22:10] possible. So you can retry with a back [22:12] off. Or if that's not possible at all, [22:13] you have some kind of fallback scenario [22:15] where you for example let the user know [22:18] sorry I cannot help with this question [22:20] because it couldn't find the right [22:21] information in the knowledge base for [22:23] example. And here quickly a simple [22:25] example within the Python programming [22:27] language you can use the try accept [22:29] blocks where if something goes wrong [22:31] within this first part of the code it [22:34] raises some type of error it will fall [22:36] back to the exception. Now you can [22:39] expand on this with a finally clause [22:41] where you can try something in another [22:43] case do this and then if that doesn't [22:45] work you can finally do something else. [22:47] Here you can build some kind of a [22:49] recovery mechanism where we can for [22:52] example check if a certain key here is [22:54] available. So we try to access a field [22:56] within a dictionary that is not present [22:58] in this case. So it's not available. [23:00] We're using the fallback information and [23:02] that results in a general output or a [23:05] standard reply in this case. It's a very [23:06]  [23:08] simple illustration. Uh this is [23:10] infinitely complex when it comes to [23:12] doing this properly within your own [23:14] applications because every try accept [23:16] block will be completely unique to what [23:19] problem you're trying to solve and what [23:20] errors may or may not pop up. And then [23:22] the final building block is what we call [23:25] feedback. So human oversight and [23:27] approval of workflows because some [23:30] processes are just too tricky right now [23:33] to be fully handled by your AI agents. [23:35] Sometimes you just want a human in the [23:37] loop to check an LM's work before it [23:39] goes live or sent to someone. So when a [23:42] task or decision is too important or [23:44] complex for full automation, like [23:46] sending very sensitive emails to [23:48] customers or making purchases, adding [23:51] approval steps where humans can review [23:53] and approve or reject before execution [23:56] is crucial. This is a basic approval [23:58] workflow like you would build for any [24:01] app, but then have really the human in [24:03] the loop where it takes a full stop and [24:05] it waits for that point. So let's say an [24:07] LLM generated a response or created some [24:10] piece of content and before sending that [24:12] out into the world, you want human [24:15] review in there. So a human for example [24:17] getting a popup within Slack with a [24:20] button like yes or no to review it, [24:23] approve it, and then if it's all good, [24:26] perfect. We'll send it, we'll execute [24:28] it. If it's a no, we can potentially [24:30] provide feedback as to what we need to [24:33] adjust. And then we send that back to [24:35] the LLM and we repeat the process one [24:38] more time. And this is where we get back [24:40] to the point that I brought up about the [24:42] importance of humans in the loop and the [24:43] difference between building what I call [24:45] AI assistants that directly work with [24:48] humans in the loop like chat GPT or [24:50] cursor. the user asks something, the LM [24:52] does something and the user immediately [24:55] sees the feedback and can adjust and [24:57] they can work in this dance going back [24:59] and forth versus fully autonomous [25:01] systems that work in the background. [25:03] Let's say a customer care ticketing [25:05] system fully autonomous. It comes in, [25:07] the AI should solve that ticket, [25:10] generate a response and then send it [25:11] back. There is a big distinction between [25:13] the two. And almost all very effective [25:16] and great AI products, if it gets to a [25:19] point where it gets too tricky, you [25:21] instead of like just optimizing your [25:23] prompt further and further and further, [25:25] you might just need a human in the loop [25:27] in order to be on the safe side of [25:29] things before shipping something into [25:32] production where it works 80% of the [25:35] time, but 20% of the time it's a [25:37] complete shitow. And how you can [25:39] actually implement that within your [25:41] application or codebase here in seven [25:42] feedback.py, by you can integrate or [25:45] create a strategic moment where you have [25:47] a full stop and do not let the agent [25:50] continue until it gets an approval. Now [25:53] there are various ways that you can do [25:55] this. This is a very simple example. You [25:57] can integrate some kind of front-end [25:58] application. You can integrate something [26:00] like Slack. You'll probably need to set [26:02] up some web hooks to ping back and [26:04] forth. Super technical beyond the scope [26:06] of this video, but the principle is the [26:08] same. You want to create a full stop [26:10] where if you for example run this and [26:12] I'm running this in the terminal over [26:14] here to showcase this. Let's say we're [26:16] generating a piece of content. So here [26:18] is the content generated right now. The [26:20] agent before it continues it waits for [26:22] approval. So I'm here right now doing [26:24] this in the terminal but again in a real [26:27] application you want to have some [26:28] systems that users can easily interact [26:30] with and get a notification from. So [26:32] let's say I do yes then final answer is [26:35] approved. When I run it one more time, I [26:37] let it generate another piece of [26:39] content. I can now ignore that or give [26:42] essentially a no and just not approve [26:45] this workflow. Now, you would also [26:47] ideally then have some feedback around [26:49] that. But again, the principle is the [26:51] same, a full stop before sending it off. [26:53]  [26:53] All right? So, those are your seven [26:55] building blocks that you need to [26:56] understand in order to build reliable AI [26:59] agents. And now what you do is you take [27:01] a big problem. You break it down into [27:03] smaller problems. And for every smaller [27:05] problem, you try to solve it using the [27:09] building blocks available only using an [27:12] LLM API called the intelligence layer [27:14] when you absolutely cannot get around [27:17] it. And now if you want to learn how you [27:20] can orchestrate entire workflows using [27:22] these building blocks, so how to [27:23] actually combine them and piece them [27:25] together, you want to check out the [27:27] workflow orchestration that I have over [27:29] here. So this is the GitHub repository [27:31] for the course that I already shared [27:33] that I have on YouTube. So this is the [27:35] same same resource, same video. I will [27:37] link it below. It's a super good [27:39] follow-up from this video where we will [27:41] bring everything together. So, make sure [27:42] to like this video, subscribe to the [27:44] channel, and then go check out this [27:46] video