[00:01] [Music]

[00:14] hi everybody So I'm Christopher Lovejoy [00:17] I'm a medical doctor turned AI engineer [00:19] and I'm going to share a playbook for [00:22] building a domain native LLM application [00:24] Uh so I spent about eight years um [00:27] training and working as a medical doctor [00:29] and then I spent the last seven years [00:30] building AI systems that incorporate [00:32] medical domain expertise Um and I did [00:35] that at a few different startups So I [00:37] worked at a a health tech startup called [00:38] Serakare um doing tech enabled home care [00:41] Uh the startup recently hit 500 million [00:43] ARR Um worked at various other startups [00:46] and I currently work at Anterior [00:48] And Anterior is a New York based [00:51] clinicianled uh company Here we provide [00:53] clinical reasoning tools to automate and [00:57] accelerate uh health insurance um and [00:59] healthcare administration Uh we serve [01:01] about 50 million we serve uh health [01:03] insurance providers that cover about 50 [01:05] million lives uh in the US And we spend [01:08] a lot of time thinking about what does [01:09] it mean to build a domain native LLM [01:12] application whether it's in healthcare [01:14] um or otherwise Uh and that's what I'm [01:16] going to talk about today And in [01:19] particular our bet really is that when [01:21] it comes to vertical AI applications the [01:24] system that you build for incorporating [01:25] your domain insights is far more [01:27] important than the sophistication of [01:28] your models and your pipelines So the [01:31] limitation these days is not like how [01:33] powerful is your model and whether it [01:34] can uh reason to the level you need it [01:37] to It's more can your model understand [01:39] the context um in that industry for that [01:42] particular customer uh and perform [01:44] perform the reasoning that it needs to [01:46] and the way that you enable that and the [01:48] way that you uh kind of iterate quickly [01:50] with your customers is by building the [01:52] system around it and there's various [01:54] components to that um that's what I'm [01:56] going to talk about So this is the kind [01:58] of high level schematic and we're going [01:59] to go through each of these parts um [02:01] throughout the talk Uh as you'll see [02:03] right in the middle there's the the PM [02:06] um and this is you know in our [02:07] experience it makes sense for this to be [02:08] a domain um expert um product manager So [02:11] in our context it's clinical um and I'm [02:14] going to go through go through this in [02:15] more detail shortly But first I think [02:17] it's worth taking a quick step back and [02:19] asking you know why is it so hard to [02:21] successfully apply large language models [02:23] to specialized industries [02:25] We think it's because of the last mile [02:27] problem And what I mean by the last mile [02:29] problem is is this problem that I I kind [02:31] of touched on just now around uh giving [02:34] the model and your your kind of AI [02:36] system more generally context and [02:38] understanding of the specific workflow [02:40] for that customer for that industry Um [02:43] and [02:46] I'm going to illustrate that with an [02:47] example um from a clinical case that [02:50] we've processed Our AI anterior is [02:52] called Florence and a 78-y old female [02:56] patient uh presented with right knee [02:58] pain The doctor recommended a knee [03:01] arthoscopy and as part of deciding [03:04] whether this treatment was appropriate [03:05] whether the doctor made an appropriate [03:06] decision Florence needs to answer [03:08] various questions Uh one of those [03:10] questions is is there documentation of [03:12] unsuccessful conservative therapy for at [03:14] least six weeks Um and you know on the [03:17] surface of it that might seem relatively [03:19] simple I mean I appreciate maybe not a [03:21] lot of doctors in the room so you might [03:23] not know what conservative therapy is [03:25] but um actually there's a lot of kind of [03:28] like hidden complexity in answering a [03:29] question like this So for example you [03:32] know conservative therapy um typically [03:35] what we mean by conservative therapy is [03:36] when there's some kind of option for uh [03:39] you know a more aggressive treatment [03:40] maybe a surgical operation that's like [03:42] the the you know the surgical treatment [03:44] And then if you're deciding not to [03:46] operate and you want to try something [03:47] conservative first that's like the [03:48] conservative therapy So it might be you [03:50] know do physiootherapy uh lose weight um [03:53] do kind of you know non-invasive things [03:55] that might help resolve the problem But [03:58] actually there's some there's still some [03:59] ambiguity there because uh you know in [04:01] some cases giving medication might be a [04:03] conservative therapy In some cases [04:05] that's actually the more aggressive [04:06] treatment and there's something else [04:07] that's more conservative Um so there's [04:09] one layer of ambiguity there Then when [04:11] we talk about unsuccessful um well what [04:13] is unsu let's say that somebody has uh [04:16] some knee pain they do some treatment [04:18] and their symptoms improve significantly [04:20] but they don't like fully resolve So is [04:23] that successful do we need like a full [04:24] resolution of symptoms or is it just [04:26] like a partial resolution is enough if [04:28] it's partial like at what point is that [04:30] enough to be quantified as successful um [04:32] so again there's kind of complexity and [04:34] nuance with with how that's interpreted [04:36] And then finally documentation for at [04:38] least six weeks again you know [04:40] documentation Are we saying that the [04:44] medical record said they started [04:45] physical therapy 8 weeks ago then it's [04:47] never mentioned again we can therefore [04:49] assume that they've been doing it for [04:50] for 8 weeks Uh or do we need like [04:52] explicit documentation that they started [04:54] treatment they did it for 8 weeks and [04:57] you know it's completed Uh where where [05:01] do we like draw the line there in terms [05:02] of what we can infer [05:06] um and yeah just kind of coming back to [05:07] echo our point So this is really our bet [05:10] that the system is more important Uh we [05:12] believe that in every vertical industry [05:15] the uh you know the team the company [05:17] that wins is the one that builds the [05:19] best system for taking those domain [05:20] insights and quickly translating them [05:22] into the pipeline giving it that context [05:24] and iterating um to create this [05:26] improvements [05:29] Um and we also you know found I guess to [05:32] talk to this counterpoint the models I [05:33] mean models obviously are important Um [05:36] and the the progress in models makes it [05:38] easier to have a good starting point but [05:40] that's only getting up to a certain [05:41] baseline And we found we kind of hit a [05:43] saturation around like 95% uh level So [05:46] we invested a lot of time and effort [05:48] improving our pipelines Um obviously 95% [05:50] is stuck still pretty reasonable And [05:52] this is that performing the like primary [05:54] task that our our AI system does which [05:56] is approving these care requests um in a [05:59] health insurance context Um so we're at [06:01] 95% and we then iterated based on this [06:04] system um that I'm going to walk through [06:06] and we really got to you know kind of [06:07] almost silly accuracy of like 99% Uh we [06:10] got this class point of um light award a [06:13] few weeks ago for this Um and really [06:16] what we found here and what we observed [06:18] is that the the models reason very well [06:21] they get to a great baseline But if [06:22] you're in in an industry where you [06:24] really need to ek out that like final [06:25] mile of performance um you need to be [06:28] able to then kind of give the model give [06:29] the pipeline that context [06:33] Uh so how do we do that well we call [06:36] this our adaptive domain intelligence [06:38] engine And what this is performing is [06:41] it's taking customer specific domain [06:43] insights and it's converting them into [06:45] performance improvements um and kind of [06:47] building a system around that And [06:49] there's broadly two main parts to this [06:51] The first part is the measurement side [06:53] of things So you know how is how is our [06:55] current pipeline doing um and then the [06:58] rest of this is the uh improvement side [07:01] So I'm going to talk first a bit more [07:02] about measurement in more detail and [07:03] then and then a bit about improvements [07:06] So measuring domain specific uh [07:08] performance [07:10] The first thing um and I think you know [07:12] a lot of this is is really just kind of [07:13] practice best practice more generally [07:15] but um the first step is to define what [07:18] is it that your users really care about [07:20] as metrics So in a health context [07:22] obviously I've been talking about [07:23] medical necessity reviews um this is our [07:25] bread and butter and there the customers [07:27] really care about false approvals They [07:28] want to minimize false approvals because [07:30] a false approval where you've approved [07:32] care means that you know a patient who [07:34] didn't need the care might get given [07:35] some care they don't need and obviously [07:37] from an insurance provider point of view [07:38] they're then paying for treatment that [07:39] they don't necessarily want to pay for [07:41] Um and often defining these metrics is [07:44] like a collaboration between the domain [07:45] experts in your company and the [07:46] customers to kind of like really [07:48] translate what are the metrics that you [07:49] care about They might be like one or two [07:51] or like usually there's just a few [07:53] metrics that matter most So in a few [07:55] other industries like legal when you're [07:57] analyzing contracts it might be that you [07:58] really want to minimize a number of uh [08:00] missed critical terms when you're when [08:01] you're identifying these clauses in the [08:03] contract for fraud detection Your [08:04] topline metric might be something like [08:05] preventing um dollar loss from fraud You [08:07] know education it might be you want to [08:09] optimize for test score improvements Um [08:11] I think it's it's definitely a helpful [08:13] exercise to push yourself to think of [08:15] like really if I'm optimizing for like [08:16] one or two metrics what is like the [08:18] metric that is most important Um and [08:21] then what you can also do hand inhand [08:23] with that um which is very helpful uh [08:26] just going off the bottom there a little [08:27] bit but uh is designing a failure mode [08:31] ontology and what I mean by this is [08:33] taking the task that you're performing [08:36] and identifying what are all the [08:37] different ways in which my AI fails and [08:39] it might be at the level of like higher [08:41] order categories so for example here [08:43] we've got medical record extraction [08:44] clinical reasoning and rules [08:45] interpretation we found that for medical [08:47] necessity review these are the three [08:48] broad categories these the three board [08:50] ways in which the AI can fail and then [08:52] within those there's various like [08:53] different subtypes Um and this is an [08:55] iterative process There's like various [08:56] techniques for doing this Um I think it [08:59] it's important here to bring in your [09:00] domain experts I think one failure mode [09:02] is that you have somebody kind of [09:04] looking at your AI traces in isolation [09:05] and coming up with these um who don't [09:07] necessarily have the context on how [09:08] things are working I think this is a a [09:10] step that's critical to have domain [09:11] experts uh leading this process [09:15] Um but really I think the the big value [09:18] ad is when you do both of these at the [09:20] same time um together because what this [09:22] gives you uh and and this is a this is a [09:24] dashboard that we've built internally I [09:26] appreciate the text might be a little [09:27] bit small um but essentially on the [09:29] right hand side you have a patient's [09:31] medical record You also have the [09:32] guidelines that the record is being [09:34] appraised against On the left hand side [09:36] you have the AI outputs Um so this is [09:39] the decision that it's made the [09:40] reasoning behind its decision and what [09:42] we enable our domain experts to do here [09:44] enable our clinicians is they can come [09:46] in they can mark whether it's correct or [09:47] incorrect and if it's incorrect then [09:49] this box here is for um defining the [09:51] failure mode So from that ontology we [09:53] just saw on the slide before they can [09:54] say this failed in this way and doing [09:58] those at the same point and having your [09:59] domain expert sit at that point doing [10:01] both of these is uh super valuable [10:04] because it then enables you to [10:06] understand things like this So on the x- [10:08] axis here we have number of false [10:10] approvals That's the metric that we [10:11] really care about in our context And [10:13] then we have the different failure modes [10:15] on on the y- axis And obviously that [10:17] tells us that if we want to minimize our [10:19] false approvals and we want to like [10:20] optimize for this this top northstar [10:22] metric that we care about these are what [10:24] we want to address first kind of in this [10:26] order Um which as a PM is then a useful [10:29] piece of information to help you [10:30] prioritize uh the work that you want to [10:32] do [10:35] So that's the measure side of things I'm [10:37] now going to go on to talk about the um [10:39] the improvements [10:41] um and particularly with this domain [10:43] specific context [10:45] So [10:48] what that also gives you this kind of [10:50] failure mode labeling we talked about [10:51] before is you get these readymade data [10:54] sets that you can iterate against And [10:56] these data sets are super valuable [10:57] because they're coming directly from [10:59] production data which means you know [11:01] that they're representative of the kind [11:02] of input data distribution that you're [11:04] going to see more so than synthetic data [11:06] would be Uh and you can now you know [11:09] when you you had those priorities on the [11:11] previous slide we saw which sort of [11:12] failure modes were causing the most [11:13] false approvals We can then pick that [11:14] data set of you know 100 cases that came [11:17] through prodical [11:19] failure mode You can give that to an [11:21] engineer An engineer can iterate against [11:22] it and you can keep on testing Okay how [11:23] is my performance against that [11:24] particular failure mode right now [11:29] and that lets you do something like this [11:30] where on the x-axis here we have the [11:32] pipeline version On the y axis we have [11:34] the performance score Um by definition [11:36] on these flaws we're starting very low [11:38] for each of these like failure mode data [11:40] sets but every time you increment your [11:42] pipeline version maybe you spent some [11:43] time focusing on this particular failure [11:45] mode and and you were able to get a big [11:46] jump in performance Um and then you can [11:48] see the other ones also jumping up as [11:49] well um on kind of subsequent releases [11:52] And you can also use this to then track [11:53] that you're not regressing on any [11:54] particular failure mode as well Um so [11:57] it's a useful useful uh visualization to [11:59] be able to make [12:02] And you can then go one step further and [12:04] actually bring your domain experts into [12:06] the kind of improvements in the [12:08] iteration itself And what that looks [12:10] like is creating this tooling that [12:13] enables a domain expert who's not [12:14] necessarily technical to come in They [12:16] can then suggest changes to the [12:17] application pipeline They can also [12:19] suggest new domain knowledge that's made [12:21] available to the pipeline And obviously [12:23] they're the best positioned to be making [12:24] these kind of um you know opinions of [12:27] what sort of domain knowledge might be [12:28] might be relevant And then you have your [12:30] pipeline in the middle that's ready to [12:32] use those if it wants to And on the [12:34] right hand side you have those domain [12:35] evals which might be these failure set [12:37] evals You might have more generic eval [12:38] sets as well and they can then tell you [12:41] in a data-driven way okay given this [12:43] domain knowledge suggestion from a [12:45] domain expert should that go live in the [12:47] platform and now it's in production and [12:48] and then um you know it should be [12:50] improving the performance for for live [12:51] customers Um and this whole loop can [12:53] happen very quickly So for example and I [12:56] think actually on the next slide yeah [12:57] I'll just show um so this is a dashboard [13:00] we saw before but this is with this [13:02] extra button which is like a domain [13:03] knowledge addition button and so again [13:05] we're keeping the same context we have [13:07] uh you know a domain expert clinician [13:09] coming in here they're reviewing the [13:10] case they're saying is it correct is it [13:12] incorrect they're saying what's the [13:13] failure mode and now they can say I [13:15] think this domain knowledge would be [13:17] helpful for the application's [13:18] performance and uh you know it might be [13:21] I think in this case appreciate it might [13:23] not be that easy to read But um the [13:25] model is kind of making some some [13:26] mistake related to understanding [13:28] suspicion of a condition because the [13:31] patient like has the condition and it [13:32] says oh there's no suspicion of the [13:34] condition um but actually they they have [13:36] it and like there's there's like you [13:38] could give some information to the model [13:40] for the medical context of how we [13:41] interpret suspicious or suspicion as a [13:43] word that would then influence the [13:45] answer Um or it could be that maybe the [13:47] reasoning uses some kind of scoring [13:48] system and you realize actually the [13:50] model doesn't have access to that [13:50] scoring system You could again you could [13:52] add that as domain knowledge um to [13:54] continually build out what the what the [13:56] model can handle And what that helps [13:59] with yeah in ter in terms of kind of [14:01] iteration speed from that you can do [14:03] that maybe you want to let your evals [14:05] automatically let that go in or maybe [14:07] you want to um have some kind of human [14:08] in the loop but it just means that you [14:10] can have this very quick process This [14:11] prod comes through you analyze it um by [14:15] through a clinical lens and then the [14:16] same day you've essentially fixed it [14:18] because you've added the domain [14:18] knowledge that should solve it You can [14:20] prove that with the evals and then it's [14:21] live [14:26] And what this means is that you know [14:27] these domain expert reviews that are [14:29] really kind of powering a lot of the [14:30] insights you're getting here are giving [14:31] you three main things They're giving you [14:33] performance metrics they're giving you [14:34] these failure modes and they're giving [14:36] you these suggested improvements um [14:37] allin one [14:40] Yep

[14:45] Yeah good question So the question is um [14:46] how do you define a domain expert like [14:47] what level of of expertise do you need [14:49] here i think it really depends on the [14:50] specific like workflow that you're doing [14:53] um and what you're kind of optimizing [14:55] for So in our context if you're [14:57] optimizing for clinical reasoning and [14:59] the quality of the clinical reasoning [15:00] you therefore want somebody with like as [15:02] much clinical experience ideally a [15:03] doctor um you know ideally they have [15:06] relevant expertise in the specialtity [15:07] that you're dealing with Uh but it but [15:10] it kind of really depends on your use [15:11] case It might be that there's actually [15:12] simpler things we also um can can do in [15:16] which case that level of expertise is [15:17] not necessary and you could have you [15:18] know like a more junior clinical person [15:20] but the idea being that it's either like [15:22] a nurse or a doctor or somebody that has [15:23] experience of doing this workflow in in [15:26] the real world Does that make sense yeah [15:28] Another question

[15:35] Yeah This is this is bespoke tooling And [15:38] I think in general my my philosophy on [15:40] this is that if you if you're really [15:43] placing a lot of weight on what you're [15:46] kind of generating and this feeds into [15:47] your system in various other different [15:48] ways in the kind of ways I'm describing [15:50] it probably makes most sense to do this [15:51] with bespoke tooling that you build [15:52] yourself because it's you want to [15:54] integrate it into the rest of your [15:55] platform and it's just generally going [15:56] to be um you know easier to do that if [15:59] you're if you're kind of like doing [16:00] everything yourself Yeah [16:03] Are these are your domain experts users [16:06] them to come in [16:08] Yeah great question Um I think it c it [16:10] can be both Um we like in our experience [16:14] typically we start with we we will hire [16:15] some people inhouse who kind of come and [16:17] do this for us to give us this initial [16:18] data so that we can do that iteration I [16:20] think there's definitely a world in [16:21] which the customer themselves might also [16:24] want to do validation of your AI and [16:26] they might actually do this kind of [16:27] process themselves in which case this [16:29] then becomes a customerf facing product [16:30] for them to to use as well Um yeah [16:34] Uh okay So love the questions but we're [16:37] going to reserve time for Chris to keep [16:39] going Sounds good And and I'm just um [16:41] these are the last couple slides now as [16:42] well So putting everything together Uh [16:45] this is the overall flow and [16:49] essentially what this what what this can [16:51] look like is you have your production [16:52] application It's generating these [16:54] decisions these AI outputs You're having [16:57] your domain experts review that giving [16:58] these performance insights That's things [17:00] like the metrics the failure modes uh [17:03] you then have your PM your kind of [17:04] domain expert PM who sits in the middle [17:06] They then have this rich information on [17:08] okay what should I prioritize based on [17:09] the failure modes based on the metrics [17:11] They can then turn to an engineer and [17:12] say um I want you to fix this failure [17:15] mode because I really care about it and [17:16] I want you to fix it up to this [17:17] performance threshold So they can say [17:18] right now you know in production we're [17:20] getting 0% or 10% on this particular [17:22] data set I want you to go away and work [17:24] on this until you get to 50% And then [17:26] the engineer can go and um you know run [17:29] different experiments have different [17:30] ideas of how they might improve this [17:31] changing prompting changing models doing [17:33] fine-tuning all this kind of thing They [17:35] then have a very tight iteration loop [17:37] because they have these ready-made [17:38] failure mode data sets They can run the [17:39] eval They can see the impact of those um [17:41] eval And then once they've kind of done [17:43] that loop and they're they're hitting [17:44] the percentage that they need they can [17:46] then go and give that back to the PM and [17:48] say "Hey here are the changes I made [17:49] This is the impact." The PM can then um [17:52] take that information and make some [17:54] decision about going live they can take [17:56] the those uh email metrics they can look [17:58] at the kind of wider context of what [18:00] this change might impact elsewhere in [18:01] the product um and then decide whether [18:03] to go live uh with that in production [18:07] So final takeaway just to wrap up um you [18:11] know to build a domain native element [18:12] application you need to solve the the [18:14] last mile problem This isn't solved by [18:16] just using more powerful models or more [18:17] sophisticated pipelines Uh you need what [18:19] we call an adaptive domain intelligence [18:21] engine Domain experts can power this [18:24] system by reviewing their AI outputs to [18:25] generate metrics to generate failure [18:27] modes and to generate suggested [18:28] improvements And this is really powerful [18:30] because it takes production data live [18:32] from kind of inside your customer's [18:33] context and it uses that to give your LM [18:35] product the nuance understanding of the [18:37] customer workflows and continually [18:38] iterate towards that and eek out the the [18:40] kind of final performance um performance [18:42] level And the end result is you have [18:44] this self-improving datadriven process [18:46] that can be managed by a domain expert [18:49] PM sitting in the middle [18:51] Um so thank you for your attention Um [18:55] uh I if you're interested in kind of [18:56] vertical AI applications or like evals [18:58] and AI product management more generally [18:59] I've written about that at my website [19:01] chris lovejoy.me Uh always interested to [19:04] talk about this So feel free to drop an [19:05] email at chrisanser.com And we're also [19:07] hiring as well at the moment So check [19:08] out anio.com/comp [19:10] for open roles Thank you [19:16] [Music]