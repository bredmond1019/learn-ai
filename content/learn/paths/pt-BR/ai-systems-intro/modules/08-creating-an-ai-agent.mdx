---
title: "Creating an AI Agent"
description: "Build an AI agent that connects to your MCP server and uses an LLM"
publishedAt: "2025-01-26"
authors:
  - brandonscheid
---

import { Quiz } from '@/components/quiz'
import { Callout } from '@/components/callout'
import { CodeBlock } from '@/components/code-block'

## introduction {#introduction}

Now comes the exciting part â€“ creating an AI agent that can think, understand, and take action! This agent will be the friendly face of Cozy CafÃ©, helping customers and providing insights to the owner.

Your AI agent will:
- Understand natural language questions
- Use the MCP feedback server to collect and analyze feedback
- Provide helpful responses to customers
- Generate insights for the cafÃ© owner

<Callout type="success">
**Real AI in Action**: By the end of this module, you'll have a working AI assistant that can have intelligent conversations and perform real tasks!
</Callout>

## understanding-agents {#understanding-agents}

### The Anatomy of an AI Agent

An AI agent combines intelligence (LLM) with capabilities (MCP servers) to accomplish tasks. Think of it as a smart employee who can understand instructions and use various tools.


### Key Agent Capabilities

1. **Natural Language Understanding** - Interprets what users really want
2. **Context Awareness** - Remembers conversation history
3. **Tool Usage** - Knows when and how to use MCP servers
4. **Response Generation** - Creates helpful, friendly responses

## agent-design {#agent-design}

### Designing Cozy CafÃ© Assistant

Let's design our cafÃ© assistant with specific personality and capabilities:


### Agent Behaviors

**For Customers:**
- Warm greeting and friendly conversation
- Easy feedback collection
- Answer questions about menu, hours, etc.
- Thank customers genuinely

**For CafÃ© Owner:**
- Summarize feedback trends
- Alert about issues
- Suggest improvements
- Provide business insights

## basic-agent {#basic-agent}

### Creating the Agent Structure

Let's build our AI agent step by step:

<CodeBlock language="python" filename="agents/cafe_assistant.py">
"""
Cozy CafÃ© AI Assistant
A friendly agent that helps with customer service and feedback
"""

import asyncio
import json
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Message:
    """Represents a conversation message"""
    role: str  # 'user' or 'assistant'
    content: str
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

class CafeAssistant:
    """AI Assistant for Cozy CafÃ©"""
    
    def __init__(self, name: str = "CafÃ© Assistant"):
        self.name = name
        self.conversation_history: List[Message] = []
        self.mcp_client = None  # We'll connect this later
        self.system_prompt = self._create_system_prompt()
        
    def _create_system_prompt(self) -> str:
        """Create the personality and instructions for our agent"""
        return """You are a friendly AI assistant for Cozy CafÃ©, a warm neighborhood coffee shop.
        
Your personality:
- Warm, friendly, and genuinely caring about customer experience
- Knowledgeable about coffee and cafÃ© operations
- Professional but approachable
- Always positive and solution-oriented

Your capabilities:
- Collect customer feedback using the feedback tool
- Answer questions about the cafÃ© (hours: 7 AM - 8 PM, location: 123 Main Street)
- Analyze feedback sentiment
- Provide insights to the cafÃ© owner

Always:
- Greet customers warmly
- Thank them for their feedback
- Be helpful and informative
- Use the available tools when appropriate"""
    
    async def process_message(self, user_input: str) -> str:
        """Process a user message and generate a response"""
        # Add user message to history
        user_message = Message(role="user", content=user_input)
        self.conversation_history.append(user_message)
        
        # For now, return a simple response
        # We'll add LLM integration next
        response = await self._generate_response(user_input)
        
        # Add assistant response to history
        assistant_message = Message(role="assistant", content=response)
        self.conversation_history.append(assistant_message)
        
        return response
    
    async def _generate_response(self, user_input: str) -> str:
        """Generate a response (placeholder for now)"""
        # Simple rule-based responses for testing
        user_input_lower = user_input.lower()
        
        if "hello" in user_input_lower or "hi" in user_input_lower:
            return "Hello! Welcome to Cozy CafÃ©! How can I help you today? Would you like to share feedback about your experience?"
        
        elif "feedback" in user_input_lower:
            return "I'd love to hear your feedback! Please share your thoughts about your experience at Cozy CafÃ©."
        
        elif "hours" in user_input_lower or "open" in user_input_lower:
            return "Cozy CafÃ© is open from 7:00 AM to 8:00 PM every day. We'd love to see you!"
        
        elif "location" in user_input_lower or "where" in user_input_lower:
            return "You can find us at 123 Main Street. We're the cozy spot with the red awning!"
        
        else:
            return "I'm here to help with any questions about Cozy CafÃ© or to collect your feedback. What would you like to know?"
    
    def get_conversation_summary(self) -> str:
        """Get a summary of the conversation"""
        if not self.conversation_history:
            return "No conversation yet."
        
        summary = f"Conversation with {len(self.conversation_history)} messages:\n"
        for msg in self.conversation_history[-5:]:  # Last 5 messages
            timestamp = msg.timestamp.strftime("%H:%M")
            summary += f"[{timestamp}] {msg.role}: {msg.content[:50]}...\n"
        
        return summary

# Test function
async def test_basic_agent():
    """Test our basic agent"""
    print("â˜• Testing Cozy CafÃ© Assistant...\n")
    
    agent = CafeAssistant()
    
    # Test conversations
    test_messages = [
        "Hello!",
        "What are your hours?",
        "I'd like to leave feedback",
        "Where are you located?"
    ]
    
    for message in test_messages:
        print(f"User: {message}")
        response = await agent.process_message(message)
        print(f"Assistant: {response}\n")
    
    print("\nğŸ“ Conversation Summary:")
    print(agent.get_conversation_summary())

if __name__ == "__main__":
    asyncio.run(test_basic_agent())
</CodeBlock>

<Callout type="info">
**Code Structure**: We've created a basic agent class that can handle conversations, maintain history, and respond to simple queries. Next, we'll add MCP connectivity!
</Callout>

## connecting-mcp {#connecting-mcp}

### Connecting to Your MCP Server

Now let's connect our agent to the feedback MCP server:

<CodeBlock language="python" filename="agents/mcp_client.py">
"""
MCP Client for connecting to our servers
"""

import asyncio
import json
from typing import Dict, Any, List
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

class MCPClient:
    """Client for connecting to MCP servers"""
    
    def __init__(self):
        self.sessions: Dict[str, ClientSession] = {}
        
    async def connect_to_server(self, server_name: str, command: List[str]):
        """Connect to an MCP server"""
        server_params = StdioServerParameters(
            command=command[0],
            args=command[1:] if len(command) > 1 else []
        )
        
        # Create client session
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Initialize the session
                await session.initialize()
                
                # Store session
                self.sessions[server_name] = session
                
                # Get available tools
                tools = await session.list_tools()
                print(f"Connected to {server_name}")
                print(f"Available tools: {[tool.name for tool in tools]}")
                
                return session
    
    async def call_tool(self, server_name: str, tool_name: str, arguments: Dict[str, Any]):
        """Call a tool on a specific server"""
        if server_name not in self.sessions:
            raise ValueError(f"Not connected to server: {server_name}")
        
        session = self.sessions[server_name]
        result = await session.call_tool(tool_name, arguments)
        return result

# Update the CafeAssistant to use MCP
class CafeAssistantWithMCP(CafeAssistant):
    """Enhanced CafÃ© Assistant with MCP connectivity"""
    
    def __init__(self, name: str = "CafÃ© Assistant"):
        super().__init__(name)
        self.mcp_client = MCPClient()
        self.connected = False
        
    async def connect_to_feedback_server(self):
        """Connect to the feedback MCP server"""
        try:
            # Command to run our feedback server
            command = ["python", "mcp_servers/feedback_server.py"]
            await self.mcp_client.connect_to_server("feedback", command)
            self.connected = True
            print("âœ… Connected to feedback server")
        except Exception as e:
            print(f"âŒ Failed to connect: {e}")
            self.connected = False
    
    async def collect_feedback(self, customer_name: str, feedback: str, rating: int):
        """Collect customer feedback using MCP"""
        if not self.connected:
            return {"error": "Not connected to feedback server"}
        
        try:
            result = await self.mcp_client.call_tool(
                "feedback",
                "collect_feedback",
                {
                    "customer_name": customer_name,
                    "feedback": feedback,
                    "rating": rating
                }
            )
            return result
        except Exception as e:
            return {"error": str(e)}
    
    async def analyze_sentiment(self, text: str):
        """Analyze sentiment of text using MCP"""
        if not self.connected:
            return {"error": "Not connected to feedback server"}
        
        try:
            result = await self.mcp_client.call_tool(
                "feedback",
                "analyze_sentiment",
                {"text": text}
            )
            return result
        except Exception as e:
            return {"error": str(e)}
    
    async def get_feedback_summary(self):
        """Get feedback summary using MCP"""
        if not self.connected:
            return {"error": "Not connected to feedback server"}
        
        try:
            result = await self.mcp_client.call_tool(
                "feedback",
                "get_feedback_summary",
                {}
            )
            return result
        except Exception as e:
            return {"error": str(e)}
</CodeBlock>

## adding-intelligence {#adding-intelligence}

### Adding LLM Intelligence

Now let's add real intelligence using a language model. For this example, we'll create a simple LLM interface:

<CodeBlock language="python" filename="agents/llm_integration.py">
"""
LLM Integration for intelligent responses
Note: In production, you would use a real LLM API like OpenAI or Anthropic
"""

import re
from typing import List, Dict, Any, Optional

class SimpleLLM:
    """
    Simplified LLM for demonstration
    In production, replace with actual LLM API calls
    """
    
    def __init__(self):
        self.context_window = []
        
    async def generate(self, prompt: str, system: str = None) -> str:
        """
        Simulate LLM generation with intelligent pattern matching
        Real implementation would call an API
        """
        # Parse intent from the prompt
        intent = self._parse_intent(prompt)
        
        # Generate appropriate response based on intent
        if intent == "collect_feedback":
            return self._generate_feedback_collection()
        elif intent == "analyze_feedback":
            return self._generate_analysis()
        elif intent == "greeting":
            return self._generate_greeting()
        elif intent == "question":
            return self._generate_answer(prompt)
        else:
            return self._generate_general_help()
    
    def _parse_intent(self, text: str) -> str:
        """Parse user intent from text"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ["feedback", "review", "experience"]):
            if any(word in text_lower for word in ["leave", "give", "share", "provide"]):
                return "collect_feedback"
            elif any(word in text_lower for word in ["analyze", "summary", "trends"]):
                return "analyze_feedback"
        
        if any(word in text_lower for word in ["hello", "hi", "hey", "greetings"]):
            return "greeting"
        
        if "?" in text or any(word in text_lower for word in ["what", "when", "where", "how", "why"]):
            return "question"
        
        return "general"
    
    def _generate_feedback_collection(self) -> str:
        return """I'd be happy to collect your feedback! Let me help you with that.

To proceed, I'll need:
1. Your name (or you can remain anonymous)
2. Your feedback about your experience
3. A rating from 1-5 stars

Please share your thoughts, and I'll make sure they reach our team!"""
    
    def _generate_analysis(self) -> str:
        return """I can help you analyze the feedback! Let me check our recent customer feedback and provide you with insights.

[Calling feedback analysis tool...]"""
    
    def _generate_greeting(self) -> str:
        import random
        greetings = [
            "Welcome to Cozy CafÃ©! â˜• How can I brighten your day?",
            "Hello there! It's lovely to see you at Cozy CafÃ©. What can I help you with today?",
            "Hi! Thanks for visiting Cozy CafÃ©. I'm here to help with any questions or to collect your feedback!"
        ]
        return random.choice(greetings)
    
    def _generate_answer(self, question: str) -> str:
        question_lower = question.lower()
        
        if "hours" in question_lower or "open" in question_lower:
            return "We're open from 7:00 AM to 8:00 PM every day! Perfect for your morning coffee or evening treat. â˜•"
        
        elif "location" in question_lower or "address" in question_lower:
            return "You'll find us at 123 Main Street - look for the cozy red awning! We're right in the heart of downtown."
        
        elif "menu" in question_lower:
            return "We offer a variety of coffee drinks, teas, fresh pastries, and light meals. Our baristas can also create custom drinks just for you!"
        
        else:
            return "That's a great question! While I can help with basic information about our hours, location, and feedback, for specific menu items or prices, our friendly staff would be happy to help you in person."
    
    def _generate_general_help(self) -> str:
        return """I'm here to help! I can:
â€¢ Collect your feedback about your Cozy CafÃ© experience
â€¢ Answer questions about our hours and location  
â€¢ Provide summaries of customer feedback (for staff)
â€¢ Help make your visit more enjoyable

What would you like to know or share?"""

# Enhanced agent with LLM
class IntelligentCafeAssistant(CafeAssistantWithMCP):
    """CafÃ© Assistant with LLM intelligence"""
    
    def __init__(self, name: str = "CafÃ© Assistant"):
        super().__init__(name)
        self.llm = SimpleLLM()
        
    async def _generate_response(self, user_input: str) -> str:
        """Generate intelligent response using LLM"""
        # Create context from conversation history
        context = self._build_context()
        
        # Generate prompt
        prompt = f"""
        System: {self.system_prompt}
        
        Conversation history:
        {context}
        
        User: {user_input}
        
        Generate a helpful response. If the user wants to leave feedback, guide them through the process.
        If they're asking for feedback analysis, use the available tools.
        """
        
        # Get LLM response
        response = await self.llm.generate(prompt, self.system_prompt)
        
        # Check if we need to use tools based on response
        if "[Calling feedback analysis tool...]" in response:
            # Replace with actual tool call
            summary = await self.get_feedback_summary()
            response = response.replace(
                "[Calling feedback analysis tool...]",
                f"Here's what I found: {json.dumps(summary, indent=2)}"
            )
        
        return response
    
    def _build_context(self) -> str:
        """Build context from conversation history"""
        if not self.conversation_history:
            return "No previous conversation"
        
        context = ""
        for msg in self.conversation_history[-6:]:  # Last 3 exchanges
            context += f"{msg.role.capitalize()}: {msg.content}\n"
        
        return context
</CodeBlock>

## agent-conversation {#agent-conversation}

### Making Your Agent Conversational

Let's create a complete conversational interface:

<CodeBlock language="python" filename="agents/conversation_manager.py">
"""
Conversation Manager for natural interactions
"""

import asyncio
from typing import Optional

class ConversationManager:
    """Manages natural conversations with the cafÃ© assistant"""
    
    def __init__(self, assistant: IntelligentCafeAssistant):
        self.assistant = assistant
        self.active = False
        
    async def start_conversation(self):
        """Start an interactive conversation"""
        print("â˜• Welcome to Cozy CafÃ© AI Assistant!")
        print("Type 'exit' to end the conversation")
        print("-" * 50)
        
        # Connect to MCP server
        await self.assistant.connect_to_feedback_server()
        
        self.active = True
        while self.active:
            try:
                # Get user input
                user_input = input("\nYou: ").strip()
                
                # Check for exit
                if user_input.lower() in ['exit', 'quit', 'bye']:
                    print("\nAssistant: Thank you for visiting Cozy CafÃ©! Have a wonderful day! â˜•")
                    break
                
                # Process message
                response = await self.assistant.process_message(user_input)
                print(f"\nAssistant: {response}")
                
                # Handle feedback collection if triggered
                if "please share your thoughts" in response.lower():
                    await self._collect_feedback_flow()
                    
            except KeyboardInterrupt:
                print("\n\nAssistant: Thanks for chatting! Come back soon!")
                break
            except Exception as e:
                print(f"\nAssistant: I apologize, I encountered an error: {e}")
        
        self.active = False
    
    async def _collect_feedback_flow(self):
        """Interactive feedback collection flow"""
        print("\n--- Feedback Collection ---")
        
        # Get customer name
        name = input("Your name (or press Enter for anonymous): ").strip()
        if not name:
            name = "Anonymous"
        
        # Get feedback
        print("Please share your experience:")
        feedback = input().strip()
        
        # Get rating
        while True:
            try:
                rating = int(input("Rating (1-5 stars): "))
                if 1 <= rating <= 5:
                    break
                else:
                    print("Please enter a number between 1 and 5")
            except ValueError:
                print("Please enter a valid number")
        
        # Collect feedback using MCP
        result = await self.assistant.collect_feedback(name, feedback, rating)
        
        if result.get("success"):
            # Analyze sentiment
            sentiment = await self.assistant.analyze_sentiment(feedback)
            
            print(f"\nâœ… {result['message']}")
            print(f"Sentiment: {sentiment.get('sentiment', 'unknown')} "
                  f"(confidence: {sentiment.get('confidence', 0):.0%})")
        else:
            print(f"\nâŒ Error: {result.get('error', 'Unknown error')}")

# Complete test script
async def run_cafe_assistant():
    """Run the complete cafÃ© assistant"""
    # Create assistant
    assistant = IntelligentCafeAssistant("Cozy")
    
    # Create conversation manager
    manager = ConversationManager(assistant)
    
    # Start conversation
    await manager.start_conversation()

if __name__ == "__main__":
    asyncio.run(run_cafe_assistant())
</CodeBlock>

### Putting It All Together

Create a main script to run your complete AI system:

<CodeBlock language="python" filename="main.py">
"""
Cozy CafÃ© AI System
Main entry point for running the complete system
"""

import asyncio
import sys
from agents.conversation_manager import ConversationManager
from agents.llm_integration import IntelligentCafeAssistant

async def main():
    """Main function to run the AI system"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     ğŸµ Cozy CafÃ© AI System ğŸµ        â•‘
    â•‘   Your Intelligent CafÃ© Assistant    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Create the intelligent assistant
    assistant = IntelligentCafeAssistant("Cozy")
    
    # Create conversation manager
    manager = ConversationManager(assistant)
    
    # Run the assistant
    try:
        await manager.start_conversation()
    except Exception as e:
        print(f"\nError: {e}")
        sys.exit(1)
    
    print("\nThank you for using Cozy CafÃ© AI System!")

if __name__ == "__main__":
    asyncio.run(main())
</CodeBlock>


## knowledge-check {#knowledge-check}

<Quiz
  questions={[
    {
      question: "What is the main role of an AI agent in our system?",
      options: [
        "To store data in files",
        "To understand user intent and coordinate actions using available tools",
        "To replace the MCP server",
        "To create websites"
      ],
      correctAnswer: 1,
      explanation: "AI agents understand what users want (using LLM intelligence) and coordinate actions by calling appropriate tools through MCP servers."
    },
    {
      question: "How does the agent connect to MCP servers?",
      options: [
        "Through direct database connections",
        "Using the MCP Client with stdio communication",
        "Via HTTP REST APIs only",
        "Through file system access"
      ],
      correctAnswer: 1,
      explanation: "The agent uses an MCP Client that communicates with servers through stdio (standard input/output), following the Model Context Protocol."
    },
    {
      question: "What makes our agent 'intelligent'?",
      options: [
        "It has a large database",
        "It uses an LLM to understand context and generate appropriate responses",
        "It runs very fast",
        "It has many if-else statements"
      ],
      correctAnswer: 1,
      explanation: "The agent's intelligence comes from using an LLM (Large Language Model) to understand user intent, maintain context, and generate natural responses."
    },
    {
      question: "What is the purpose of the conversation manager?",
      options: [
        "To save conversations to disk",
        "To limit conversation length",
        "To handle the interactive flow and coordinate between user, agent, and tools",
        "To translate languages"
      ],
      correctAnswer: 2,
      explanation: "The conversation manager handles the interactive flow, maintaining the conversation loop, and coordinating between user input, agent processing, and tool execution."
    }
  ]}
/>

<Callout type="success">
**Amazing Progress!** You've built an intelligent AI agent that can understand natural language, maintain conversations, and use MCP servers to perform real tasks. Next, we'll put everything together into a complete system!
</Callout>