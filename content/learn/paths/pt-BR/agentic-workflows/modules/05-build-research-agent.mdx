---
title: "Construir um Agente de Pesquisa"
description: "Projeto final que integra todos os conceitos de fluxos de trabalho agênticos para construir um agente de pesquisa abrangente capaz de pesquisa autônoma, análise e geração de relatórios com supervisão humana."
duration: "180 minutos"
difficulty: "avançado"
order: 5
pathId: "agentic-workflows"
moduleId: "build-research-agent"
type: "project"
objectives:
  - "Integrar padrões de arquitetura de agentes, planejamento, orquestração de ferramentas e design human-in-the-loop"
  - "Construir um agente de pesquisa completo com capacidades de pesquisa autônoma"
  - "Implementar planejamento sofisticado e gerenciamento de objetivos para tarefas de pesquisa"
  - "Criar orquestração robusta de ferramentas para web scraping, análise de dados e geração de relatórios"
  - "Projetar mecanismos eficazes de supervisão humana e controle de qualidade"
  - "Implantar um assistente de pesquisa funcional com tomada de decisão explicável"
prerequisites:
  - "Padrões de Arquitetura de Agentes"
  - "Construindo Sistemas de Planejamento"
  - "Orquestração de Ferramentas"
  - "Design Human-in-the-Loop"
  - "Experiência com APIs web e processamento de dados"
tags:
  - "automação-pesquisa"
  - "coleta-informações"
  - "integração-agentes"
  - "controle-qualidade"
  - "geração-relatórios"
  - "projeto-final"
version: "1.0.0"
lastUpdated: "2025-06-20"
author: "Equipe de Engenharia de IA"
estimatedCompletionTime: 270
---

# Construir um Agente de Pesquisa

Bem-vindo ao projeto final do caminho de aprendizagem de Fluxos de Trabalho de IA Agêntica! Neste módulo, você integrará tudo o que aprendeu para construir um agente de pesquisa sofisticado capaz de pesquisa autônoma, análise e geração de relatórios com supervisão humana apropriada. Este projeto demonstra como todos os conceitos—arquiteturas de agentes, sistemas de planejamento, orquestração de ferramentas e design human-in-the-loop—trabalham juntos em uma aplicação do mundo real.

## Objetivos de Aprendizagem

Ao final deste módulo, você será capaz de:

- Integrar padrões de arquitetura de agentes, planejamento, orquestração de ferramentas e design human-in-the-loop
- Construir um agente de pesquisa completo com capacidades de pesquisa autônoma
- Implementar planejamento sofisticado e gerenciamento de objetivos para tarefas de pesquisa
- Criar orquestração robusta de ferramentas para web scraping, análise de dados e geração de relatórios
- Projetar mecanismos eficazes de supervisão humana e controle de qualidade
- Implantar um assistente de pesquisa funcional com tomada de decisão explicável

## Visão Geral do Projeto

Nosso agente de pesquisa será capaz de:

1. **Pesquisa Autônoma**: Planejar e executar pesquisa abrangente sobre tópicos dados
2. **Coleta de Informações Multi-Fonte**: Usar busca web, bases de dados acadêmicas e APIs
3. **Análise Inteligente**: Processar e sintetizar informações de múltiplas fontes
4. **Controle de Qualidade**: Verificação de fatos e validação de descobertas de pesquisa
5. **Colaboração Humana**: Envolver humanos em pontos críticos de decisão
6. **Geração de Relatórios**: Criar entregas de pesquisa estruturadas e profissionais
7. **Processo Explicável**: Fornecer explicações claras da metodologia e descobertas de pesquisa

## Arquitetura do Agente de Pesquisa

### Design Central do Sistema

Nosso agente de pesquisa segue uma arquitetura híbrida combinando abordagens reativas e deliberativas:

```python
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import asyncio
import time
import logging
import json
from datetime import datetime, timedelta

# Importar todos os módulos anteriores (simplificado para demonstração)
# Na prática, estes estariam em arquivos separados
from agent_architecture import Agent, ReActAgent
from planning_systems import STRIPSPlanner, HTNPlanner, GoalManager
from tool_orchestration import ToolRegistry, ToolSelector, WorkflowOrchestrator
from human_in_loop import HumanAICoordinator, InterventionManager, ExplainableAgent

class ResearchPhase(Enum):
    PLANNING = "planning"
    INFORMATION_GATHERING = "information_gathering"
    ANALYSIS = "analysis"
    SYNTHESIS = "synthesis"
    QUALITY_CONTROL = "quality_control"
    REPORTING = "reporting"
    REVIEW = "review"

class ResearchTaskType(Enum):
    LITERATURE_REVIEW = "literature_review"
    MARKET_ANALYSIS = "market_analysis"
    COMPETITIVE_INTELLIGENCE = "competitive_intelligence"
    TECHNICAL_ANALYSIS = "technical_analysis"
    TREND_ANALYSIS = "trend_analysis"
    FACT_CHECKING = "fact_checking"

@dataclass
class ResearchTopic:
    """Representa um tópico de pesquisa com requisitos e restrições"""
    topic_id: str
    title: str
    description: str
    research_type: ResearchTaskType
    scope: str  # "broad", "focused", "deep"
    deadline: datetime
    quality_requirements: Dict[str, Any]
    constraints: Dict[str, Any] = field(default_factory=dict)
    target_audience: str = "general"
    deliverable_format: str = "report"  # "report", "presentation", "summary"
    
class ResearchAgent:
    """Agente de pesquisa principal integrando todos os subsistemas"""
    
    def __init__(self, agent_name: str = "ResearchAgent"):
        self.agent_name = agent_name
        
        # Subsistemas principais
        self.tool_registry = ToolRegistry()
        self.tool_selector = ToolSelector(self.tool_registry)
        self.orchestrator = WorkflowOrchestrator(self.tool_registry, self.tool_selector)
        self.planner = ResearchPlanner()
        self.quality_controller = QualityController()
        self.report_generator = ReportGenerator()
        self.intervention_manager = InterventionManager()
        self.explainable_agent = ExplainableAgent(agent_name)
        
        # Estado da pesquisa
        self.active_research: Dict[str, Any] = {}
        self.research_history: List[Dict[str, Any]] = []
        self.knowledge_base: Dict[str, Any] = {}
        
        # Inicializar ferramentas e capacidades
        self._setup_research_tools()
        self._setup_intervention_points()
        
    def _setup_research_tools(self):
        """Inicializar ferramentas específicas de pesquisa"""
        
        # Ferramentas de pesquisa web
        web_search_tool = WebSearchTool()
        academic_search_tool = AcademicSearchTool()
        web_scraper_tool = WebScrapingTool()
        
        # Ferramentas de processamento de dados
        text_analyzer_tool = TextAnalysisTool()
        data_processor_tool = DataProcessingTool()
        fact_checker_tool = FactCheckingTool()
        
        # Ferramentas de geração de relatórios
        summarizer_tool = SummarizerTool()
        visualizer_tool = VisualizationTool()
        document_generator_tool = DocumentGeneratorTool()
        
        # Registrar todas as ferramentas
        tools = [
            web_search_tool, academic_search_tool, web_scraper_tool,
            text_analyzer_tool, data_processor_tool, fact_checker_tool,
            summarizer_tool, visualizer_tool, document_generator_tool
        ]
        
        for tool in tools:
            asyncio.create_task(self.tool_registry.register_tool(tool))
    
    def _setup_intervention_points(self):
        """Configurar pontos de intervenção humana para qualidade de pesquisa"""
        
        # Intervenção de fontes de baixa qualidade
        low_quality_point = InterventionPoint(
            trigger_type=InterventionTrigger.LOW_CONFIDENCE,
            condition=lambda ctx: ctx.get("source_credibility", 1.0) < 0.6,
            urgency=InterventionUrgency.MEDIUM,
            description="Fontes de baixa credibilidade detectadas",
            escalation_path=["research_supervisor", "domain_expert"],
            timeout=1800.0,  # 30 minutos
            fallback_action="conservative",
            required_expertise=["research_methodology", "source_evaluation"]
        )
        
        # Intervenção de informações conflitantes
        conflict_point = InterventionPoint(
            trigger_type=InterventionTrigger.ERROR_DETECTION,
            condition=lambda ctx: ctx.get("information_conflicts", []),
            urgency=InterventionUrgency.HIGH,
            description="Informações conflitantes detectadas entre fontes",
            escalation_path=["fact_checker", "domain_expert", "research_supervisor"],
            timeout=3600.0,  # 1 hora
            fallback_action="defer",
            required_expertise=["fact_checking", "domain_knowledge"]
        )
        
        # Intervenção de expansão de escopo de pesquisa
        scope_point = InterventionPoint(
            trigger_type=InterventionTrigger.STAKEHOLDER_REQUEST,
            condition=lambda ctx: ctx.get("scope_expansion_needed", False),
            urgency=InterventionUrgency.MEDIUM,
            description="Escopo da pesquisa pode precisar de expansão",
            escalation_path=["project_manager", "stakeholder"],
            timeout=7200.0,  # 2 horas
            fallback_action="proceed_as_planned",
            required_expertise=["project_management", "research_planning"]
        )
        
        self.intervention_manager.register_intervention_point(low_quality_point)
        self.intervention_manager.register_intervention_point(conflict_point)
        self.intervention_manager.register_intervention_point(scope_point)
        
        # Registrar especialistas humanos
        self.intervention_manager.register_human_expert(
            "research_supervisor", ["research_methodology", "quality_control"], True
        )
        self.intervention_manager.register_human_expert(
            "domain_expert", ["domain_knowledge", "source_evaluation"], True
        )
        self.intervention_manager.register_human_expert(
            "fact_checker", ["fact_checking", "verification"], True
        )
    
    async def conduct_research(self, research_topic: ResearchTopic) -> Dict[str, Any]:
        """Método principal para conduzir pesquisa abrangente"""
        
        research_id = f"research_{int(time.time() * 1000)}"
        
        try:
            # Inicializar sessão de pesquisa
            research_session = {
                "research_id": research_id,
                "topic": research_topic,
                "start_time": datetime.now(),
                "current_phase": ResearchPhase.PLANNING,
                "findings": {},
                "sources": [],
                "quality_metrics": {},
                "interventions": [],
                "deliverables": {}
            }
            
            self.active_research[research_id] = research_session
            
            # Fase 1: Planejamento de Pesquisa
            await self._planning_phase(research_session)
            
            # Fase 2: Coleta de Informações
            await self._information_gathering_phase(research_session)
            
            # Fase 3: Análise e Síntese
            await self._analysis_phase(research_session)
            
            # Fase 4: Controle de Qualidade
            await self._quality_control_phase(research_session)
            
            # Fase 5: Geração de Relatório
            await self._reporting_phase(research_session)
            
            # Fase 6: Revisão Humana
            await self._review_phase(research_session)
            
            # Finalizar pesquisa
            research_session["end_time"] = datetime.now()
            research_session["total_duration"] = (
                research_session["end_time"] - research_session["start_time"]
            ).total_seconds()
            
            # Mover para histórico
            self.research_history.append(research_session)
            if research_id in self.active_research:
                del self.active_research[research_id]
            
            logging.info(f"Pesquisa {research_id} completada com sucesso")
            
            return {
                "success": True,
                "research_id": research_id,
                "deliverables": research_session["deliverables"],
                "quality_metrics": research_session["quality_metrics"],
                "duration": research_session["total_duration"]
            }
            
        except Exception as e:
            logging.error(f"Pesquisa {research_id} falhou: {e}")
            return {
                "success": False,
                "research_id": research_id,
                "error": str(e),
                "partial_results": research_session.get("findings", {})
            }
    
    async def _planning_phase(self, research_session: Dict[str, Any]):
        """Fase 1: Planejar a abordagem de pesquisa"""
        
        research_session["current_phase"] = ResearchPhase.PLANNING
        logging.info(f"Iniciando fase de planejamento para {research_session['research_id']}")
        
        topic = research_session["topic"]
        
        # Criar plano de pesquisa
        research_plan = await self.planner.create_research_plan(topic)
        research_session["plan"] = research_plan
        
        # Configurar objetivos e marcos
        goals = self.planner.extract_research_goals(topic, research_plan)
        research_session["goals"] = goals
        
        # Verificar intervenções de planejamento
        planning_context = {
            "research_type": topic.research_type.value,
            "scope": topic.scope,
            "complexity": research_plan.get("complexity", "medium"),
            "estimated_duration": research_plan.get("estimated_duration", 0)
        }
        
        intervention = await self.intervention_manager.check_intervention_needed(planning_context)
        if intervention:
            research_session["interventions"].append(intervention)
        
        logging.info(f"Fase de planejamento concluída para {research_session['research_id']}")
    
    async def _information_gathering_phase(self, research_session: Dict[str, Any]):
        """Fase 2: Coletar informações de múltiplas fontes"""
        
        research_session["current_phase"] = ResearchPhase.INFORMATION_GATHERING
        logging.info(f"Iniciando coleta de informações para {research_session['research_id']}")
        
        topic = research_session["topic"]
        plan = research_session["plan"]
        
        # Executar fluxo de trabalho de coleta de informações
        gathering_workflow = self._create_information_gathering_workflow(topic, plan)
        workflow_result = await self.orchestrator.execute_workflow(gathering_workflow)
        
        # Processar e armazenar descobertas
        raw_information = self._extract_workflow_outputs(workflow_result)
        processed_information = await self._process_raw_information(raw_information)
        
        research_session["findings"]["raw_information"] = raw_information
        research_session["findings"]["processed_information"] = processed_information
        research_session["sources"] = self._extract_sources(raw_information)
        
        # Verificar qualidade da informação
        quality_context = {
            "source_count": len(research_session["sources"]),
            "source_credibility": self._calculate_average_credibility(research_session["sources"]),
            "information_coverage": self._assess_information_coverage(processed_information, topic)
        }
        
        intervention = await self.intervention_manager.check_intervention_needed(quality_context)
        if intervention:
            research_session["interventions"].append(intervention)
        
        logging.info(f"Coleta de informações concluída para {research_session['research_id']}")
    
    async def _analysis_phase(self, research_session: Dict[str, Any]):
        """Fase 3: Analisar e sintetizar informações"""
        
        research_session["current_phase"] = ResearchPhase.ANALYSIS
        logging.info(f"Iniciando fase de análise para {research_session['research_id']}")
        
        processed_info = research_session["findings"]["processed_information"]
        topic = research_session["topic"]
        
        # Realizar vários tipos de análise
        analysis_results = {}
        
        # Análise de tendências
        if topic.research_type in [ResearchTaskType.MARKET_ANALYSIS, ResearchTaskType.TREND_ANALYSIS]:
            analysis_results["trends"] = await self._analyze_trends(processed_info)
        
        # Análise comparativa
        if topic.research_type == ResearchTaskType.COMPETITIVE_INTELLIGENCE:
            analysis_results["competitive_landscape"] = await self._analyze_competition(processed_info)
        
        # Análise técnica
        if topic.research_type == ResearchTaskType.TECHNICAL_ANALYSIS:
            analysis_results["technical_insights"] = await self._analyze_technical_aspects(processed_info)
        
        # Síntese geral
        analysis_results["synthesis"] = await self._synthesize_information(processed_info, topic)
        
        # Identificar insights-chave e conclusões
        analysis_results["key_insights"] = await self._extract_key_insights(analysis_results)
        analysis_results["conclusions"] = await self._draw_conclusions(analysis_results, topic)
        
        research_session["findings"]["analysis"] = analysis_results
        
        # Verificar informações conflitantes
        conflicts = self._detect_information_conflicts(analysis_results)
        if conflicts:
            conflict_context = {
                "information_conflicts": conflicts,
                "conflict_severity": len(conflicts)
            }
            
            intervention = await self.intervention_manager.check_intervention_needed(conflict_context)
            if intervention:
                research_session["interventions"].append(intervention)
        
        logging.info(f"Fase de análise concluída para {research_session['research_id']}")
    
    async def _quality_control_phase(self, research_session: Dict[str, Any]):
        """Fase 4: Controle de qualidade e validação"""
        
        research_session["current_phase"] = ResearchPhase.QUALITY_CONTROL
        logging.info(f"Iniciando controle de qualidade para {research_session['research_id']}")
        
        # Executar verificações abrangentes de qualidade
        quality_results = await self.quality_controller.assess_research_quality(
            research_session["findings"],
            research_session["sources"],
            research_session["topic"]
        )
        
        research_session["quality_metrics"] = quality_results
        
        # Verificar se a qualidade atende aos requisitos
        topic = research_session["topic"]
        quality_requirements = topic.quality_requirements
        
        quality_issues = self._identify_quality_issues(quality_results, quality_requirements)
        
        if quality_issues:
            # Tentar resolver problemas de qualidade
            improvements = await self._improve_research_quality(research_session, quality_issues)
            research_session["quality_improvements"] = improvements
        
        logging.info(f"Controle de qualidade concluído para {research_session['research_id']}")
    
    async def _reporting_phase(self, research_session: Dict[str, Any]):
        """Fase 5: Gerar entregas de pesquisa"""
        
        research_session["current_phase"] = ResearchPhase.REPORTING
        logging.info(f"Iniciando geração de relatório para {research_session['research_id']}")
        
        topic = research_session["topic"]
        findings = research_session["findings"]
        
        # Gerar entregas com base nos requisitos
        deliverables = {}
        
        if topic.deliverable_format in ["report", "comprehensive"]:
            report = await self.report_generator.generate_research_report(
                topic, findings, research_session["sources"]
            )
            deliverables["report"] = report
        
        if topic.deliverable_format in ["presentation", "comprehensive"]:
            presentation = await self.report_generator.generate_presentation(
                topic, findings, research_session["sources"]
            )
            deliverables["presentation"] = presentation
        
        if topic.deliverable_format in ["summary", "comprehensive"]:
            summary = await self.report_generator.generate_executive_summary(
                topic, findings, research_session["sources"]
            )
            deliverables["summary"] = summary
        
        # Gerar visualizações se aplicável
        if findings.get("analysis", {}).get("trends"):
            visualizations = await self.report_generator.generate_visualizations(
                findings["analysis"]
            )
            deliverables["visualizations"] = visualizations
        
        research_session["deliverables"] = deliverables
        
        logging.info(f"Geração de relatório concluída para {research_session['research_id']}")
    
    async def _review_phase(self, research_session: Dict[str, Any]):
        """Fase 6: Revisão humana e aprovação final"""
        
        research_session["current_phase"] = ResearchPhase.REVIEW
        logging.info(f"Iniciando fase de revisão para {research_session['research_id']}")
        
        # Preparar pacote de revisão
        review_package = {
            "research_summary": self._create_research_summary(research_session),
            "quality_metrics": research_session["quality_metrics"],
            "deliverables": research_session["deliverables"],
            "methodology": research_session.get("plan", {}),
            "sources": research_session["sources"]
        }
        
        # Verificar se revisão humana é necessária
        topic = research_session["topic"]
        quality_score = research_session["quality_metrics"].get("overall_score", 0.5)
        
        requires_review = (
            quality_score < 0.8 or
            topic.research_type in [ResearchTaskType.FACT_CHECKING] or
            len(research_session["interventions"]) > 0 or
            topic.quality_requirements.get("human_review_required", False)
        )
        
        if requires_review:
            # Solicitar revisão humana
            review_context = {
                "requires_human_review": True,
                "quality_score": quality_score,
                "research_complexity": topic.scope
            }
            
            intervention = await self.intervention_manager.check_intervention_needed(review_context)
            if intervention:
                research_session["interventions"].append(intervention)
                
                # Aguardar feedback humano
                # Na prática, isso integraria com um sistema real de revisão
                review_feedback = await self._simulate_human_review(review_package)
                research_session["review_feedback"] = review_feedback
                
                # Aplicar feedback de revisão
                if review_feedback.get("requires_changes"):
                    await self._apply_review_feedback(research_session, review_feedback)
        
        logging.info(f"Fase de revisão concluída para {research_session['research_id']}")
    
    def get_research_status(self, research_id: str) -> Optional[Dict[str, Any]]:
        """Obter status atual do projeto de pesquisa"""
        
        if research_id in self.active_research:
            session = self.active_research[research_id]
            return {
                "research_id": research_id,
                "status": "active",
                "current_phase": session["current_phase"].value,
                "progress": self._calculate_progress(session),
                "estimated_completion": self._estimate_completion_time(session),
                "quality_score": session.get("quality_metrics", {}).get("overall_score", 0),
                "intervention_count": len(session.get("interventions", []))
            }
        
        # Verificar pesquisa concluída
        for session in self.research_history:
            if session["research_id"] == research_id:
                return {
                    "research_id": research_id,
                    "status": "completed",
                    "completion_time": session["end_time"],
                    "duration": session["total_duration"],
                    "quality_score": session.get("quality_metrics", {}).get("overall_score", 0),
                    "deliverables": list(session.get("deliverables", {}).keys())
                }
        
        return None
    
    def _calculate_progress(self, session: Dict[str, Any]) -> float:
        """Calcular porcentagem de progresso da pesquisa"""
        phase_weights = {
            ResearchPhase.PLANNING: 0.15,
            ResearchPhase.INFORMATION_GATHERING: 0.30,
            ResearchPhase.ANALYSIS: 0.25,
            ResearchPhase.QUALITY_CONTROL: 0.15,
            ResearchPhase.REPORTING: 0.10,
            ResearchPhase.REVIEW: 0.05
        }
        
        current_phase = session["current_phase"]
        completed_weight = 0.0
        
        for phase, weight in phase_weights.items():
            if phase.value < current_phase.value:
                completed_weight += weight
            elif phase == current_phase:
                completed_weight += weight * 0.5  # Assumir 50% de conclusão da fase atual
                break
        
        return min(1.0, completed_weight)

# Implementações de ferramentas específicas de pesquisa
class WebSearchTool(Tool):
    """Ferramenta aprimorada de busca web para pesquisa"""
    
    def __init__(self):
        metadata = ToolMetadata(
            name="web_search",
            version="2.0.0",
            description="Busca web avançada com pontuação de credibilidade",
            category=ToolCategory.WEB_SCRAPING,
            capability=ToolCapability(
                input_types=["query", "search_parameters"],
                output_types=["search_results", "credibility_scores"],
                required_params=["query"],
                optional_params=["num_results", "date_range", "source_filter"],
                max_concurrent_calls=3,
                average_execution_time=2.0
            )
        )
        super().__init__(metadata)
    
    async def execute(self, **kwargs) -> Any:
        """Executar busca web avançada"""
        start_time = time.time()
        self.call_count += 1
        
        try:
            query = kwargs.get("query")
            num_results = kwargs.get("num_results", 10)
            date_range = kwargs.get("date_range", "any")
            
            # Simular busca avançada (na prática, usar APIs reais de busca)
            await asyncio.sleep(1.0)
            
            # Resultados de busca simulados com pontuação de credibilidade
            results = []
            for i in range(num_results):
                result = {
                    "title": f"Resultado de Busca {i+1} para '{query}'",
                    "url": f"https://example{i+1}.com/article",
                    "snippet": f"Informação relevante sobre {query}...",
                    "source": f"example{i+1}.com",
                    "date": "2024-01-15",
                    "credibility_score": 0.7 + (i % 3) * 0.1,  # Variar credibilidade
                    "relevance_score": 0.9 - (i * 0.05)
                }
                results.append(result)
            
            execution_time = time.time() - start_time
            self.total_execution_time += execution_time
            
            return {
                "query": query,
                "results": results,
                "total_results": len(results),
                "average_credibility": sum(r["credibility_score"] for r in results) / len(results),
                "execution_time": execution_time
            }
            
        except Exception as e:
            self.error_count += 1
            execution_time = time.time() - start_time
            self.total_execution_time += execution_time
            raise e
    
    async def health_check(self) -> bool:
        """Verificar se busca web está disponível"""
        try:
            await asyncio.sleep(0.1)
            return True
        except:
            return False

# Ferramentas adicionais de pesquisa seriam implementadas de forma similar...
# Por brevidade, incluirei métodos-chave na classe principal

class ResearchPlanner:
    """Planejador especializado para tarefas de pesquisa"""
    
    async def create_research_plan(self, topic: ResearchTopic) -> Dict[str, Any]:
        """Criar plano abrangente de pesquisa"""
        
        plan = {
            "research_id": f"plan_{int(time.time())}",
            "topic": topic.title,
            "research_type": topic.research_type.value,
            "approach": self._determine_research_approach(topic),
            "phases": self._plan_research_phases(topic),
            "resources": self._identify_required_resources(topic),
            "timeline": self._create_timeline(topic),
            "quality_checkpoints": self._define_quality_checkpoints(topic),
            "risk_mitigation": self._identify_risks_and_mitigation(topic)
        }
        
        return plan
    
    def _determine_research_approach(self, topic: ResearchTopic) -> str:
        """Determinar a melhor abordagem de pesquisa para o tópico"""
        
        if topic.research_type == ResearchTaskType.LITERATURE_REVIEW:
            return "systematic_review"
        elif topic.research_type == ResearchTaskType.MARKET_ANALYSIS:
            return "multi_source_analysis"
        elif topic.research_type == ResearchTaskType.COMPETITIVE_INTELLIGENCE:
            return "competitive_benchmarking"
        elif topic.research_type == ResearchTaskType.FACT_CHECKING:
            return "verification_focused"
        else:
            return "comprehensive_analysis"

class QualityController:
    """Controle de qualidade e validação de pesquisa"""
    
    async def assess_research_quality(self, findings: Dict[str, Any], 
                                    sources: List[Dict[str, Any]], 
                                    topic: ResearchTopic) -> Dict[str, Any]:
        """Avaliar qualidade geral da pesquisa"""
        
        quality_metrics = {}
        
        # Avaliação de qualidade das fontes
        quality_metrics["source_quality"] = self._assess_source_quality(sources)
        
        # Completude da informação
        quality_metrics["completeness"] = self._assess_completeness(findings, topic)
        
        # Verificação de consistência
        quality_metrics["consistency"] = self._check_consistency(findings)
        
        # Avaliação de objetividade
        quality_metrics["objectivity"] = self._assess_objectivity(findings)
        
        # Atualidade
        quality_metrics["currency"] = self._assess_currency(sources)
        
        # Calcular pontuação geral
        weights = {
            "source_quality": 0.25,
            "completeness": 0.25,
            "consistency": 0.20,
            "objectivity": 0.15,
            "currency": 0.15
        }
        
        overall_score = sum(
            quality_metrics[metric] * weights[metric] 
            for metric in weights.keys()
        )
        
        quality_metrics["overall_score"] = overall_score
        quality_metrics["quality_grade"] = self._assign_quality_grade(overall_score)
        
        return quality_metrics
    
    def _assess_source_quality(self, sources: List[Dict[str, Any]]) -> float:
        """Avaliar a qualidade das fontes de pesquisa"""
        if not sources:
            return 0.0
        
        total_credibility = sum(source.get("credibility_score", 0.5) for source in sources)
        return total_credibility / len(sources)
    
    def _assign_quality_grade(self, score: float) -> str:
        """Atribuir nota de qualidade baseada na pontuação"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"

class ReportGenerator:
    """Gerar entregas de pesquisa"""
    
    async def generate_research_report(self, topic: ResearchTopic, 
                                     findings: Dict[str, Any], 
                                     sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Gerar relatório abrangente de pesquisa"""
        
        report = {
            "title": f"Relatório de Pesquisa: {topic.title}",
            "executive_summary": self._create_executive_summary(findings),
            "methodology": self._describe_methodology(topic),
            "findings": self._format_findings(findings),
            "analysis": self._format_analysis(findings.get("analysis", {})),
            "conclusions": self._format_conclusions(findings.get("analysis", {})),
            "recommendations": self._generate_recommendations(findings, topic),
            "sources": self._format_sources(sources),
            "appendices": self._create_appendices(findings),
            "metadata": {
                "generated_date": datetime.now().isoformat(),
                "research_type": topic.research_type.value,
                "scope": topic.scope,
                "target_audience": topic.target_audience
            }
        }
        
        return report
    
    def _create_executive_summary(self, findings: Dict[str, Any]) -> str:
        """Criar resumo executivo da pesquisa"""
        
        summary_parts = []
        
        # Insights principais
        if "analysis" in findings and "key_insights" in findings["analysis"]:
            insights = findings["analysis"]["key_insights"]
            summary_parts.append(f"Insights Principais: {'; '.join(insights[:3])}")
        
        # Conclusões principais
        if "analysis" in findings and "conclusions" in findings["analysis"]:
            conclusions = findings["analysis"]["conclusions"]
            summary_parts.append(f"Conclusões: {'; '.join(conclusions[:2])}")
        
        # Cobertura de dados
        info_count = len(findings.get("processed_information", []))
        summary_parts.append(f"Baseado em análise de {info_count} fontes de informação")
        
        return ". ".join(summary_parts) + "."

# Exemplo de uso e demonstração
async def demonstrate_research_agent():
    """Demonstrar o agente de pesquisa completo"""
    
    # Criar agente de pesquisa
    agent = ResearchAgent("AdvancedResearchAgent")
    
    # Definir tópico de pesquisa
    research_topic = ResearchTopic(
        topic_id="ai_trends_2024",
        title="Tendências de Tecnologia de IA em 2024",
        description="Análise abrangente de tecnologias emergentes de IA, adoção de mercado e previsões futuras para 2024",
        research_type=ResearchTaskType.TREND_ANALYSIS,
        scope="broad",
        deadline=datetime.now() + timedelta(hours=4),
        quality_requirements={
            "minimum_sources": 10,
            "credibility_threshold": 0.7,
            "human_review_required": True
        },
        target_audience="executivos_tecnologia",
        deliverable_format="comprehensive"
    )
    
    print(f"Iniciando pesquisa sobre: {research_topic.title}")
    
    # Conduzir pesquisa
    result = await agent.conduct_research(research_topic)
    
    if result["success"]:
        print(f"Pesquisa concluída com sucesso!")
        print(f"ID da Pesquisa: {result['research_id']}")
        print(f"Duração: {result['duration']:.2f} segundos")
        print(f"Pontuação de Qualidade: {result['quality_metrics'].get('overall_score', 0):.2f}")
        print(f"Entregas: {list(result['deliverables'].keys())}")
    else:
        print(f"Pesquisa falhou: {result['error']}")
    
    # Obter status da pesquisa
    status = agent.get_research_status(result["research_id"])
    print(f"Status Final: {status}")

# Executar a demonstração
# await demonstrate_research_agent()
```

## Quiz Interativo

Teste sua compreensão sobre construção de agentes de pesquisa:

### Pergunta 1
Qual padrão de arquitetura é mais adequado para um agente de pesquisa que precisa tanto de respostas reativas quanto planejamento complexo?

A) Puramente reativo
B) Puramente deliberativo
C) Arquitetura híbrida
D) Sistema multi-agente

**Resposta: C) Arquitetura híbrida**

Agentes de pesquisa precisam de capacidades reativas para respostas imediatas e planejamento deliberativo para estratégias complexas de pesquisa, tornando a arquitetura híbrida ideal.

### Pergunta 2
Qual é o ponto de intervenção mais crítico para qualidade de pesquisa?

A) Aprovação da fase de planejamento
B) Validação de credibilidade de fontes de informação
C) Revisão de formatação de relatório
D) Gerenciamento de cronograma

**Resposta: B) Validação de credibilidade de fontes de informação**

A credibilidade das fontes impacta diretamente a qualidade e confiabilidade da pesquisa, tornando-a o ponto de intervenção mais crítico para manter a integridade da pesquisa.

### Pergunta 3
Qual métrica de qualidade é mais importante para entregas de pesquisa?

A) Completude da cobertura de informações
B) Velocidade de conclusão da pesquisa
C) Número de fontes consultadas
D) Comprimento do relatório final

**Resposta: A) Completude da cobertura de informações**

A completude garante que a pesquisa aborda adequadamente o escopo e requisitos do tópico, fornecendo insights abrangentes e confiáveis.

## Exercícios Práticos

### Exercício 1: Implementação Básica de Agente de Pesquisa
**Tempo: 60 minutos**

Construa um agente de pesquisa simplificado com:
1. Capacidades básicas de planejamento para tarefas de pesquisa
2. Busca web simples e coleta de informações
3. Avaliação básica de qualidade de fontes
4. Geração simples de relatório

### Exercício 2: Recursos Avançados de Pesquisa
**Tempo: 90 minutos**

Aprimore seu agente de pesquisa com:
1. Orquestração de informações multi-fonte
2. Detecção e resolução de conflitos
3. Pontos de intervenção humana
4. Metodologia de pesquisa explicável
5. Mecanismos de controle de qualidade

### Exercício 3: Implantação Completa do Sistema de Pesquisa
**Tempo: 120 minutos**

Implante um agente de pesquisa pronto para produção apresentando:
1. Orquestração completa de fluxo de trabalho
2. Controle abrangente de qualidade
3. Múltiplos formatos de entrega
4. Integração de revisão humana
5. Monitoramento de desempenho e análises
6. Interface de usuário para solicitações de pesquisa

## Resumo

Construir um agente de pesquisa integra todos os conceitos deste caminho de aprendizagem:

- **Arquitetura de Agente**: Design híbrido combinando abordagens reativas e deliberativas
- **Sistemas de Planejamento**: Planejamento especializado de pesquisa com gerenciamento de objetivos
- **Orquestração de Ferramentas**: Coordenação de múltiplas ferramentas de coleta e análise de informações
- **Human-in-the-Loop**: Pontos estratégicos de intervenção para controle de qualidade e supervisão
- **Garantia de Qualidade**: Mecanismos abrangentes de validação e verificação de fatos
- **IA Explicável**: Metodologia transparente de pesquisa e tomada de decisão

Este projeto final demonstra como sistemas agênticos sofisticados podem aumentar as capacidades humanas em domínios complexos como pesquisa, fornecendo tanto automação quanto supervisão humana apropriada.

## Parabéns!

Você completou com sucesso o caminho de aprendizagem de Fluxos de Trabalho de IA Agêntica! Agora você tem o conhecimento e habilidades para:

- Projetar e implementar arquiteturas sofisticadas de agentes de IA
- Construir sistemas de planejamento para tarefas complexas de múltiplas etapas
- Orquestrar múltiplas ferramentas e serviços efetivamente
- Integrar expertise e supervisão humana apropriadamente
- Criar sistemas de IA explicáveis e confiáveis

Essas habilidades formam a base para construir sistemas de IA agêntica prontos para produção que podem lidar com a complexidade do mundo real mantendo confiabilidade, transparência e controle humano.

## Próximos Passos

Continue sua jornada de engenharia de IA com o caminho de aprendizagem **Sistemas de IA em Produção**, onde você aprenderá a implantar, monitorar, escalar e proteger sistemas de IA em ambientes de produção.

---

## Recursos Adicionais

- [Melhores Práticas de Metodologia de Pesquisa](https://libguides.usc.edu/writingguide/researchprocess)
- [Avaliação de Qualidade de Informação](https://guides.library.harvard.edu/c.php?g=310264&p=2071003)
- [Ferramentas Automatizadas de Pesquisa](https://www.semanticscholar.org/)
- [Padrões de Desenvolvimento de Agentes de IA](https://www.oreilly.com/library/view/building-intelligent-agents/)
- [Ética em Pesquisa e IA](https://www.ieee.org/about/corporate/governance/p7000.html)