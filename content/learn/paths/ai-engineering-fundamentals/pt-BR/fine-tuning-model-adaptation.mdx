# Fine-tuning e Adaptação de Modelo

## when-to-fine-tune

Fine-tuning é poderoso mas nem sempre necessário. Entender quando fazer fine-tuning versus quando usar engenharia de prompts é crucial para desenvolvimento eficiente de sistemas de IA.

<Callout type="info">
Fine-tuning deve ser seu último recurso, não sua primeira escolha. Comece com engenharia de prompts, depois considere RAG, e só faça fine-tuning quando outras abordagens ficarem aquém.
</Callout>

### Framework de Decisão

<Diagram>
graph TD
    A[Requisitos da Tarefa] --> B{Suficiente com prompting?}
    B -->|Sim| C[Use Engenharia de Prompts]
    B -->|Não| D{Precisa de conhecimento externo?}
    D -->|Sim| E[Implemente Sistema RAG]
    D -->|Não| F{Formato/estilo consistente necessário?}
    F -->|Sim| G[Considere Fine-tuning]
    F -->|Não| H{Terminologia específica do domínio?}
    H -->|Sim| G
    H -->|Não| I[Revisite Engenharia de Prompts]
    G --> J{Tem dataset de qualidade?}
    J -->|Sim| K[Prossiga com Fine-tuning]
    J -->|Não| L[Colete e Rotule Dados]
</Diagram>

### Quando Fine-tuning Vale a Pena

**✅ Bons Candidatos para Fine-tuning:**

1. **Requisitos de Formato Consistente**
   - Saída estruturada (JSON, XML) que deve ser perfeita
   - Estilo ou tom de escrita específico
   - Padrões de formatação específicos do domínio

2. **Expertise de Domínio**
   - Terminologia médica, jurídica ou técnica
   - Conhecimento específico da indústria
   - Padrões de raciocínio especializados

3. **Otimização de Custo**
   - Prompts mais curtos após fine-tuning
   - Uso reduzido de tokens
   - Melhor performance com modelos menores

4. **Requisitos de Latência**
   - Aplicações em tempo real
   - Cenários de deployment edge
   - Chamadas de API de alta frequência

<CodeExample language="python">
# Exemplo: Assistente de diagnóstico médico
# Antes do fine-tuning: Prompt longo com contexto médico
long_prompt = """
Você é um assistente de IA médico. Siga estas diretrizes:
- Sempre inclua diagnóstico diferencial
- Use códigos ICD-10 quando aplicável
- Considere histórico do paciente e sintomas
- Recomende exames adicionais quando apropriado
- Nunca forneça diagnóstico definitivo sem exame adequado

Paciente apresenta: {symptoms}
Histórico médico: {history}
Exame físico: {exam_findings}

Por favor forneça análise seguindo protocolo médico...
"""

# Após fine-tuning: Prompt muito mais curto e especializado
short_prompt = "Paciente: {symptoms} | Histórico: {history} | Exame: {exam_findings}"
# Modelo com fine-tuning já conhece protocolos médicos e formatação
</CodeExample>

### Quando Evitar Fine-tuning

**❌ Candidatos Ruins para Fine-tuning:**

1. **Requisitos que Mudam Frequentemente**
   - Funcionalidades experimentais
   - Casos de uso em rápida evolução
   - Cenários de teste A/B

2. **Dados Limitados**
   - < 1.000 exemplos de alta qualidade
   - Datasets desbalanceados
   - Anotações de baixa qualidade

3. **Tarefas de Conhecimento Geral**
   - Raciocínio de senso comum
   - Q&A básico
   - Assistência geral de escrita

<CodeExample language="python">
# Exemplo: Não faça fine-tuning para isso
# Isso é melhor tratado com engenharia de prompts + RAG

def customer_support_bad_example():
    """
    Não faça fine-tuning para suporte ao cliente que precisa:
    - Acessar informação atual de produtos
    - Lidar com atualizações de política
    - Responder a lançamentos de novos produtos
    - Adaptar a promoções sazonais
    """
    # Isso muda muito frequentemente para fine-tuning ser efetivo
    pass

def customer_support_good_example():
    """
    Em vez disso, use RAG + prompts bem elaborados:
    """
    return {
        'knowledge_base': 'current_product_info',
        'prompt_template': 'professional_support_style',
        'retrieval_system': 'semantic_search',
        'fallback': 'human_escalation'
    }
</CodeExample>

## preparing-datasets

Dados de qualidade são a fundação de fine-tuning bem-sucedido. Dados ruins levam a modelos ruins, independentemente da técnica.

<Callout type="warning">
O princípio "lixo entra, lixo sai" é especialmente verdadeiro para fine-tuning. Gaste 80% do seu tempo na qualidade dos dados, 20% no treinamento real.
</Callout>

### Requisitos de Dataset

**Requisitos Mínimos:**
- 1.000+ exemplos de alta qualidade
- Formatação consistente
- Representação balanceada
- Relações claras input-output

**Recomendado:**
- 5.000+ exemplos para uso em produção
- Múltiplos anotadores para controle de qualidade
- Auditorias e atualizações regulares de dados

### Estratégias de Coleta de Dados

<CodeExample language="python">
import pandas as pd
from typing import List, Dict, Tuple
import json

class DatasetBuilder:
    def __init__(self):
        self.examples = []
        self.quality_metrics = {}
        
    def add_example(self, input_text: str, output_text: str, 
                   source: str = "manual", confidence: float = 1.0):
        """Adiciona um exemplo de treinamento com metadados"""
        example = {
            'input': input_text.strip(),
            'output': output_text.strip(),
            'source': source,
            'confidence': confidence,
            'length_input': len(input_text.split()),
            'length_output': len(output_text.split()),
            'created_at': pd.Timestamp.now()
        }
        self.examples.append(example)
        
    def from_user_interactions(self, interactions: List[Dict]):
        """Extrai dados de treinamento de interações do usuário"""
        for interaction in interactions:
            if interaction.get('rating', 0) >= 4:  # Apenas interações bem avaliadas
                self.add_example(
                    input_text=interaction['user_input'],
                    output_text=interaction['ai_response'],
                    source='user_feedback',
                    confidence=interaction['rating'] / 5.0
                )
                
    def from_existing_model(self, model_name: str, prompts: List[str]):
        """Gera dados sintéticos de modelo existente"""
        for prompt in prompts:
            response = generate_completion(prompt, model=model_name)
            
            # Revisão humana necessária para dados sintéticos
            self.add_example(
                input_text=prompt,
                output_text=response,
                source=f'synthetic_{model_name}',
                confidence=0.8  # Menor confiança para sintético
            )
            
    def validate_quality(self) -> Dict:
        """Validação abrangente de qualidade"""
        df = pd.DataFrame(self.examples)
        
        quality_report = {
            'total_examples': len(df),
            'avg_input_length': df['length_input'].mean(),
            'avg_output_length': df['length_output'].mean(),
            'length_variance': {
                'input_std': df['length_input'].std(),
                'output_std': df['length_output'].std()
            },
            'source_distribution': df['source'].value_counts().to_dict(),
            'confidence_stats': {
                'mean': df['confidence'].mean(),
                'min': df['confidence'].min(),
                'below_threshold': (df['confidence'] < 0.7).sum()
            }
        }
        
        # Sinalizar problemas potenciais
        issues = []
        
        if quality_report['total_examples'] < 1000:
            issues.append(f"Baixa contagem de amostras: {quality_report['total_examples']}")
            
        if quality_report['length_variance']['input_std'] > 100:
            issues.append("Alta variância de comprimento de entrada - considere agrupamento")
            
        if quality_report['confidence_stats']['below_threshold'] > len(df) * 0.2:
            issues.append("Muitos exemplos de baixa confiança")
            
        quality_report['issues'] = issues
        return quality_report
</CodeExample>

### Técnicas de Aumento de Dados

<CodeExample language="python">
import random
from transformers import pipeline

class DataAugmenter:
    def __init__(self):
        self.paraphraser = pipeline("text2text-generation", 
                                  model="tuner007/pegasus_paraphrase")
        
    def paraphrase_inputs(self, dataset: List[Dict], 
                         augmentation_factor: int = 2) -> List[Dict]:
        """Gera versões parafraseadas das entradas"""
        augmented = []
        
        for example in dataset:
            # Exemplo original
            augmented.append(example)
            
            # Gerar paráfrases
            for _ in range(augmentation_factor):
                paraphrased = self.paraphraser(
                    example['input'], 
                    max_length=len(example['input'].split()) + 10,
                    do_sample=True
                )[0]['generated_text']
                
                augmented.append({
                    'input': paraphrased,
                    'output': example['output'],
                    'source': 'paraphrased',
                    'confidence': example['confidence'] * 0.9
                })
                
        return augmented
    
    def add_variations(self, example: Dict) -> List[Dict]:
        """Adiciona variações sistemáticas aos exemplos"""
        variations = [example]  # Original
        
        input_text = example['input']
        output_text = example['output']
        
        # Variações de formalidade
        variations.extend([
            {
                'input': self.make_formal(input_text),
                'output': output_text,
                'source': 'formal_variation',
                'confidence': example['confidence']
            },
            {
                'input': self.make_casual(input_text),
                'output': output_text,
                'source': 'casual_variation',
                'confidence': example['confidence']
            }
        ])
        
        # Variações de comprimento
        if len(input_text.split()) > 10:
            variations.append({
                'input': self.compress_text(input_text),
                'output': output_text,
                'source': 'compressed',
                'confidence': example['confidence']
            })
            
        return variations
    
    def make_formal(self, text: str) -> str:
        """Converte texto para estilo mais formal"""
        replacements = {
            "não é": "não é",
            "não pode": "não pode",
            "não vai": "não irá",
            "é": "é",
            "você é": "você é"
        }
        
        for informal, formal in replacements.items():
            text = text.replace(informal, formal)
            
        return text
    
    def make_casual(self, text: str) -> str:
        """Converte texto para estilo mais casual"""
        replacements = {
            "não irá": "não vai",
            "é necessário": "precisa",
            "você é": "você é"
        }
        
        for formal, casual in replacements.items():
            text = text.replace(formal, casual)
            
        return text
</CodeExample>

### Pipeline de Controle de Qualidade

<CodeExample language="python">
from dataclasses import dataclass
from typing import Set
import re

@dataclass
class QualityIssue:
    type: str
    description: str
    severity: str  # 'low', 'medium', 'high'
    example_id: str

class QualityController:
    def __init__(self):
        self.rules = []
        self.add_default_rules()
        
    def add_rule(self, rule_func, name: str, severity: str = 'medium'):
        """Adiciona uma regra de controle de qualidade"""
        self.rules.append({
            'func': rule_func,
            'name': name,
            'severity': severity
        })
        
    def add_default_rules(self):
        """Adiciona regras padrão de controle de qualidade"""
        
        # Regras baseadas em comprimento
        self.add_rule(
            lambda ex: len(ex['input'].split()) < 3,
            "Entrada muito curta",
            'high'
        )
        
        self.add_rule(
            lambda ex: len(ex['output'].split()) < 2,
            "Saída muito curta",
            'high'
        )
        
        # Regras de qualidade de conteúdo
        self.add_rule(
            lambda ex: ex['input'].lower() == ex['output'].lower(),
            "Entrada igual à saída",
            'high'
        )
        
        self.add_rule(
            lambda ex: len(set(ex['input'].split()) & set(ex['output'].split())) / 
                      max(len(ex['input'].split()), 1) > 0.8,
            "Alta sobreposição de palavras",
            'medium'
        )
        
        # Regras de consistência de formato
        self.add_rule(
            lambda ex: bool(re.search(r'[^\x00-\x7F]', ex['input'])),
            "Caracteres não-ASCII na entrada",
            'low'
        )
        
    def check_example(self, example: Dict, example_id: str) -> List[QualityIssue]:
        """Verifica um único exemplo contra todas as regras"""
        issues = []
        
        for rule in self.rules:
            try:
                if rule['func'](example):
                    issues.append(QualityIssue(
                        type=rule['name'],
                        description=f"Exemplo falha: {rule['name']}",
                        severity=rule['severity'],
                        example_id=example_id
                    ))
            except Exception as e:
                issues.append(QualityIssue(
                    type="Erro de execução de regra",
                    description=f"Erro verificando {rule['name']}: {str(e)}",
                    severity='high',
                    example_id=example_id
                ))
                
        return issues
    
    def audit_dataset(self, dataset: List[Dict]) -> Dict:
        """Auditoria abrangente do dataset"""
        all_issues = []
        
        for i, example in enumerate(dataset):
            issues = self.check_example(example, str(i))
            all_issues.extend(issues)
            
        # Resumir problemas
        issue_summary = {}
        for issue in all_issues:
            key = f"{issue.severity}_{issue.type}"
            issue_summary[key] = issue_summary.get(key, 0) + 1
            
        return {
            'total_issues': len(all_issues),
            'issue_breakdown': issue_summary,
            'high_severity_count': len([i for i in all_issues if i.severity == 'high']),
            'recommendations': self.get_recommendations(all_issues, len(dataset))
        }
    
    def get_recommendations(self, issues: List[QualityIssue], total_examples: int) -> List[str]:
        """Gera recomendações acionáveis"""
        recommendations = []
        
        high_severity = len([i for i in issues if i.severity == 'high'])
        issue_rate = len(issues) / total_examples
        
        if high_severity > total_examples * 0.1:
            recommendations.append("Revisar e corrigir problemas de alta severidade antes do treinamento")
            
        if issue_rate > 0.3:
            recommendations.append("Considerar melhorar processo de coleta de dados")
            
        if any('muito curta' in i.type for i in issues):
            recommendations.append("Adicionar requisitos de comprimento mínimo à coleta de dados")
            
        return recommendations
</CodeExample>

## fine-tuning-techniques

### Escolhendo a Abordagem Certa

<Diagram>
flowchart TD
    A[Abordagem de Fine-tuning] --> B[Fine-tuning Completo]
    A --> C[Métodos Parameter-Efficient]
    A --> D[Métodos Adapter]
    
    B --> B1[Retreinamento completo do modelo]
    B --> B2[Alto custo computacional]
    B --> B3[Melhor performance]
    
    C --> C1[LoRA - Low-Rank Adaptation]
    C --> C2[Prefix Tuning]
    C --> C3[Prompt Tuning]
    
    D --> D1[Adapters específicos da tarefa]
    D --> D2[Abordagem modular]
    D --> D3[Deploy fácil]
</Diagram>

### LoRA (Low-Rank Adaptation)

LoRA é frequentemente a melhor escolha para a maioria dos cenários de fine-tuning - eficiente e efetivo.

<CodeExample language="python">
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
import torch

class LoRAFineTuner:
    def __init__(self, base_model_name: str, task_type: str = "CAUSAL_LM"):
        self.base_model_name = base_model_name
        self.task_type = getattr(TaskType, task_type)
        self.tokenizer = None
        self.model = None
        self.peft_model = None
        
    def setup_model(self, lora_config: dict = None):
        """Inicializa modelo com configuração LoRA"""
        
        # Configuração LoRA padrão
        if lora_config is None:
            lora_config = {
                "r": 16,  # Rank
                "lora_alpha": 32,  # Parâmetro de escala
                "target_modules": ["q_proj", "v_proj"],  # Quais camadas adaptar
                "lora_dropout": 0.1,
                "bias": "none",
                "task_type": self.task_type,
            }
        
        # Carregar tokenizer e modelo
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Adicionar token de padding se ausente
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # Aplicar LoRA
        peft_config = LoraConfig(**lora_config)
        self.peft_model = get_peft_model(self.model, peft_config)
        
        # Imprimir parâmetros treináveis
        self.peft_model.print_trainable_parameters()
        
    def prepare_dataset(self, examples: List[Dict], max_length: int = 512):
        """Prepara dataset para treinamento"""
        
        def tokenize_function(example):
            # Combinar entrada e saída para LM causal
            text = f"{example['input']}\n{example['output']}"
            
            tokenized = self.tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=max_length,
                return_tensors="pt"
            )
            
            # Para LM causal, labels são iguais aos input_ids
            tokenized["labels"] = tokenized["input_ids"].clone()
            
            return tokenized
        
        # Processar todos os exemplos
        tokenized_dataset = []
        for example in examples:
            tokenized_dataset.append(tokenize_function(example))
            
        return tokenized_dataset
    
    def train(self, train_dataset, eval_dataset=None, training_args=None):
        """Fazer fine-tuning do modelo com LoRA"""
        from transformers import Trainer, TrainingArguments
        
        if training_args is None:
            training_args = TrainingArguments(
                output_dir="./lora_model",
                num_train_epochs=3,
                per_device_train_batch_size=4,
                per_device_eval_batch_size=4,
                gradient_accumulation_steps=2,
                warmup_steps=100,
                learning_rate=5e-4,
                fp16=True,
                logging_steps=10,
                evaluation_strategy="steps" if eval_dataset else "no",
                eval_steps=100,
                save_steps=500,
                save_total_limit=2,
                remove_unused_columns=False,
            )
        
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        
        # Treinar o modelo
        trainer.train()
        
        # Salvar o adapter
        self.peft_model.save_pretrained("./lora_adapter")
        self.tokenizer.save_pretrained("./lora_adapter")
        
        return trainer

# Exemplo de uso
fine_tuner = LoRAFineTuner("microsoft/DialoGPT-medium")
fine_tuner.setup_model()

# Preparar seu dataset
train_data = fine_tuner.prepare_dataset(training_examples)
eval_data = fine_tuner.prepare_dataset(validation_examples)

# Treinar
trainer = fine_tuner.train(train_data, eval_data)
</CodeExample>

### Estratégias de Treinamento Avançadas

<CodeExample language="python">
import numpy as np
from transformers import EarlyStoppingCallback
import wandb

class AdvancedTrainer:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.training_history = []
        
    def curriculum_learning(self, dataset: List[Dict], 
                          difficulty_metric: callable = None):
        """Implementa curriculum learning - exemplos fáceis primeiro"""
        
        if difficulty_metric is None:
            # Padrão: usar comprimento da entrada como dificuldade
            difficulty_metric = lambda x: len(x['input'].split())
        
        # Ordenar por dificuldade
        sorted_data = sorted(dataset, key=difficulty_metric)
        
        # Criar fases do currículo
        phases = []
        total_examples = len(sorted_data)
        
        # Fase 1: 30% mais fáceis
        phases.append(sorted_data[:int(0.3 * total_examples)])
        
        # Fase 2: 50% médios
        phases.append(sorted_data[:int(0.8 * total_examples)])
        
        # Fase 3: Todos os exemplos
        phases.append(sorted_data)
        
        return phases
    
    def adaptive_learning_rate(self, trainer, eval_metric_history: List[float]):
        """Ajusta taxa de aprendizado baseado na performance"""
        
        if len(eval_metric_history) < 3:
            return  # Precisa de histórico para adaptar
        
        # Verificar se performance está estagnando
        recent_improvement = (eval_metric_history[-1] - eval_metric_history[-3])
        
        if recent_improvement < 0.001:  # Platô de performance
            # Reduzir taxa de aprendizado
            current_lr = trainer.optimizer.param_groups[0]['lr']
            new_lr = current_lr * 0.5
            
            for param_group in trainer.optimizer.param_groups:
                param_group['lr'] = new_lr
                
            print(f"Taxa de aprendizado reduzida para {new_lr}")
    
    def multi_objective_training(self, primary_loss, secondary_losses: Dict):
        """Equilibra múltiplos objetivos de treinamento"""
        
        class MultiObjectiveLoss:
            def __init__(self, weights: Dict[str, float]):
                self.weights = weights
                
            def __call__(self, primary, secondary_dict):
                total_loss = self.weights['primary'] * primary
                
                for name, loss in secondary_dict.items():
                    if name in self.weights:
                        total_loss += self.weights[name] * loss
                
                return total_loss
        
        # Exemplo: Equilibrar fluência e factualidade
        loss_weights = {
            'primary': 0.7,      # Loss da tarefa principal
            'fluency': 0.2,      # Penalidade de fluência
            'factuality': 0.1    # Consistência factual
        }
        
        return MultiObjectiveLoss(loss_weights)
    
    def setup_monitoring(self, project_name: str):
        """Configura monitoramento abrangente de treinamento"""
        
        # Inicializar Weights & Biases
        wandb.init(project=project_name)
        
        # Rastreamento de métricas customizadas
        class MetricsCallback:
            def __init__(self):
                self.step = 0
                
            def on_log(self, args, state, control, model=None, logs=None, **kwargs):
                if logs:
                    # Rastrear métricas customizadas
                    if 'eval_loss' in logs:
                        wandb.log({
                            'eval_loss': logs['eval_loss'],
                            'step': self.step
                        })
                    
                    # Rastrear normas de gradiente
                    if model:
                        total_norm = 0
                        for p in model.parameters():
                            if p.grad is not None:
                                param_norm = p.grad.data.norm(2)
                                total_norm += param_norm.item() ** 2
                        total_norm = total_norm ** (1. / 2)
                        
                        wandb.log({
                            'gradient_norm': total_norm,
                            'step': self.step
                        })
                    
                    self.step += 1
        
        return MetricsCallback()
    
    def progressive_unfreezing(self, model, num_phases: int = 3):
        """Descongela camadas do modelo gradualmente durante treinamento"""
        
        # Obter todos os nomes de camada
        layer_names = [name for name, _ in model.named_parameters()]
        layers_per_phase = len(layer_names) // num_phases
        
        def unfreeze_phase(phase: int):
            """Descongela camadas para uma fase específica"""
            start_idx = phase * layers_per_phase
            end_idx = start_idx + layers_per_phase
            
            for i, (name, param) in enumerate(model.named_parameters()):
                if i >= start_idx and i < end_idx:
                    param.requires_grad = True
                    print(f"Camada descongelada: {name}")
        
        return unfreeze_phase
</CodeExample>

## evaluation-deployment

### Framework Abrangente de Avaliação

<CodeExample language="python">
from typing import List, Dict, Any
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import pandas as pd

class ModelEvaluator:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.metrics = {}
        
    def evaluate_generation_quality(self, test_examples: List[Dict]) -> Dict:
        """Avalia qualidade de geração de texto"""
        
        predictions = []
        references = []
        
        for example in test_examples:
            # Gerar predição
            inputs = self.tokenizer(
                example['input'], 
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            prediction = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            predictions.append(prediction)
            references.append(example['output'])
        
        # Calcular métricas
        metrics = {
            'bleu': self.calculate_bleu(predictions, references),
            'rouge': self.calculate_rouge(predictions, references),
            'semantic_similarity': self.calculate_semantic_similarity(predictions, references),
            'length_ratio': self.calculate_length_metrics(predictions, references),
            'factual_consistency': self.evaluate_factual_consistency(predictions, references)
        }
        
        return metrics
    
    def calculate_bleu(self, predictions: List[str], references: List[str]) -> float:
        """Calcula pontuação BLEU"""
        from nltk.translate.bleu_score import sentence_bleu
        import nltk
        nltk.download('punkt', quiet=True)
        
        bleu_scores = []
        for pred, ref in zip(predictions, references):
            pred_tokens = nltk.word_tokenize(pred.lower())
            ref_tokens = nltk.word_tokenize(ref.lower())
            
            score = sentence_bleu([ref_tokens], pred_tokens)
            bleu_scores.append(score)
        
        return np.mean(bleu_scores)
    
    def calculate_rouge(self, predictions: List[str], references: List[str]) -> Dict:
        """Calcula pontuações ROUGE"""
        from rouge_score import rouge_scorer
        
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}
        
        for pred, ref in zip(predictions, references):
            rouge_scores = scorer.score(ref, pred)
            for metric in scores.keys():
                scores[metric].append(rouge_scores[metric].fmeasure)
        
        return {metric: np.mean(scores[metric]) for metric in scores}
    
    def calculate_semantic_similarity(self, predictions: List[str], references: List[str]) -> float:
        """Calcula similaridade semântica usando embeddings"""
        from sentence_transformers import SentenceTransformer
        
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        pred_embeddings = model.encode(predictions)
        ref_embeddings = model.encode(references)
        
        similarities = []
        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
            similarity = np.dot(pred_emb, ref_emb) / (
                np.linalg.norm(pred_emb) * np.linalg.norm(ref_emb)
            )
            similarities.append(similarity)
        
        return np.mean(similarities)
    
    def evaluate_factual_consistency(self, predictions: List[str], references: List[str]) -> float:
        """Avalia consistência factual"""
        # Versão simplificada - na prática, use modelos especializados como FactCC
        
        consistency_scores = []
        for pred, ref in zip(predictions, references):
            # Extrair fatos chave (simplificado)
            pred_facts = set(self.extract_facts(pred))
            ref_facts = set(self.extract_facts(ref))
            
            if not ref_facts:
                consistency_scores.append(1.0)  # Nenhum fato para verificar
                continue
            
            # Calcular sobreposição
            consistent_facts = pred_facts.intersection(ref_facts)
            consistency = len(consistent_facts) / len(ref_facts)
            consistency_scores.append(consistency)
        
        return np.mean(consistency_scores)
    
    def extract_facts(self, text: str) -> List[str]:
        """Extrai fatos chave do texto (implementação simplificada)"""
        import re
        
        # Extrair fatos numéricos
        numbers = re.findall(r'\b\d+\.?\d*\b', text)
        
        # Extrair entidades nomeadas (simplificado)
        entities = re.findall(r'\b[A-Z][a-z]+\b', text)
        
        return numbers + entities
    
    def benchmark_performance(self, test_suite: Dict[str, List[Dict]]) -> Dict:
        """Executa benchmark abrangente através de múltiplas suítes de teste"""
        
        results = {}
        
        for suite_name, examples in test_suite.items():
            print(f"Avaliando {suite_name}...")
            
            suite_results = self.evaluate_generation_quality(examples)
            results[suite_name] = suite_results
            
            # Métricas específicas da tarefa
            if 'reasoning' in suite_name.lower():
                results[suite_name]['logical_consistency'] = self.evaluate_reasoning(examples)
            
            if 'code' in suite_name.lower():
                results[suite_name]['syntax_validity'] = self.evaluate_code_syntax(examples)
        
        return results
    
    def create_evaluation_report(self, results: Dict) -> str:
        """Gera relatório abrangente de avaliação"""
        
        report = "# Relatório de Avaliação do Modelo\n\n"
        
        # Resumo geral
        report += "## Performance Geral\n\n"
        
        avg_bleu = np.mean([r['bleu'] for r in results.values()])
        avg_rouge = np.mean([r['rouge']['rougeL'] for r in results.values()])
        avg_semantic = np.mean([r['semantic_similarity'] for r in results.values()])
        
        report += f"- BLEU Médio: {avg_bleu:.3f}\n"
        report += f"- ROUGE-L Médio: {avg_rouge:.3f}\n"
        report += f"- Similaridade Semântica Média: {avg_semantic:.3f}\n\n"
        
        # Detalhamento
        report += "## Resultados Detalhados por Suíte de Teste\n\n"
        
        for suite_name, metrics in results.items():
            report += f"### {suite_name}\n\n"
            
            for metric_name, value in metrics.items():
                if isinstance(value, dict):
                    for sub_metric, sub_value in value.items():
                        report += f"- {metric_name}.{sub_metric}: {sub_value:.3f}\n"
                else:
                    report += f"- {metric_name}: {value:.3f}\n"
            
            report += "\n"
        
        # Recomendações
        report += "## Recomendações\n\n"
        
        if avg_bleu < 0.3:
            report += "- Considere melhorar fluência através de dados de treinamento adicionais\n"
        
        if avg_semantic < 0.7:
            report += "- Foque na consistência semântica nos exemplos de treinamento\n"
        
        return report
</CodeExample>

### Estratégias de Deployment

<CodeExample language="python">
import os
from typing import Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

class ModelDeployment:
    def __init__(self, base_model_path: str, adapter_path: Optional[str] = None):
        self.base_model_path = base_model_path
        self.adapter_path = adapter_path
        self.model = None
        self.tokenizer = None
        
    def load_for_inference(self, device: str = "auto"):
        """Carrega modelo otimizado para inferência"""
        
        # Carregar tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
        
        # Carregar modelo base
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map=device,
            low_cpu_mem_usage=True
        )
        
        # Carregar adapter se fornecido
        if self.adapter_path:
            self.model = PeftModel.from_pretrained(
                self.model, 
                self.adapter_path,
                torch_dtype=torch.float16
            )
            
            # Mesclar adapter para inferência mais rápida
            self.model = self.model.merge_and_unload()
        
        # Otimizar para inferência
        self.model.eval()
        if hasattr(self.model, 'compile'):
            self.model = torch.compile(self.model)
    
    def create_api_endpoint(self, app_framework='fastapi'):
        """Cria endpoint de API REST"""
        
        if app_framework == 'fastapi':
            from fastapi import FastAPI, HTTPException
            from pydantic import BaseModel
            
            app = FastAPI(title="API do Modelo Fine-tuned")
            
            class GenerationRequest(BaseModel):
                prompt: str
                max_tokens: int = 150
                temperature: float = 0.7
                top_p: float = 0.9
            
            class GenerationResponse(BaseModel):
                generated_text: str
                input_tokens: int
                output_tokens: int
                
            @app.post("/generate", response_model=GenerationResponse)
            async def generate_text(request: GenerationRequest):
                try:
                    # Tokenizar entrada
                    inputs = self.tokenizer(
                        request.prompt,
                        return_tensors="pt",
                        truncation=True,
                        max_length=512
                    )
                    
                    input_token_count = inputs['input_ids'].shape[1]
                    
                    # Gerar
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=request.max_tokens,
                            temperature=request.temperature,
                            top_p=request.top_p,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                    
                    # Decodificar saída
                    generated_text = self.tokenizer.decode(
                        outputs[0][input_token_count:],
                        skip_special_tokens=True
                    )
                    
                    output_token_count = outputs[0].shape[0] - input_token_count
                    
                    return GenerationResponse(
                        generated_text=generated_text,
                        input_tokens=input_token_count,
                        output_tokens=output_token_count
                    )
                    
                except Exception as e:
                    raise HTTPException(status_code=500, detail=str(e))
            
            return app
    
    def create_docker_config(self) -> str:
        """Gera Dockerfile para deployment"""
        
        dockerfile = """
FROM nvidia/cuda:11.8-runtime-ubuntu20.04

# Instalar Python e dependências
RUN apt-get update && apt-get install -y \\
    python3 \\
    python3-pip \\
    && rm -rf /var/lib/apt/lists/*

# Definir diretório de trabalho
WORKDIR /app

# Copiar requirements
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# Copiar arquivos do modelo
COPY model/ ./model/
COPY adapter/ ./adapter/

# Copiar código da aplicação
COPY app.py .

# Expor porta
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:8000/health || exit 1

# Executar aplicação
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
"""
        return dockerfile
    
    def create_kubernetes_config(self) -> str:
        """Gera configuração de deployment Kubernetes"""
        
        k8s_config = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fine-tuned-model
  labels:
    app: fine-tuned-model
spec:
  replicas: 2
  selector:
    matchLabels:
      app: fine-tuned-model
  template:
    metadata:
      labels:
        app: fine-tuned-model
    spec:
      containers:
      - name: model-server
        image: your-registry/fine-tuned-model:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
          limits:
            memory: "16Gi"
            cpu: "4000m"
            nvidia.com/gpu: 1
        env:
        - name: MODEL_PATH
          value: "/app/model"
        - name: ADAPTER_PATH
          value: "/app/adapter"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: fine-tuned-model-service
spec:
  selector:
    app: fine-tuned-model
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
"""
        return k8s_config
</CodeExample>

## quiz

<Quiz>
  <Question
    question="Quando fine-tuning é preferível a engenharia de prompts + RAG?"
    options={[
      "Quando você precisa acessar informação atual",
      "Quando você precisa formatação de saída consistente e expertise específica do domínio",
      "Quando você tem recursos computacionais limitados",
      "Quando os requisitos mudam frequentemente"
    ]}
    correct={1}
    explanation="Fine-tuning é melhor para requisitos de formatação consistente e conhecimento específico do domínio que não muda frequentemente. Para informação atual, RAG é melhor; para recursos limitados, engenharia de prompts é mais eficiente; para requisitos em mudança, prompting oferece mais flexibilidade."
  />
  
  <Question
    question="Qual é o tamanho mínimo recomendado de dataset para fine-tuning de produção?"
    options={[
      "100 exemplos",
      "500 exemplos", 
      "1.000 exemplos",
      "10.000 exemplos"
    ]}
    correct={2}
    explanation="Embora 1.000 exemplos seja o mínimo para resultados decentes, 5.000+ exemplos são recomendados para uso em produção. Qualidade importa mais que quantidade, mas quantidade suficiente é necessária para generalização."
  />
  
  <Question
    question="Qual é a principal vantagem do LoRA sobre fine-tuning completo?"
    options={[
      "Melhor performance final",
      "Funciona com datasets menores",
      "Custo computacional muito menor mantendo qualidade",
      "Elimina a necessidade de avaliação"
    ]}
    correct={2}
    explanation="LoRA (Low-Rank Adaptation) reduz significativamente custos computacionais treinando apenas um pequeno número de parâmetros adicionais enquanto alcança performance próxima ao fine-tuning completo."
  />
  
  <Question
    question="Qual métrica é mais importante para avaliar consistência factual?"
    options={[
      "Pontuação BLEU",
      "Pontuação ROUGE",
      "Perplexidade",
      "Verificação de fatos contra referência"
    ]}
    correct={3}
    explanation="Embora BLEU e ROUGE meçam similaridade de texto e perplexidade meça qualidade de modelagem de linguagem, consistência factual requer verificação específica se fatos gerados correspondem aos fatos de referência."
  />
  
  <Question
    question="No curriculum learning, qual é a progressão de treinamento recomendada?"
    options={[
      "Exemplos difíceis primeiro, depois fáceis",
      "Ordem aleatória durante todo o treinamento",
      "Exemplos fáceis primeiro, aumentando gradualmente a dificuldade", 
      "Usar apenas exemplos de dificuldade similar"
    ]}
    correct={2}
    explanation="Curriculum learning funciona começando com exemplos mais fáceis para estabelecer padrões básicos, então gradualmente introduzindo exemplos mais complexos conforme o modelo se torna mais capaz."
  />
</Quiz>

## Summary

Você dominou fine-tuning e adaptação de modelo:

✅ **Framework de Decisão**: Quando fazer fine-tuning vs. engenharia de prompts vs. RAG
✅ **Preparação de Dados**: Controle de qualidade, aumento e pipelines de validação
✅ **Técnicas de Treinamento**: LoRA, curriculum learning e estratégias avançadas
✅ **Avaliação**: Métricas abrangentes e abordagens de benchmarking
✅ **Deployment**: Deployment pronto para produção com APIs e monitoramento

Próximo módulo: **Avaliação e Teste de Sistemas de IA** - Aprenda abordagens sistemáticas para testar e validar sistemas de IA em produção.