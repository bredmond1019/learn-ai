# AI System Architecture Patterns

## enterprise-patterns

Enterprise AI systems require robust, scalable, and maintainable architectures that can handle real-world complexity. Understanding proven architectural patterns is essential for building systems that scale from prototype to production.

<Callout type="info">
AI systems are not just about the modelsâ€”they're complex distributed systems that require the same architectural rigor as any enterprise software, with additional considerations for model lifecycle, data pipelines, and inference optimization.
</Callout>

### Core Enterprise AI Patterns

<Diagram>
graph TB
    A[Client Applications] --> B[API Gateway]
    B --> C[Load Balancer]
    C --> D[Model Serving Layer]
    
    D --> E[Model Registry]
    D --> F[Feature Store]
    D --> G[Inference Cache]
    
    H[Data Pipeline] --> I[Feature Engineering]
    I --> F
    
    J[Training Pipeline] --> K[Model Training]
    K --> L[Model Validation]
    L --> E
    
    M[Monitoring Service] --> N[Model Performance]
    M --> O[Data Drift Detection]
    M --> P[Infrastructure Metrics]
    
    Q[Configuration Service] --> D
    Q --> H
    Q --> J
</Diagram>

### The Model-as-a-Service Pattern

<CodeExample language="python">
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import asyncio
import logging
from datetime import datetime
import uuid

@dataclass
class ModelMetadata:
    model_id: str
    version: str
    framework: str
    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]
    performance_metrics: Dict[str, float]
    created_at: str
    tags: List[str]

@dataclass
class InferenceRequest:
    request_id: str
    model_id: str
    version: Optional[str]
    inputs: Dict[str, Any]
    parameters: Dict[str, Any]
    metadata: Dict[str, Any]

@dataclass
class InferenceResponse:
    request_id: str
    model_id: str
    version: str
    outputs: Dict[str, Any]
    confidence: Optional[float]
    latency_ms: float
    metadata: Dict[str, Any]

class ModelService(ABC):
    """Abstract base class for model services"""
    
    @abstractmethod
    async def predict(self, request: InferenceRequest) -> InferenceResponse:
        pass
    
    @abstractmethod
    async def health_check(self) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    def get_metadata(self) -> ModelMetadata:
        pass

class ModelRegistry:
    """Central registry for model metadata and versioning"""
    
    def __init__(self):
        self.models: Dict[str, Dict[str, ModelMetadata]] = {}
        self.deployments: Dict[str, str] = {}  # model_id -> active_version
        
    def register_model(self, metadata: ModelMetadata):
        """Register a new model version"""
        if metadata.model_id not in self.models:
            self.models[metadata.model_id] = {}
        
        self.models[metadata.model_id][metadata.version] = metadata
        
        # Set as active version if it's the first version
        if metadata.model_id not in self.deployments:
            self.deployments[metadata.model_id] = metadata.version
    
    def get_model_metadata(self, model_id: str, version: Optional[str] = None) -> Optional[ModelMetadata]:
        """Get model metadata for specific version or active version"""
        if model_id not in self.models:
            return None
        
        if version is None:
            version = self.deployments.get(model_id)
            if version is None:
                return None
        
        return self.models[model_id].get(version)
    
    def promote_model(self, model_id: str, version: str):
        """Promote a model version to active"""
        if model_id in self.models and version in self.models[model_id]:
            self.deployments[model_id] = version
        else:
            raise ValueError(f"Model {model_id} version {version} not found")
    
    def list_models(self) -> List[Dict[str, Any]]:
        """List all registered models"""
        result = []
        for model_id, versions in self.models.items():
            active_version = self.deployments.get(model_id)
            result.append({
                'model_id': model_id,
                'versions': list(versions.keys()),
                'active_version': active_version,
                'latest_version': max(versions.keys()) if versions else None
            })
        return result

class ModelOrchestrator:
    """Orchestrates requests across multiple model services"""
    
    def __init__(self, registry: ModelRegistry):
        self.registry = registry
        self.model_services: Dict[str, ModelService] = {}
        self.load_balancers: Dict[str, 'LoadBalancer'] = {}
        
    def register_service(self, model_id: str, version: str, service: ModelService):
        """Register a model service instance"""
        key = f"{model_id}:{version}"
        self.model_services[key] = service
        
        # Update load balancer
        if model_id not in self.load_balancers:
            self.load_balancers[model_id] = LoadBalancer()
        
        self.load_balancers[model_id].add_instance(version, service)
    
    async def predict(self, request: InferenceRequest) -> InferenceResponse:
        """Route prediction request to appropriate model service"""
        
        # Resolve version if not specified
        if request.version is None:
            metadata = self.registry.get_model_metadata(request.model_id)
            if metadata is None:
                raise ValueError(f"Model {request.model_id} not found")
            request.version = metadata.version
        
        # Get load balancer for model
        if request.model_id not in self.load_balancers:
            raise ValueError(f"No services available for model {request.model_id}")
        
        load_balancer = self.load_balancers[request.model_id]
        
        # Route request
        service = await load_balancer.get_instance(request.version)
        if service is None:
            raise ValueError(f"No service available for {request.model_id}:{request.version}")
        
        # Execute prediction
        try:
            response = await service.predict(request)
            await load_balancer.record_success(request.version)
            return response
        except Exception as e:
            await load_balancer.record_failure(request.version)
            raise

class LoadBalancer:
    """Load balancer for model service instances"""
    
    def __init__(self, strategy: str = "round_robin"):
        self.strategy = strategy
        self.instances: Dict[str, List[ModelService]] = {}
        self.current_index: Dict[str, int] = {}
        self.health_status: Dict[str, Dict[int, bool]] = {}
        
    def add_instance(self, version: str, service: ModelService):
        """Add a service instance"""
        if version not in self.instances:
            self.instances[version] = []
            self.current_index[version] = 0
            self.health_status[version] = {}
        
        self.instances[version].append(service)
        instance_id = len(self.instances[version]) - 1
        self.health_status[version][instance_id] = True
    
    async def get_instance(self, version: str) -> Optional[ModelService]:
        """Get next available instance using load balancing strategy"""
        
        if version not in self.instances or not self.instances[version]:
            return None
        
        if self.strategy == "round_robin":
            return await self._round_robin_select(version)
        elif self.strategy == "least_connections":
            return await self._least_connections_select(version)
        else:
            return self.instances[version][0]  # Fallback to first instance
    
    async def _round_robin_select(self, version: str) -> Optional[ModelService]:
        """Round-robin load balancing"""
        instances = self.instances[version]
        health = self.health_status[version]
        
        # Find next healthy instance
        start_index = self.current_index[version]
        for i in range(len(instances)):
            index = (start_index + i) % len(instances)
            if health.get(index, True):  # Default to healthy if not tracked
                self.current_index[version] = (index + 1) % len(instances)
                return instances[index]
        
        return None  # No healthy instances
    
    async def record_success(self, version: str):
        """Record successful request"""
        # Update health status, metrics, etc.
        pass
    
    async def record_failure(self, version: str):
        """Record failed request"""
        # Update health status, potentially mark instance as unhealthy
        pass

class FeatureStore:
    """Centralized feature storage and serving"""
    
    def __init__(self):
        self.online_features: Dict[str, Dict[str, Any]] = {}
        self.feature_schemas: Dict[str, Dict[str, Any]] = {}
        
    def register_feature_group(self, name: str, schema: Dict[str, Any]):
        """Register a feature group schema"""
        self.feature_schemas[name] = schema
        
    async def get_features(self, feature_group: str, entity_id: str) -> Dict[str, Any]:
        """Get features for an entity"""
        
        # Mock implementation - in production, this would query a database
        if feature_group in self.online_features and entity_id in self.online_features[feature_group]:
            return self.online_features[feature_group][entity_id]
        
        # Return default values based on schema
        if feature_group in self.feature_schemas:
            defaults = {}
            for feature_name, feature_def in self.feature_schemas[feature_group].items():
                defaults[feature_name] = feature_def.get('default', 0)
            return defaults
        
        return {}
    
    async def write_features(self, feature_group: str, entity_id: str, features: Dict[str, Any]):
        """Write features to online store"""
        
        if feature_group not in self.online_features:
            self.online_features[feature_group] = {}
        
        self.online_features[feature_group][entity_id] = features

class ConfigurationService:
    """Centralized configuration management"""
    
    def __init__(self):
        self.configs: Dict[str, Any] = {}
        self.watchers: Dict[str, List[callable]] = {}
        
    def set_config(self, key: str, value: Any):
        """Set configuration value"""
        old_value = self.configs.get(key)
        self.configs[key] = value
        
        # Notify watchers
        if key in self.watchers and old_value != value:
            for callback in self.watchers[key]:
                try:
                    callback(key, value, old_value)
                except Exception as e:
                    logging.error(f"Error in config watcher: {e}")
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        return self.configs.get(key, default)
    
    def watch_config(self, key: str, callback: callable):
        """Watch for configuration changes"""
        if key not in self.watchers:
            self.watchers[key] = []
        self.watchers[key].append(callback)

class AIServiceMesh:
    """Service mesh for AI services with advanced routing and observability"""
    
    def __init__(self):
        self.registry = ModelRegistry()
        self.orchestrator = ModelOrchestrator(self.registry)
        self.feature_store = FeatureStore()
        self.config_service = ConfigurationService()
        self.circuit_breakers: Dict[str, 'CircuitBreaker'] = {}
        
    async def serve_request(self, request: InferenceRequest) -> InferenceResponse:
        """Serve an inference request through the service mesh"""
        
        request.request_id = request.request_id or str(uuid.uuid4())
        start_time = datetime.now()
        
        try:
            # Apply circuit breaker pattern
            circuit_breaker = self._get_circuit_breaker(request.model_id)
            if circuit_breaker.is_open():
                raise Exception("Circuit breaker is open")
            
            # Enrich request with features if needed
            if request.metadata.get('use_features', False):
                entity_id = request.metadata.get('entity_id')
                if entity_id:
                    features = await self.feature_store.get_features(
                        request.model_id, entity_id
                    )
                    request.inputs.update(features)
            
            # Route to model service
            response = await self.orchestrator.predict(request)
            
            # Record success
            await circuit_breaker.record_success()
            
            # Update latency
            latency = (datetime.now() - start_time).total_seconds() * 1000
            response.latency_ms = latency
            
            return response
            
        except Exception as e:
            await circuit_breaker.record_failure()
            
            # Return error response
            return InferenceResponse(
                request_id=request.request_id,
                model_id=request.model_id,
                version="unknown",
                outputs={"error": str(e)},
                confidence=None,
                latency_ms=(datetime.now() - start_time).total_seconds() * 1000,
                metadata={"error": True}
            )
    
    def _get_circuit_breaker(self, model_id: str) -> 'CircuitBreaker':
        """Get or create circuit breaker for model"""
        if model_id not in self.circuit_breakers:
            self.circuit_breakers[model_id] = CircuitBreaker(
                failure_threshold=5,
                timeout_seconds=60
            )
        return self.circuit_breakers[model_id]

class CircuitBreaker:
    """Circuit breaker pattern implementation"""
    
    def __init__(self, failure_threshold: int = 5, timeout_seconds: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout_seconds = timeout_seconds
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        
    def is_open(self) -> bool:
        """Check if circuit breaker is open"""
        if self.state == "OPEN":
            if self.last_failure_time:
                time_since_failure = (datetime.now() - self.last_failure_time).total_seconds()
                if time_since_failure >= self.timeout_seconds:
                    self.state = "HALF_OPEN"
                    return False
            return True
        return False
    
    async def record_success(self):
        """Record successful operation"""
        self.failure_count = 0
        self.state = "CLOSED"
    
    async def record_failure(self):
        """Record failed operation"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()
        
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
</CodeExample>

### The Feature Pipeline Pattern

<CodeExample language="python">
from typing import Dict, Any, List, Optional
import asyncio
from dataclasses import dataclass
from datetime import datetime, timedelta
import pandas as pd

@dataclass
class FeatureDefinition:
    name: str
    data_type: str
    source: str
    transformation: str
    dependencies: List[str]
    freshness_sla: int  # minutes
    
class FeaturePipeline:
    """Feature engineering and serving pipeline"""
    
    def __init__(self):
        self.feature_definitions: Dict[str, FeatureDefinition] = {}
        self.transformations: Dict[str, callable] = {}
        self.data_sources: Dict[str, 'DataSource'] = {}
        self.scheduler = FeatureScheduler()
        
    def register_feature(self, definition: FeatureDefinition, 
                        transformation: callable):
        """Register a feature definition and its transformation"""
        self.feature_definitions[definition.name] = definition
        self.transformations[definition.name] = transformation
        
        # Schedule feature computation
        self.scheduler.schedule_feature(
            definition.name, 
            definition.freshness_sla,
            self._compute_feature
        )
    
    def register_data_source(self, name: str, source: 'DataSource'):
        """Register a data source"""
        self.data_sources[name] = source
        
    async def _compute_feature(self, feature_name: str):
        """Compute a feature"""
        definition = self.feature_definitions[feature_name]
        transformation = self.transformations[feature_name]
        
        # Get source data
        source_data = await self._get_source_data(definition.source)
        
        # Apply transformation
        feature_values = await transformation(source_data)
        
        # Store in feature store
        await self._store_features(feature_name, feature_values)
        
    async def _get_source_data(self, source_name: str) -> pd.DataFrame:
        """Get data from source"""
        if source_name not in self.data_sources:
            raise ValueError(f"Data source {source_name} not found")
        
        return await self.data_sources[source_name].get_data()
    
    async def _store_features(self, feature_name: str, 
                            feature_values: Dict[str, Any]):
        """Store computed features"""
        # Implementation would store to feature store
        pass

class FeatureScheduler:
    """Schedule feature computation jobs"""
    
    def __init__(self):
        self.scheduled_features: Dict[str, Dict] = {}
        self.running = False
        
    def schedule_feature(self, feature_name: str, interval_minutes: int,
                        compute_function: callable):
        """Schedule a feature for regular computation"""
        self.scheduled_features[feature_name] = {
            'interval_minutes': interval_minutes,
            'compute_function': compute_function,
            'last_run': None,
            'next_run': datetime.now()
        }
    
    async def start(self):
        """Start the scheduler"""
        self.running = True
        while self.running:
            current_time = datetime.now()
            
            for feature_name, schedule in self.scheduled_features.items():
                if current_time >= schedule['next_run']:
                    try:
                        await schedule['compute_function'](feature_name)
                        schedule['last_run'] = current_time
                        schedule['next_run'] = current_time + timedelta(
                            minutes=schedule['interval_minutes']
                        )
                    except Exception as e:
                        logging.error(f"Feature computation failed for {feature_name}: {e}")
            
            await asyncio.sleep(60)  # Check every minute
    
    def stop(self):
        """Stop the scheduler"""
        self.running = False

class DataSource:
    """Abstract data source"""
    
    async def get_data(self) -> pd.DataFrame:
        """Get data from source"""
        raise NotImplementedError

class DatabaseSource(DataSource):
    """Database data source"""
    
    def __init__(self, connection_string: str, query: str):
        self.connection_string = connection_string
        self.query = query
    
    async def get_data(self) -> pd.DataFrame:
        """Get data from database"""
        # Mock implementation
        return pd.DataFrame({
            'entity_id': ['user_1', 'user_2', 'user_3'],
            'value': [1.0, 2.0, 3.0]
        })

class StreamingSource(DataSource):
    """Streaming data source"""
    
    def __init__(self, topic: str, window_minutes: int = 60):
        self.topic = topic
        self.window_minutes = window_minutes
    
    async def get_data(self) -> pd.DataFrame:
        """Get data from streaming source"""
        # Mock implementation
        return pd.DataFrame({
            'entity_id': ['user_1', 'user_2'],
            'event_count': [10, 15],
            'timestamp': [datetime.now(), datetime.now()]
        })
</CodeExample>

## microservices-ai

### Microservices Architecture for AI Systems

Breaking AI systems into microservices provides scalability, maintainability, and deployment flexibility, but requires careful service boundary design.

<Diagram>
graph TB
    subgraph "API Layer"
        A[API Gateway]
        B[Authentication Service]
        C[Rate Limiting Service]
    end
    
    subgraph "AI Services"
        D[Text Classification Service]
        E[Text Generation Service]
        F[Embedding Service]
        G[Recommendation Service]
    end
    
    subgraph "Data Services"
        H[Feature Store Service]
        I[Model Registry Service]
        J[Experiment Tracking Service]
    end
    
    subgraph "Infrastructure Services"
        K[Monitoring Service]
        L[Configuration Service]
        M[Logging Service]
    end
    
    A --> D
    A --> E
    A --> F
    A --> G
    
    D --> H
    E --> H
    F --> I
    G --> H
    
    D --> K
    E --> K
    F --> K
    G --> K
</Diagram>

<CodeExample language="python">
from fastapi import FastAPI, HTTPException, Depends, status
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
import httpx
from datetime import datetime
import logging

# Service Models
class TextClassificationRequest(BaseModel):
    text: str
    model_version: Optional[str] = None
    confidence_threshold: Optional[float] = 0.5

class TextClassificationResponse(BaseModel):
    text: str
    predicted_class: str
    confidence: float
    all_predictions: Dict[str, float]
    model_version: str
    processing_time_ms: float

class EmbeddingRequest(BaseModel):
    texts: List[str]
    model_type: Optional[str] = "default"
    normalize: Optional[bool] = True

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    model_type: str
    dimension: int
    processing_time_ms: float

# Text Classification Microservice
class TextClassificationService:
    """Microservice for text classification"""
    
    def __init__(self):
        self.app = FastAPI(title="Text Classification Service", version="1.0.0")
        self.models = {}
        self.setup_routes()
        
    def setup_routes(self):
        """Setup FastAPI routes"""
        
        @self.app.post("/classify", response_model=TextClassificationResponse)
        async def classify_text(request: TextClassificationRequest):
            start_time = datetime.now()
            
            try:
                # Load model if not cached
                model = await self._get_model(request.model_version)
                
                # Perform classification
                predictions = await self._classify(model, request.text)
                
                # Filter by confidence threshold
                filtered_predictions = {
                    k: v for k, v in predictions.items() 
                    if v >= request.confidence_threshold
                }
                
                if not filtered_predictions:
                    predicted_class = max(predictions, key=predictions.get)
                    confidence = predictions[predicted_class]
                else:
                    predicted_class = max(filtered_predictions, key=filtered_predictions.get)
                    confidence = filtered_predictions[predicted_class]
                
                processing_time = (datetime.now() - start_time).total_seconds() * 1000
                
                return TextClassificationResponse(
                    text=request.text,
                    predicted_class=predicted_class,
                    confidence=confidence,
                    all_predictions=predictions,
                    model_version=request.model_version or "default",
                    processing_time_ms=processing_time
                )
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy", "timestamp": datetime.now()}
        
        @self.app.get("/models")
        async def list_models():
            return {"available_models": list(self.models.keys())}
    
    async def _get_model(self, version: Optional[str]):
        """Get or load model"""
        version = version or "default"
        
        if version not in self.models:
            # Mock model loading
            await asyncio.sleep(0.1)  # Simulate loading time
            self.models[version] = {"loaded_at": datetime.now()}
        
        return self.models[version]
    
    async def _classify(self, model, text: str) -> Dict[str, float]:
        """Perform text classification"""
        # Mock classification
        await asyncio.sleep(0.01)  # Simulate inference time
        
        return {
            "positive": 0.7,
            "negative": 0.2,
            "neutral": 0.1
        }

# Embedding Microservice
class EmbeddingService:
    """Microservice for text embeddings"""
    
    def __init__(self):
        self.app = FastAPI(title="Embedding Service", version="1.0.0")
        self.models = {}
        self.setup_routes()
    
    def setup_routes(self):
        """Setup FastAPI routes"""
        
        @self.app.post("/embed", response_model=EmbeddingResponse)
        async def generate_embeddings(request: EmbeddingRequest):
            start_time = datetime.now()
            
            try:
                # Get model
                model = await self._get_model(request.model_type)
                
                # Generate embeddings
                embeddings = await self._generate_embeddings(
                    model, request.texts, request.normalize
                )
                
                processing_time = (datetime.now() - start_time).total_seconds() * 1000
                
                return EmbeddingResponse(
                    embeddings=embeddings,
                    model_type=request.model_type,
                    dimension=len(embeddings[0]) if embeddings else 0,
                    processing_time_ms=processing_time
                )
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy", "timestamp": datetime.now()}
    
    async def _get_model(self, model_type: str):
        """Get or load embedding model"""
        if model_type not in self.models:
            await asyncio.sleep(0.2)  # Simulate loading
            self.models[model_type] = {
                "dimension": 384,
                "loaded_at": datetime.now()
            }
        
        return self.models[model_type]
    
    async def _generate_embeddings(self, model, texts: List[str], 
                                 normalize: bool) -> List[List[float]]:
        """Generate embeddings for texts"""
        # Mock embedding generation
        await asyncio.sleep(0.01 * len(texts))
        
        embeddings = []
        for text in texts:
            # Generate mock embedding
            embedding = [0.1 * i for i in range(model["dimension"])]
            
            if normalize:
                # Simple normalization
                norm = sum(x**2 for x in embedding) ** 0.5
                embedding = [x / norm for x in embedding]
            
            embeddings.append(embedding)
        
        return embeddings

# Service Discovery and Communication
class ServiceRegistry:
    """Service registry for microservices"""
    
    def __init__(self):
        self.services: Dict[str, List[Dict[str, Any]]] = {}
        
    def register_service(self, service_name: str, host: str, port: int,
                        health_check_url: str):
        """Register a service"""
        if service_name not in self.services:
            self.services[service_name] = []
        
        service_info = {
            "host": host,
            "port": port,
            "health_check_url": health_check_url,
            "registered_at": datetime.now(),
            "healthy": True
        }
        
        self.services[service_name].append(service_info)
    
    def get_service_instance(self, service_name: str) -> Optional[Dict[str, Any]]:
        """Get a healthy service instance"""
        if service_name not in self.services:
            return None
        
        healthy_instances = [
            instance for instance in self.services[service_name]
            if instance["healthy"]
        ]
        
        if not healthy_instances:
            return None
        
        # Simple round-robin selection
        return healthy_instances[0]
    
    async def health_check_all(self):
        """Perform health checks on all registered services"""
        async with httpx.AsyncClient() as client:
            for service_name, instances in self.services.items():
                for instance in instances:
                    try:
                        response = await client.get(
                            f"http://{instance['host']}:{instance['port']}{instance['health_check_url']}",
                            timeout=5.0
                        )
                        instance["healthy"] = response.status_code == 200
                    except:
                        instance["healthy"] = False

class ServiceClient:
    """Client for communicating with other microservices"""
    
    def __init__(self, service_registry: ServiceRegistry):
        self.registry = service_registry
        self.http_client = httpx.AsyncClient()
    
    async def call_service(self, service_name: str, endpoint: str,
                          method: str = "GET", data: Optional[Dict] = None) -> Dict[str, Any]:
        """Call another microservice"""
        
        # Get service instance
        instance = self.registry.get_service_instance(service_name)
        if not instance:
            raise Exception(f"No healthy instance found for service {service_name}")
        
        # Build URL
        url = f"http://{instance['host']}:{instance['port']}{endpoint}"
        
        # Make request
        try:
            if method == "GET":
                response = await self.http_client.get(url)
            elif method == "POST":
                response = await self.http_client.post(url, json=data)
            else:
                raise ValueError(f"Unsupported method: {method}")
            
            if response.status_code != 200:
                raise Exception(f"Service call failed: {response.status_code}")
            
            return response.json()
            
        except Exception as e:
            # Mark instance as unhealthy
            instance["healthy"] = False
            raise e

# Orchestration Service
class AIOrchestrationService:
    """Orchestration service that combines multiple AI services"""
    
    def __init__(self, service_registry: ServiceRegistry):
        self.app = FastAPI(title="AI Orchestration Service", version="1.0.0")
        self.registry = service_registry
        self.client = ServiceClient(service_registry)
        self.setup_routes()
    
    def setup_routes(self):
        """Setup orchestration routes"""
        
        @self.app.post("/analyze_text")
        async def analyze_text(request: Dict[str, Any]):
            """Comprehensive text analysis using multiple services"""
            
            text = request.get("text", "")
            if not text:
                raise HTTPException(status_code=400, detail="Text is required")
            
            results = {}
            
            try:
                # Parallel service calls
                tasks = [
                    self._classify_text(text),
                    self._generate_embedding(text),
                ]
                
                classification, embedding = await asyncio.gather(*tasks)
                
                results["classification"] = classification
                results["embedding"] = embedding
                results["status"] = "success"
                
                return results
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
    
    async def _classify_text(self, text: str) -> Dict[str, Any]:
        """Call text classification service"""
        return await self.client.call_service(
            "text-classification",
            "/classify",
            "POST",
            {"text": text}
        )
    
    async def _generate_embedding(self, text: str) -> Dict[str, Any]:
        """Call embedding service"""
        return await self.client.call_service(
            "embedding",
            "/embed",
            "POST",
            {"texts": [text]}
        )

# API Gateway
class APIGateway:
    """API Gateway for routing and cross-cutting concerns"""
    
    def __init__(self, service_registry: ServiceRegistry):
        self.app = FastAPI(title="AI API Gateway", version="1.0.0")
        self.registry = service_registry
        self.client = ServiceClient(service_registry)
        self.rate_limiter = RateLimiter()
        self.auth_service = AuthenticationService()
        self.setup_routes()
    
    def setup_routes(self):
        """Setup gateway routes"""
        
        @self.app.middleware("http")
        async def add_cors_header(request, call_next):
            """Add CORS headers"""
            response = await call_next(request)
            response.headers["Access-Control-Allow-Origin"] = "*"
            return response
        
        @self.app.post("/api/v1/text/classify")
        async def classify_text(
            request: TextClassificationRequest,
            user = Depends(self.auth_service.get_current_user)
        ):
            """Proxy to text classification service"""
            
            # Rate limiting
            if not await self.rate_limiter.allow_request(user["user_id"], "classify"):
                raise HTTPException(status_code=429, detail="Rate limit exceeded")
            
            # Proxy to service
            return await self.client.call_service(
                "text-classification",
                "/classify",
                "POST",
                request.dict()
            )
        
        @self.app.post("/api/v1/text/embed")
        async def generate_embeddings(
            request: EmbeddingRequest,
            user = Depends(self.auth_service.get_current_user)
        ):
            """Proxy to embedding service"""
            
            # Rate limiting
            if not await self.rate_limiter.allow_request(user["user_id"], "embed"):
                raise HTTPException(status_code=429, detail="Rate limit exceeded")
            
            # Proxy to service
            return await self.client.call_service(
                "embedding",
                "/embed",
                "POST",
                request.dict()
            )

class RateLimiter:
    """Rate limiting implementation"""
    
    def __init__(self):
        self.requests: Dict[str, List[datetime]] = {}
        self.limits = {
            "classify": {"requests": 100, "window_minutes": 1},
            "embed": {"requests": 50, "window_minutes": 1}
        }
    
    async def allow_request(self, user_id: str, operation: str) -> bool:
        """Check if request should be allowed"""
        
        if operation not in self.limits:
            return True
        
        key = f"{user_id}:{operation}"
        current_time = datetime.now()
        
        # Initialize if not exists
        if key not in self.requests:
            self.requests[key] = []
        
        # Clean old requests
        window_minutes = self.limits[operation]["window_minutes"]
        cutoff_time = current_time - timedelta(minutes=window_minutes)
        self.requests[key] = [
            req_time for req_time in self.requests[key]
            if req_time > cutoff_time
        ]
        
        # Check limit
        max_requests = self.limits[operation]["requests"]
        if len(self.requests[key]) >= max_requests:
            return False
        
        # Add current request
        self.requests[key].append(current_time)
        return True

class AuthenticationService:
    """Authentication service"""
    
    def __init__(self):
        self.users = {
            "test_user": {
                "user_id": "test_user",
                "email": "test@example.com",
                "role": "user"
            }
        }
    
    async def get_current_user(self, token: str = "test_token") -> Dict[str, Any]:
        """Get current user from token"""
        # Mock authentication
        return self.users["test_user"]
</CodeExample>

## event-driven-ai

### Event-Driven Architecture for AI Systems

Event-driven architectures enable real-time AI systems that respond to data changes, user actions, and system events asynchronously.

<CodeExample language="python">
import asyncio
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
import json
import logging
from enum import Enum
import uuid

class EventType(Enum):
    DATA_INGESTED = "data.ingested"
    MODEL_UPDATED = "model.updated"
    PREDICTION_REQUESTED = "prediction.requested"
    PREDICTION_COMPLETED = "prediction.completed"
    FEEDBACK_RECEIVED = "feedback.received"
    DRIFT_DETECTED = "drift.detected"
    ALERT_TRIGGERED = "alert.triggered"

@dataclass
class Event:
    id: str
    type: EventType
    source: str
    timestamp: datetime
    data: Dict[str, Any]
    correlation_id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class EventBus:
    """Central event bus for publishing and subscribing to events"""
    
    def __init__(self):
        self.subscribers: Dict[EventType, List[Callable]] = {}
        self.event_store: List[Event] = []
        self.logger = logging.getLogger(__name__)
    
    def subscribe(self, event_type: EventType, handler: Callable):
        """Subscribe to an event type"""
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        self.subscribers[event_type].append(handler)
    
    async def publish(self, event: Event):
        """Publish an event to all subscribers"""
        
        # Store event
        self.event_store.append(event)
        
        # Log event
        self.logger.info(f"Published event: {event.type.value} from {event.source}")
        
        # Notify subscribers
        if event.type in self.subscribers:
            tasks = []
            for handler in self.subscribers[event.type]:
                tasks.append(self._handle_event(handler, event))
            
            # Execute handlers concurrently
            await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _handle_event(self, handler: Callable, event: Event):
        """Handle event with error handling"""
        try:
            await handler(event)
        except Exception as e:
            self.logger.error(f"Error handling event {event.id}: {e}")

class DataIngestionService:
    """Service that ingests data and publishes events"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.processed_data: List[Dict[str, Any]] = []
    
    async def ingest_data(self, data: Dict[str, Any], source: str):
        """Ingest new data and publish event"""
        
        # Process data
        processed_item = {
            "id": str(uuid.uuid4()),
            "source": source,
            "data": data,
            "ingested_at": datetime.now().isoformat(),
            "processed": True
        }
        
        self.processed_data.append(processed_item)
        
        # Publish event
        event = Event(
            id=str(uuid.uuid4()),
            type=EventType.DATA_INGESTED,
            source="data-ingestion-service",
            timestamp=datetime.now(),
            data=processed_item
        )
        
        await self.event_bus.publish(event)
        
        return processed_item

class ModelTrainingService:
    """Service that trains models based on data ingestion events"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.training_data: List[Dict[str, Any]] = []
        
        # Subscribe to data ingestion events
        self.event_bus.subscribe(EventType.DATA_INGESTED, self.handle_data_ingested)
    
    async def handle_data_ingested(self, event: Event):
        """Handle data ingestion event"""
        
        data_item = event.data
        self.training_data.append(data_item)
        
        # Check if we have enough data to trigger training
        if len(self.training_data) >= 100:  # Threshold for retraining
            await self.trigger_training()
    
    async def trigger_training(self):
        """Trigger model training"""
        
        # Mock training process
        await asyncio.sleep(1)  # Simulate training time
        
        new_model = {
            "model_id": str(uuid.uuid4()),
            "version": "1.0.0",
            "trained_at": datetime.now().isoformat(),
            "training_data_size": len(self.training_data),
            "performance_metrics": {
                "accuracy": 0.95,
                "f1_score": 0.92
            }
        }
        
        # Clear training data
        self.training_data = []
        
        # Publish model updated event
        event = Event(
            id=str(uuid.uuid4()),
            type=EventType.MODEL_UPDATED,
            source="model-training-service",
            timestamp=datetime.now(),
            data=new_model
        )
        
        await self.event_bus.publish(event)

class ModelDeploymentService:
    """Service that deploys models based on training completion events"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.deployed_models: Dict[str, Dict[str, Any]] = {}
        
        # Subscribe to model update events
        self.event_bus.subscribe(EventType.MODEL_UPDATED, self.handle_model_updated)
    
    async def handle_model_updated(self, event: Event):
        """Handle model update event"""
        
        model_info = event.data
        
        # Validate model before deployment
        is_valid = await self.validate_model(model_info)
        
        if is_valid:
            await self.deploy_model(model_info)
        else:
            logging.warning(f"Model validation failed: {model_info['model_id']}")
    
    async def validate_model(self, model_info: Dict[str, Any]) -> bool:
        """Validate model before deployment"""
        
        # Mock validation
        await asyncio.sleep(0.1)
        
        # Check performance metrics
        accuracy = model_info.get("performance_metrics", {}).get("accuracy", 0)
        return accuracy >= 0.90  # Minimum accuracy threshold
    
    async def deploy_model(self, model_info: Dict[str, Any]):
        """Deploy the model"""
        
        # Mock deployment
        await asyncio.sleep(0.5)
        
        deployment_info = {
            **model_info,
            "deployed_at": datetime.now().isoformat(),
            "deployment_status": "active",
            "endpoint": f"/api/models/{model_info['model_id']}/predict"
        }
        
        self.deployed_models[model_info["model_id"]] = deployment_info
        
        logging.info(f"Model deployed: {model_info['model_id']}")

class PredictionService:
    """Service that handles prediction requests"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.active_model = None
        
        # Subscribe to model deployment events
        self.event_bus.subscribe(EventType.MODEL_UPDATED, self.handle_model_updated)
    
    async def handle_model_updated(self, event: Event):
        """Update active model when new model is deployed"""
        self.active_model = event.data
        logging.info(f"Updated active model to: {self.active_model['model_id']}")
    
    async def predict(self, input_data: Dict[str, Any], 
                     correlation_id: Optional[str] = None) -> Dict[str, Any]:
        """Make a prediction"""
        
        request_id = str(uuid.uuid4())
        correlation_id = correlation_id or request_id
        
        # Publish prediction requested event
        request_event = Event(
            id=request_id,
            type=EventType.PREDICTION_REQUESTED,
            source="prediction-service",
            timestamp=datetime.now(),
            data={
                "input_data": input_data,
                "model_id": self.active_model["model_id"] if self.active_model else None
            },
            correlation_id=correlation_id
        )
        
        await self.event_bus.publish(request_event)
        
        # Perform prediction
        if not self.active_model:
            raise Exception("No active model available")
        
        # Mock prediction
        await asyncio.sleep(0.1)
        prediction_result = {
            "prediction": "positive",
            "confidence": 0.87,
            "model_id": self.active_model["model_id"],
            "request_id": request_id
        }
        
        # Publish prediction completed event
        completion_event = Event(
            id=str(uuid.uuid4()),
            type=EventType.PREDICTION_COMPLETED,
            source="prediction-service",
            timestamp=datetime.now(),
            data=prediction_result,
            correlation_id=correlation_id
        )
        
        await self.event_bus.publish(completion_event)
        
        return prediction_result

class FeedbackCollectionService:
    """Service that collects user feedback"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.feedback_data: List[Dict[str, Any]] = []
    
    async def collect_feedback(self, prediction_id: str, feedback: Dict[str, Any]):
        """Collect user feedback on a prediction"""
        
        feedback_entry = {
            "feedback_id": str(uuid.uuid4()),
            "prediction_id": prediction_id,
            "feedback": feedback,
            "collected_at": datetime.now().isoformat()
        }
        
        self.feedback_data.append(feedback_entry)
        
        # Publish feedback event
        event = Event(
            id=str(uuid.uuid4()),
            type=EventType.FEEDBACK_RECEIVED,
            source="feedback-collection-service",
            timestamp=datetime.now(),
            data=feedback_entry
        )
        
        await self.event_bus.publish(event)

class DriftDetectionService:
    """Service that monitors for data drift"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.prediction_history: List[Dict[str, Any]] = []
        
        # Subscribe to prediction events
        self.event_bus.subscribe(
            EventType.PREDICTION_COMPLETED,
            self.handle_prediction_completed
        )
    
    async def handle_prediction_completed(self, event: Event):
        """Handle completed predictions for drift detection"""
        
        prediction_data = event.data
        self.prediction_history.append(prediction_data)
        
        # Keep only recent predictions
        if len(self.prediction_history) > 1000:
            self.prediction_history = self.prediction_history[-1000:]
        
        # Check for drift periodically
        if len(self.prediction_history) % 100 == 0:
            await self.check_for_drift()
    
    async def check_for_drift(self):
        """Check for data drift"""
        
        # Mock drift detection
        await asyncio.sleep(0.1)
        
        # Simple drift simulation
        recent_predictions = self.prediction_history[-100:]
        avg_confidence = sum(
            p.get("confidence", 0) for p in recent_predictions
        ) / len(recent_predictions)
        
        # Drift detected if average confidence drops significantly
        if avg_confidence < 0.7:
            drift_event = Event(
                id=str(uuid.uuid4()),
                type=EventType.DRIFT_DETECTED,
                source="drift-detection-service",
                timestamp=datetime.now(),
                data={
                    "drift_type": "confidence_degradation",
                    "avg_confidence": avg_confidence,
                    "threshold": 0.7,
                    "sample_size": len(recent_predictions)
                }
            )
            
            await self.event_bus.publish(drift_event)

class AlertingService:
    """Service that handles alerting based on events"""
    
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.alerts: List[Dict[str, Any]] = []
        
        # Subscribe to drift detection events
        self.event_bus.subscribe(EventType.DRIFT_DETECTED, self.handle_drift_detected)
    
    async def handle_drift_detected(self, event: Event):
        """Handle drift detection event"""
        
        drift_data = event.data
        
        alert = {
            "alert_id": str(uuid.uuid4()),
            "type": "model_performance_degradation",
            "severity": "warning",
            "message": f"Data drift detected: {drift_data['drift_type']}",
            "data": drift_data,
            "created_at": datetime.now().isoformat()
        }
        
        self.alerts.append(alert)
        
        # Publish alert event
        alert_event = Event(
            id=str(uuid.uuid4()),
            type=EventType.ALERT_TRIGGERED,
            source="alerting-service",
            timestamp=datetime.now(),
            data=alert
        )
        
        await self.event_bus.publish(alert_event)
        
        # Send actual alert (email, Slack, etc.)
        await self.send_alert(alert)
    
    async def send_alert(self, alert: Dict[str, Any]):
        """Send alert to external systems"""
        logging.warning(f"ALERT: {alert['message']}")

class EventDrivenAISystem:
    """Complete event-driven AI system"""
    
    def __init__(self):
        self.event_bus = EventBus()
        
        # Initialize services
        self.data_ingestion = DataIngestionService(self.event_bus)
        self.model_training = ModelTrainingService(self.event_bus)
        self.model_deployment = ModelDeploymentService(self.event_bus)
        self.prediction = PredictionService(self.event_bus)
        self.feedback_collection = FeedbackCollectionService(self.event_bus)
        self.drift_detection = DriftDetectionService(self.event_bus)
        self.alerting = AlertingService(self.event_bus)
    
    async def simulate_workflow(self):
        """Simulate a complete AI workflow"""
        
        print("Starting event-driven AI system simulation...")
        
        # 1. Ingest some data
        for i in range(150):  # Enough to trigger training
            await self.data_ingestion.ingest_data(
                {"text": f"sample text {i}", "label": "positive"},
                "training_data"
            )
            await asyncio.sleep(0.01)  # Small delay
        
        # Wait for training to complete
        await asyncio.sleep(2)
        
        # 2. Make some predictions
        for i in range(50):
            await self.prediction.predict(
                {"text": f"test input {i}"},
                correlation_id=f"user_session_{i % 10}"
            )
            await asyncio.sleep(0.02)
        
        # 3. Provide some feedback
        await self.feedback_collection.collect_feedback(
            "some_prediction_id",
            {"rating": 4, "correct": True}
        )
        
        # Wait for drift detection
        await asyncio.sleep(1)
        
        print("Simulation complete!")
        print(f"Total events published: {len(self.event_bus.event_store)}")
        print(f"Alerts generated: {len(self.alerting.alerts)}")

# Usage example
async def run_event_driven_system():
    system = EventDrivenAISystem()
    await system.simulate_workflow()

# Run with: asyncio.run(run_event_driven_system())
</CodeExample>

## deployment-strategies

### Modern Deployment and Infrastructure Patterns

<CodeExample language="python">
from typing import Dict, List, Any, Optional
import yaml
import json
from dataclasses import dataclass
from enum import Enum

class DeploymentStrategy(Enum):
    BLUE_GREEN = "blue_green"
    CANARY = "canary"
    ROLLING = "rolling"
    A_B_TEST = "a_b_test"

@dataclass
class DeploymentConfig:
    strategy: DeploymentStrategy
    model_version: str
    traffic_split: Dict[str, float]
    success_criteria: Dict[str, Any]
    rollback_criteria: Dict[str, Any]
    monitoring_duration_minutes: int

class KubernetesDeployment:
    """Kubernetes deployment patterns for AI models"""
    
    def generate_deployment_yaml(self, 
                                model_name: str,
                                model_version: str,
                                config: Dict[str, Any]) -> str:
        """Generate Kubernetes deployment YAML"""
        
        deployment = {
            'apiVersion': 'apps/v1',
            'kind': 'Deployment',
            'metadata': {
                'name': f'{model_name}-{model_version}',
                'labels': {
                    'app': model_name,
                    'version': model_version,
                    'component': 'model-server'
                }
            },
            'spec': {
                'replicas': config.get('replicas', 3),
                'selector': {
                    'matchLabels': {
                        'app': model_name,
                        'version': model_version
                    }
                },
                'template': {
                    'metadata': {
                        'labels': {
                            'app': model_name,
                            'version': model_version
                        }
                    },
                    'spec': {
                        'containers': [{
                            'name': 'model-server',
                            'image': config.get('image', f'{model_name}:{model_version}'),
                            'ports': [{
                                'containerPort': config.get('port', 8000),
                                'name': 'http'
                            }],
                            'env': [
                                {
                                    'name': 'MODEL_VERSION',
                                    'value': model_version
                                },
                                {
                                    'name': 'MODEL_PATH',
                                    'value': config.get('model_path', '/models')
                                }
                            ],
                            'resources': {
                                'requests': {
                                    'memory': config.get('memory_request', '2Gi'),
                                    'cpu': config.get('cpu_request', '1000m'),
                                    'nvidia.com/gpu': config.get('gpu_request', '0')
                                },
                                'limits': {
                                    'memory': config.get('memory_limit', '4Gi'),
                                    'cpu': config.get('cpu_limit', '2000m'),
                                    'nvidia.com/gpu': config.get('gpu_limit', '1')
                                }
                            },
                            'livenessProbe': {
                                'httpGet': {
                                    'path': '/health',
                                    'port': 'http'
                                },
                                'initialDelaySeconds': 30,
                                'periodSeconds': 10
                            },
                            'readinessProbe': {
                                'httpGet': {
                                    'path': '/ready',
                                    'port': 'http'
                                },
                                'initialDelaySeconds': 10,
                                'periodSeconds': 5
                            }
                        }],
                        'nodeSelector': config.get('node_selector', {}),
                        'tolerations': config.get('tolerations', []),
                        'affinity': config.get('affinity', {})
                    }
                }
            }
        }
        
        return yaml.dump(deployment, default_flow_style=False)
    
    def generate_service_yaml(self, model_name: str) -> str:
        """Generate Kubernetes service YAML"""
        
        service = {
            'apiVersion': 'v1',
            'kind': 'Service',
            'metadata': {
                'name': f'{model_name}-service',
                'labels': {
                    'app': model_name
                }
            },
            'spec': {
                'selector': {
                    'app': model_name
                },
                'ports': [{
                    'port': 80,
                    'targetPort': 'http',
                    'protocol': 'TCP'
                }],
                'type': 'ClusterIP'
            }
        }
        
        return yaml.dump(service, default_flow_style=False)
    
    def generate_hpa_yaml(self, model_name: str, 
                         min_replicas: int = 2,
                         max_replicas: int = 10,
                         target_cpu: int = 70) -> str:
        """Generate Horizontal Pod Autoscaler YAML"""
        
        hpa = {
            'apiVersion': 'autoscaling/v2',
            'kind': 'HorizontalPodAutoscaler',
            'metadata': {
                'name': f'{model_name}-hpa'
            },
            'spec': {
                'scaleTargetRef': {
                    'apiVersion': 'apps/v1',
                    'kind': 'Deployment',
                    'name': model_name
                },
                'minReplicas': min_replicas,
                'maxReplicas': max_replicas,
                'metrics': [
                    {
                        'type': 'Resource',
                        'resource': {
                            'name': 'cpu',
                            'target': {
                                'type': 'Utilization',
                                'averageUtilization': target_cpu
                            }
                        }
                    },
                    {
                        'type': 'Resource',
                        'resource': {
                            'name': 'memory',
                            'target': {
                                'type': 'Utilization',
                                'averageUtilization': 80
                            }
                        }
                    }
                ]
            }
        }
        
        return yaml.dump(hpa, default_flow_style=False)

class BlueGreenDeployment:
    """Blue-Green deployment strategy"""
    
    def __init__(self):
        self.environments = {
            'blue': {'active': True, 'version': None},
            'green': {'active': False, 'version': None}
        }
    
    async def deploy(self, new_version: str, config: DeploymentConfig) -> Dict[str, Any]:
        """Execute blue-green deployment"""
        
        # Determine target environment
        current_env = 'blue' if self.environments['blue']['active'] else 'green'
        target_env = 'green' if current_env == 'blue' else 'blue'
        
        deployment_steps = [
            f"1. Deploy version {new_version} to {target_env} environment",
            f"2. Run health checks on {target_env}",
            f"3. Run integration tests on {target_env}",
            f"4. Switch traffic from {current_env} to {target_env}",
            f"5. Monitor for {config.monitoring_duration_minutes} minutes",
            f"6. Keep {current_env} as backup for quick rollback"
        ]
        
        # Mock deployment execution
        for i, step in enumerate(deployment_steps):
            print(f"Executing: {step}")
            await asyncio.sleep(0.1)  # Simulate step execution
        
        # Update environment states
        self.environments[target_env]['active'] = True
        self.environments[target_env]['version'] = new_version
        self.environments[current_env]['active'] = False
        
        return {
            'status': 'success',
            'active_environment': target_env,
            'active_version': new_version,
            'backup_environment': current_env,
            'backup_version': self.environments[current_env]['version']
        }
    
    async def rollback(self) -> Dict[str, Any]:
        """Rollback to previous version"""
        
        current_env = 'blue' if self.environments['blue']['active'] else 'green'
        backup_env = 'green' if current_env == 'blue' else 'blue'
        
        # Switch back
        self.environments[backup_env]['active'] = True
        self.environments[current_env]['active'] = False
        
        return {
            'status': 'rollback_complete',
            'active_environment': backup_env,
            'active_version': self.environments[backup_env]['version']
        }

class CanaryDeployment:
    """Canary deployment strategy"""
    
    def __init__(self):
        self.traffic_splits = {}
        self.metrics_history = []
    
    async def deploy(self, new_version: str, config: DeploymentConfig) -> Dict[str, Any]:
        """Execute canary deployment"""
        
        phases = [
            {'traffic_percent': 5, 'duration_minutes': 10},
            {'traffic_percent': 25, 'duration_minutes': 15},
            {'traffic_percent': 50, 'duration_minutes': 20},
            {'traffic_percent': 100, 'duration_minutes': 5}
        ]
        
        for phase in phases:
            print(f"Canary phase: {phase['traffic_percent']}% traffic to {new_version}")
            
            # Update traffic split
            self.traffic_splits = {
                'stable': 100 - phase['traffic_percent'],
                'canary': phase['traffic_percent']
            }
            
            # Monitor metrics
            metrics = await self._monitor_canary_metrics(
                phase['duration_minutes'], 
                config.success_criteria
            )
            
            # Check success criteria
            if not self._evaluate_success_criteria(metrics, config.success_criteria):
                return await self._rollback_canary()
        
        # Canary successful, promote to stable
        return {
            'status': 'success',
            'promoted_version': new_version,
            'traffic_split': {'stable': 100, 'canary': 0}
        }
    
    async def _monitor_canary_metrics(self, duration_minutes: int, 
                                    criteria: Dict[str, Any]) -> Dict[str, float]:
        """Monitor canary metrics"""
        
        # Mock metrics collection
        await asyncio.sleep(0.1 * duration_minutes)  # Simulate monitoring
        
        metrics = {
            'error_rate': 0.02,  # 2% error rate
            'latency_p95': 150,  # 150ms
            'throughput': 1000,  # 1000 RPS
            'accuracy': 0.95     # 95% accuracy
        }
        
        self.metrics_history.append({
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics,
            'traffic_split': self.traffic_splits.copy()
        })
        
        return metrics
    
    def _evaluate_success_criteria(self, metrics: Dict[str, float],
                                 criteria: Dict[str, Any]) -> bool:
        """Evaluate if metrics meet success criteria"""
        
        for metric_name, threshold in criteria.items():
            if metric_name not in metrics:
                continue
            
            metric_value = metrics[metric_name]
            
            if metric_name in ['error_rate']:
                # Lower is better
                if metric_value > threshold:
                    return False
            elif metric_name in ['latency_p95']:
                # Lower is better
                if metric_value > threshold:
                    return False
            else:
                # Higher is better (accuracy, throughput)
                if metric_value < threshold:
                    return False
        
        return True
    
    async def _rollback_canary(self) -> Dict[str, Any]:
        """Rollback canary deployment"""
        
        self.traffic_splits = {'stable': 100, 'canary': 0}
        
        return {
            'status': 'rollback',
            'reason': 'Success criteria not met',
            'traffic_split': self.traffic_splits
        }

class TerraformInfrastructure:
    """Infrastructure as Code using Terraform patterns"""
    
    def generate_ai_infrastructure(self, config: Dict[str, Any]) -> Dict[str, str]:
        """Generate Terraform configuration for AI infrastructure"""
        
        # Main infrastructure file
        main_tf = f"""
# Provider configuration
terraform {{
  required_providers {{
    aws = {{
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }}
    kubernetes = {{
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }}
  }}
}}

provider "aws" {{
  region = var.aws_region
}}

# EKS Cluster for model serving
module "eks" {{
  source = "./modules/eks"
  
  cluster_name    = var.cluster_name
  cluster_version = var.k8s_version
  
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets
  
  node_groups = {{
    general = {{
      instance_types = ["t3.medium"]
      min_size       = 2
      max_size       = 10
      desired_size   = 3
    }}
    
    gpu = {{
      instance_types = ["g4dn.xlarge"]
      min_size       = 0
      max_size       = 5
      desired_size   = 1
      
      taints = [{{
        key    = "nvidia.com/gpu"
        value  = "true"
        effect = "NO_SCHEDULE"
      }}]
    }}
  }}
}}

# S3 bucket for model artifacts
resource "aws_s3_bucket" "model_artifacts" {{
  bucket = "${{var.project_name}}-model-artifacts"
}}

resource "aws_s3_bucket_versioning" "model_artifacts" {{
  bucket = aws_s3_bucket.model_artifacts.id
  versioning_configuration {{
    status = "Enabled"
  }}
}}

# RDS for metadata storage
resource "aws_db_instance" "metadata" {{
  identifier = "${{var.project_name}}-metadata"
  
  engine         = "postgres"
  engine_version = "14.9"
  instance_class = "db.t3.micro"
  
  allocated_storage     = 20
  max_allocated_storage = 100
  
  db_name  = "ai_metadata"
  username = var.db_username
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  skip_final_snapshot = true
}}

# ElastiCache for caching
resource "aws_elasticache_replication_group" "cache" {{
  replication_group_id       = "${{var.project_name}}-cache"
  description                = "Redis cache for AI system"
  
  node_type            = "cache.t3.micro"
  port                 = 6379
  parameter_group_name = "default.redis7"
  
  num_cache_clusters = 2
  
  subnet_group_name  = aws_elasticache_subnet_group.main.name
  security_group_ids = [aws_security_group.cache.id]
}}

# CloudWatch for monitoring
resource "aws_cloudwatch_log_group" "ai_logs" {{
  name              = "/aws/ai-system/${{var.project_name}}"
  retention_in_days = 14
}}
"""
        
        # Variables file
        variables_tf = f"""
variable "aws_region" {{
  description = "AWS region"
  type        = string
  default     = "us-west-2"
}}

variable "project_name" {{
  description = "Project name"
  type        = string
  default     = "{config.get('project_name', 'ai-system')}"
}}

variable "cluster_name" {{
  description = "EKS cluster name"
  type        = string
  default     = "${{var.project_name}}-eks"
}}

variable "k8s_version" {{
  description = "Kubernetes version"
  type        = string
  default     = "1.27"
}}

variable "db_username" {{
  description = "Database username"
  type        = string
  default     = "ai_admin"
}}

variable "db_password" {{
  description = "Database password"
  type        = string
  sensitive   = true
}}
"""
        
        # Outputs file
        outputs_tf = """
output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = module.eks.cluster_endpoint
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = module.eks.cluster_name
}

output "model_bucket" {
  description = "S3 bucket for model artifacts"
  value       = aws_s3_bucket.model_artifacts.bucket
}

output "database_endpoint" {
  description = "RDS database endpoint"
  value       = aws_db_instance.metadata.endpoint
}

output "cache_endpoint" {
  description = "ElastiCache endpoint"
  value       = aws_elasticache_replication_group.cache.primary_endpoint_address
}
"""
        
        return {
            'main.tf': main_tf,
            'variables.tf': variables_tf,
            'outputs.tf': outputs_tf
        }

class DockerContainerization:
    """Docker containerization patterns for AI models"""
    
    def generate_dockerfile(self, model_type: str, config: Dict[str, Any]) -> str:
        """Generate Dockerfile for AI model"""
        
        if model_type == "pytorch":
            return self._generate_pytorch_dockerfile(config)
        elif model_type == "tensorflow":
            return self._generate_tensorflow_dockerfile(config)
        else:
            return self._generate_generic_dockerfile(config)
    
    def _generate_pytorch_dockerfile(self, config: Dict[str, Any]) -> str:
        """Generate PyTorch-specific Dockerfile"""
        
        return f"""
# Multi-stage build for PyTorch model
FROM python:3.9-slim as base

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    build-essential \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Production stage
FROM base as production

# Copy model artifacts
COPY models/ ./models/
COPY src/ ./src/

# Set environment variables
ENV MODEL_PATH=/app/models
ENV PYTHONPATH=/app/src
ENV WORKERS={config.get('workers', 4)}

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:8000/health || exit 1

# Expose port
EXPOSE 8000

# Run application
CMD ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "$WORKERS"]
"""
    
    def _generate_tensorflow_dockerfile(self, config: Dict[str, Any]) -> str:
        """Generate TensorFlow-specific Dockerfile"""
        
        return f"""
FROM tensorflow/serving:{config.get('tf_version', '2.13.0')}

# Copy model
COPY models/ /models/

# Set environment variables
ENV MODEL_NAME={config.get('model_name', 'model')}
ENV MODEL_BASE_PATH=/models

# Expose ports
EXPOSE 8500 8501

# Start TensorFlow Serving
CMD ["tensorflow_model_server", "--port=8500", "--rest_api_port=8501", "--model_name=$MODEL_NAME", "--model_base_path=$MODEL_BASE_PATH"]
"""
    
    def generate_docker_compose(self, services: List[str]) -> str:
        """Generate docker-compose.yml for multi-service setup"""
        
        compose = {
            'version': '3.8',
            'services': {},
            'networks': {
                'ai-network': {
                    'driver': 'bridge'
                }
            },
            'volumes': {
                'model-data': {},
                'redis-data': {},
                'postgres-data': {}
            }
        }
        
        # Add services
        if 'api-gateway' in services:
            compose['services']['api-gateway'] = {
                'build': './api-gateway',
                'ports': ['8080:8080'],
                'environment': [
                    'MODEL_SERVICE_URL=http://model-service:8000',
                    'REDIS_URL=redis://redis:6379'
                ],
                'depends_on': ['model-service', 'redis'],
                'networks': ['ai-network']
            }
        
        if 'model-service' in services:
            compose['services']['model-service'] = {
                'build': './model-service',
                'ports': ['8000:8000'],
                'environment': [
                    'MODEL_PATH=/models',
                    'POSTGRES_URL=postgresql://user:pass@postgres:5432/ai'
                ],
                'volumes': ['model-data:/models'],
                'depends_on': ['postgres'],
                'networks': ['ai-network']
            }
        
        if 'redis' in services:
            compose['services']['redis'] = {
                'image': 'redis:7-alpine',
                'ports': ['6379:6379'],
                'volumes': ['redis-data:/data'],
                'networks': ['ai-network']
            }
        
        if 'postgres' in services:
            compose['services']['postgres'] = {
                'image': 'postgres:14',
                'environment': [
                    'POSTGRES_DB=ai',
                    'POSTGRES_USER=user',
                    'POSTGRES_PASSWORD=pass'
                ],
                'volumes': ['postgres-data:/var/lib/postgresql/data'],
                'ports': ['5432:5432'],
                'networks': ['ai-network']
            }
        
        return yaml.dump(compose, default_flow_style=False)
</CodeExample>

## quiz

<Quiz>
  <Question
    question="What is the primary benefit of the Model-as-a-Service pattern?"
    options={[
      "It reduces memory usage",
      "It provides centralized model management, versioning, and consistent serving interfaces",
      "It eliminates the need for monitoring",
      "It automatically optimizes model performance"
    ]}
    correct={1}
    explanation="Model-as-a-Service provides centralized management of model metadata, versions, and serving interfaces, enabling consistent deployment patterns and easier model lifecycle management across the organization."
  />
  
  <Question
    question="Which deployment strategy is best for testing new models with minimal risk?"
    options={[
      "Blue-Green deployment",
      "Rolling deployment",
      "Canary deployment",
      "Big Bang deployment"
    ]}
    correct={2}
    explanation="Canary deployment gradually routes traffic to new versions while monitoring metrics, allowing you to detect issues with minimal impact and easily rollback if needed."
  />
  
  <Question
    question="In microservices architecture for AI systems, what is the most important consideration for service boundaries?"
    options={[
      "Database technology",
      "Programming language consistency",
      "Business capability and model lifecycle alignment",
      "Team size"
    ]}
    correct={2}
    explanation="Service boundaries should align with business capabilities and model lifecycles to ensure services can evolve independently while maintaining clear responsibilities for model training, serving, and data processing."
  />
  
  <Question
    question="What is the key advantage of event-driven architecture for AI systems?"
    options={[
      "It's easier to implement than REST APIs",
      "It enables real-time, asynchronous processing of data changes and model updates",
      "It requires less infrastructure",
      "It guarantees faster response times"
    ]}
    correct={1}
    explanation="Event-driven architecture enables real-time, asynchronous processing where services can react to data changes, model updates, and feedback without tight coupling, supporting dynamic ML workflows."
  />
  
  <Question
    question="Which infrastructure pattern is most suitable for variable AI workloads?"
    options={[
      "Fixed-size clusters",
      "Manual scaling",
      "Auto-scaling with GPU node pools",
      "Single large instance"
    ]}
    correct={2}
    explanation="Auto-scaling with GPU node pools provides cost-effective resource management for variable AI workloads, automatically scaling up for training/inference peaks and scaling down during low usage periods."
  />
</Quiz>

## Summary

You've mastered enterprise AI architecture patterns:

âœ… **Enterprise Patterns**: Model-as-a-Service, Feature Store, and Configuration Management
âœ… **Microservices**: Service boundaries, communication patterns, and orchestration
âœ… **Event-Driven Architecture**: Asynchronous processing, event buses, and reactive systems
âœ… **Deployment Strategies**: Blue-Green, Canary, and infrastructure automation
âœ… **Infrastructure Patterns**: Kubernetes, containerization, and Infrastructure as Code

Final module: **Capstone: Build an AI-Powered Application** - Apply everything you've learned to build a complete, production-ready AI application from scratch.