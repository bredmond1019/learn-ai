{
  "metadata": {
    "id": "scaling-strategies",
    "pathId": "production-ai",
    "title": "Scaling Strategies & Load Management",
    "description": "Master the art of scaling AI systems in production, including GPU allocation, model loading optimization, caching strategies, queue-based architectures, and auto-scaling policies specific to AI workloads.",
    "duration": "120 minutes",
    "type": "concept",
    "difficulty": "advanced",
    "order": 3,
    "prerequisites": [
      "Understanding of AI model deployment",
      "Basic knowledge of distributed systems",
      "Familiarity with containerization and orchestration",
      "Experience with production infrastructure"
    ],
    "objectives": [
      "Understand unique challenges of scaling AI systems vs traditional applications",
      "Design horizontal and vertical scaling strategies for AI workloads",
      "Implement intelligent load balancing for AI endpoints",
      "Build caching strategies optimized for AI inference",
      "Architect queue-based systems for handling traffic bursts",
      "Configure auto-scaling policies based on AI-specific metrics",
      "Optimize cost while maintaining performance at scale",
      "Deploy multi-region AI services with data residency considerations"
    ],
    "tags": [
      "scaling",
      "load-management",
      "gpu-optimization",
      "caching",
      "queue-architecture",
      "auto-scaling",
      "cost-optimization",
      "multi-region"
    ],
    "version": "1.0.0",
    "lastUpdated": "2025-06-28",
    "author": "AI Engineering Team",
    "estimatedCompletionTime": 360
  },
  "sections": [
    {
      "id": "scaling-challenges",
      "title": "AI Scaling Challenges",
      "type": "content",
      "order": 1,
      "estimatedDuration": "20 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#scaling-challenges",
        "codeExamples": [
          {
            "id": "resource-comparison",
            "title": "AI vs Traditional App Resource Requirements",
            "description": "Comparing resource needs and scaling characteristics",
            "language": "python",
            "code": "# Traditional Web Application\nclass TraditionalApp:\n    def __init__(self):\n        self.memory_mb = 512  # Typical web app memory\n        self.cpu_cores = 0.5  # Fractional CPU allocation\n        self.startup_time_s = 2  # Fast startup\n        self.stateless = True  # Easy to scale horizontally\n        \n    def handle_request(self, request):\n        # Simple CPU-bound operation\n        return process_request(request)\n\n# AI Application\nclass AIApplication:\n    def __init__(self):\n        self.memory_gb = 16  # Large model memory footprint\n        self.gpu_required = True  # GPU dependency\n        self.model_load_time_s = 30  # Slow model loading\n        self.batch_processing = True  # Efficient batching needed\n        \n    def load_model(self):\n        # Load multi-GB model into GPU memory\n        self.model = load_large_model('model.bin')  # 5-10GB file\n        self.model.to('cuda')  # GPU memory allocation\n        \n    def inference(self, batch):\n        # GPU-accelerated inference\n        with torch.no_grad():\n            return self.model(batch)\n\n# Scaling Implications\nclass ScalingComparison:\n    def __init__(self):\n        self.traditional_scaling = {\n            'horizontal_ease': 'High',  # Easy to add instances\n            'vertical_limit': 'Low',  # Don't need much resources\n            'cold_start': 'Fast',  # Quick to spin up\n            'resource_sharing': 'Efficient',  # Multiple apps per host\n            'cost_model': 'Linear'  # Predictable costs\n        }\n        \n        self.ai_scaling = {\n            'horizontal_ease': 'Medium',  # Model loading overhead\n            'vertical_limit': 'High',  # GPU memory constraints\n            'cold_start': 'Slow',  # Model loading time\n            'resource_sharing': 'Limited',  # GPU exclusivity\n            'cost_model': 'Step-function'  # GPU instance tiers\n        }",
            "highlightLines": [14, 15, 16, 17, 18, 22, 23, 24, 33, 34, 35, 36, 37, 41, 42, 43, 44, 45],
            "runnable": false
          },
          {
            "id": "gpu-allocation-challenge",
            "title": "GPU Memory and Allocation Challenges",
            "description": "Understanding GPU resource constraints in scaling",
            "language": "python",
            "code": "import torch\nimport psutil\nimport GPUtil\nfrom typing import Dict, List, Optional\nimport threading\nimport queue\n\nclass GPUResourceManager:\n    def __init__(self):\n        self.gpus = GPUtil.getGPUs()\n        self.allocation_lock = threading.Lock()\n        self.gpu_allocations = {gpu.id: [] for gpu in self.gpus}\n        \n    def get_gpu_status(self) -> List[Dict]:\n        \"\"\"Get current GPU utilization status\"\"\"\n        status = []\n        for gpu in GPUtil.getGPUs():\n            status.append({\n                'id': gpu.id,\n                'name': gpu.name,\n                'memory_used_mb': gpu.memoryUsed,\n                'memory_total_mb': gpu.memoryTotal,\n                'memory_free_mb': gpu.memoryFree,\n                'utilization_percent': gpu.load * 100,\n                'temperature_c': gpu.temperature\n            })\n        return status\n        \n    def allocate_model_to_gpu(self, model_size_mb: int) -> Optional[int]:\n        \"\"\"Find and allocate GPU for model deployment\"\"\"\n        with self.allocation_lock:\n            for gpu in GPUtil.getGPUs():\n                if gpu.memoryFree > model_size_mb * 1.2:  # 20% overhead\n                    self.gpu_allocations[gpu.id].append(model_size_mb)\n                    return gpu.id\n            return None  # No GPU available\n            \n    def estimate_model_capacity(self) -> Dict:\n        \"\"\"Estimate how many models can fit on available GPUs\"\"\"\n        capacity = {}\n        \n        # Common model sizes (MB)\n        model_sizes = {\n            'small_bert': 420,      # DistilBERT\n            'base_bert': 440,       # BERT-base\n            'large_bert': 1340,     # BERT-large\n            'gpt2': 548,           # GPT-2 base\n            'gpt2_large': 3000,    # GPT-2 large\n            't5_small': 242,       # T5-small\n            't5_base': 892,        # T5-base\n            't5_large': 3000,      # T5-large\n            'llama_7b': 13000,     # LLaMA 7B\n            'llama_13b': 26000     # LLaMA 13B\n        }\n        \n        for gpu in GPUtil.getGPUs():\n            gpu_capacity = {}\n            available_memory = gpu.memoryFree * 0.9  # Keep 10% buffer\n            \n            for model_name, size_mb in model_sizes.items():\n                # Account for inference overhead (activations, etc.)\n                total_required = size_mb * 1.5\n                gpu_capacity[model_name] = int(available_memory // total_required)\n                \n            capacity[f'GPU_{gpu.id}_{gpu.name}'] = {\n                'memory_available_mb': available_memory,\n                'model_capacity': gpu_capacity\n            }\n            \n        return capacity\n\n# Scaling Challenge Example\nclass ModelScalingChallenge:\n    def __init__(self, model_name: str, model_size_mb: int):\n        self.model_name = model_name\n        self.model_size_mb = model_size_mb\n        self.instances = []\n        self.gpu_manager = GPUResourceManager()\n        \n    def scale_up(self, target_instances: int) -> Dict:\n        \"\"\"Attempt to scale to target number of instances\"\"\"\n        results = {\n            'requested': target_instances,\n            'successful': 0,\n            'failed': 0,\n            'failure_reasons': []\n        }\n        \n        current_count = len(self.instances)\n        needed = target_instances - current_count\n        \n        for i in range(needed):\n            gpu_id = self.gpu_manager.allocate_model_to_gpu(self.model_size_mb)\n            if gpu_id is not None:\n                self.instances.append({\n                    'instance_id': f'{self.model_name}-{len(self.instances)}',\n                    'gpu_id': gpu_id,\n                    'status': 'loading'\n                })\n                results['successful'] += 1\n            else:\n                results['failed'] += 1\n                results['failure_reasons'].append(\n                    f'Insufficient GPU memory for instance {i+1}'\n                )\n                \n        return results",
            "highlightLines": [29, 30, 31, 32, 33, 34, 35, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 60, 61, 62, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "scaling-strategies",
      "title": "Horizontal vs Vertical Scaling",
      "type": "content",
      "order": 2,
      "estimatedDuration": "25 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#scaling-strategies",
        "codeExamples": [
          {
            "id": "horizontal-scaling",
            "title": "Horizontal Scaling Implementation",
            "description": "Implementing horizontal scaling for AI services with model replication",
            "language": "python",
            "code": "import asyncio\nimport aiohttp\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom dataclasses import dataclass\nimport time\nimport hashlib\n\n@dataclass\nclass ModelInstance:\n    instance_id: str\n    endpoint: str\n    gpu_id: int\n    model_version: str\n    status: str\n    load_percentage: float\n    last_health_check: float\n\nclass HorizontalScalingOrchestrator:\n    def __init__(self, model_name: str, base_port: int = 8000):\n        self.model_name = model_name\n        self.base_port = base_port\n        self.instances: List[ModelInstance] = []\n        self.health_check_interval = 30  # seconds\n        self.scale_threshold_high = 0.8  # 80% utilization\n        self.scale_threshold_low = 0.3   # 30% utilization\n        \n    async def deploy_instance(self, gpu_id: int) -> ModelInstance:\n        \"\"\"Deploy a new model instance on specified GPU\"\"\"\n        instance_id = f\"{self.model_name}-{len(self.instances)}-{int(time.time())}\"\n        port = self.base_port + len(self.instances)\n        \n        # Kubernetes deployment manifest\n        deployment_config = {\n            'apiVersion': 'apps/v1',\n            'kind': 'Deployment',\n            'metadata': {\n                'name': instance_id,\n                'labels': {'app': self.model_name, 'instance': instance_id}\n            },\n            'spec': {\n                'replicas': 1,\n                'selector': {'matchLabels': {'instance': instance_id}},\n                'template': {\n                    'metadata': {'labels': {'instance': instance_id}},\n                    'spec': {\n                        'containers': [{\n                            'name': 'model-server',\n                            'image': f'{self.model_name}:latest',\n                            'ports': [{'containerPort': 8080}],\n                            'env': [\n                                {'name': 'CUDA_VISIBLE_DEVICES', 'value': str(gpu_id)},\n                                {'name': 'MODEL_NAME', 'value': self.model_name},\n                                {'name': 'INSTANCE_ID', 'value': instance_id}\n                            ],\n                            'resources': {\n                                'limits': {\n                                    'nvidia.com/gpu': 1,\n                                    'memory': '16Gi',\n                                    'cpu': '4'\n                                },\n                                'requests': {\n                                    'nvidia.com/gpu': 1,\n                                    'memory': '12Gi',\n                                    'cpu': '2'\n                                }\n                            },\n                            'readinessProbe': {\n                                'httpGet': {'path': '/health', 'port': 8080},\n                                'initialDelaySeconds': 60,\n                                'periodSeconds': 10\n                            },\n                            'livenessProbe': {\n                                'httpGet': {'path': '/health', 'port': 8080},\n                                'initialDelaySeconds': 90,\n                                'periodSeconds': 30\n                            }\n                        }],\n                        'nodeSelector': {'gpu-type': 'nvidia-a100'}\n                    }\n                }\n            }\n        }\n        \n        # Deploy and wait for readiness\n        instance = ModelInstance(\n            instance_id=instance_id,\n            endpoint=f'http://{instance_id}:8080',\n            gpu_id=gpu_id,\n            model_version='v1.0',\n            status='deploying',\n            load_percentage=0.0,\n            last_health_check=time.time()\n        )\n        \n        self.instances.append(instance)\n        \n        # Simulate deployment (in production, use K8s API)\n        await asyncio.sleep(60)  # Model loading time\n        instance.status = 'ready'\n        \n        return instance\n        \n    async def scale_decision(self) -> Dict[str, any]:\n        \"\"\"Make scaling decisions based on load metrics\"\"\"\n        if not self.instances:\n            return {'action': 'scale_up', 'reason': 'No instances running'}\n            \n        # Calculate average load across instances\n        active_instances = [i for i in self.instances if i.status == 'ready']\n        if not active_instances:\n            return {'action': 'wait', 'reason': 'No ready instances'}\n            \n        avg_load = np.mean([i.load_percentage for i in active_instances])\n        \n        decision = {\n            'current_instances': len(active_instances),\n            'average_load': avg_load,\n            'action': 'none',\n            'reason': ''\n        }\n        \n        if avg_load > self.scale_threshold_high:\n            decision['action'] = 'scale_up'\n            decision['reason'] = f'High load: {avg_load:.1%}'\n            decision['recommended_instances'] = min(\n                len(active_instances) + 2,  # Add 2 instances\n                8  # Maximum instances\n            )\n        elif avg_load < self.scale_threshold_low and len(active_instances) > 1:\n            decision['action'] = 'scale_down'\n            decision['reason'] = f'Low load: {avg_load:.1%}'\n            decision['recommended_instances'] = max(\n                len(active_instances) - 1,  # Remove 1 instance\n                1  # Minimum instances\n            )\n            \n        return decision\n\n# Load Balancer for Horizontal Scaling\nclass AILoadBalancer:\n    def __init__(self, instances: List[ModelInstance]):\n        self.instances = instances\n        self.request_counter = 0\n        self.routing_strategy = 'least_loaded'  # or 'round_robin', 'hash_based'\n        \n    def select_instance(self, request_id: str) -> Optional[ModelInstance]:\n        \"\"\"Select best instance for request\"\"\"\n        ready_instances = [i for i in self.instances if i.status == 'ready']\n        \n        if not ready_instances:\n            return None\n            \n        if self.routing_strategy == 'round_robin':\n            instance = ready_instances[self.request_counter % len(ready_instances)]\n            self.request_counter += 1\n            return instance\n            \n        elif self.routing_strategy == 'least_loaded':\n            return min(ready_instances, key=lambda x: x.load_percentage)\n            \n        elif self.routing_strategy == 'hash_based':\n            # Consistent hashing for session affinity\n            hash_value = int(hashlib.md5(request_id.encode()).hexdigest(), 16)\n            return ready_instances[hash_value % len(ready_instances)]\n            \n    async def route_request(self, request_data: Dict) -> Dict:\n        \"\"\"Route inference request to selected instance\"\"\"\n        instance = self.select_instance(request_data.get('request_id', str(time.time())))\n        \n        if not instance:\n            return {'error': 'No available instances'}\n            \n        # Forward request to selected instance\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{instance.endpoint}/inference\",\n                json=request_data\n            ) as response:\n                result = await response.json()\n                \n        # Update instance load (simplified)\n        instance.load_percentage = min(instance.load_percentage + 0.1, 1.0)\n        \n        return {\n            'result': result,\n            'routed_to': instance.instance_id,\n            'instance_load': instance.load_percentage\n        }",
            "highlightLines": [25, 26, 34, 35, 36, 37, 38, 51, 52, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 110, 111, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 146, 147, 148, 149, 153, 154, 155, 157, 158, 159, 160],
            "runnable": false
          },
          {
            "id": "vertical-scaling",
            "title": "Vertical Scaling Strategy",
            "description": "Implementing vertical scaling for GPU-intensive AI workloads",
            "language": "python",
            "code": "import torch\nimport psutil\nimport GPUtil\nfrom typing import Dict, List, Optional, Tuple\nimport threading\nimport time\nfrom enum import Enum\n\nclass GPUTier(Enum):\n    T4 = {'memory_gb': 16, 'compute_capability': 7.5, 'cost_per_hour': 0.35}\n    V100 = {'memory_gb': 32, 'compute_capability': 7.0, 'cost_per_hour': 2.48}\n    A100_40GB = {'memory_gb': 40, 'compute_capability': 8.0, 'cost_per_hour': 3.67}\n    A100_80GB = {'memory_gb': 80, 'compute_capability': 8.0, 'cost_per_hour': 5.12}\n    H100 = {'memory_gb': 80, 'compute_capability': 9.0, 'cost_per_hour': 8.00}\n\nclass VerticalScalingManager:\n    def __init__(self, initial_gpu_tier: GPUTier):\n        self.current_tier = initial_gpu_tier\n        self.performance_history = []\n        self.scaling_lock = threading.Lock()\n        self.cost_threshold = 10.0  # Max $/hour\n        \n    def analyze_performance_metrics(self) -> Dict:\n        \"\"\"Analyze current performance and resource utilization\"\"\"\n        metrics = {\n            'timestamp': time.time(),\n            'gpu_utilization': 0.0,\n            'memory_usage_gb': 0.0,\n            'inference_latency_ms': 0.0,\n            'batch_size': 0,\n            'throughput_rps': 0.0\n        }\n        \n        # Get GPU metrics\n        gpus = GPUtil.getGPUs()\n        if gpus:\n            gpu = gpus[0]\n            metrics['gpu_utilization'] = gpu.load\n            metrics['memory_usage_gb'] = gpu.memoryUsed / 1024\n            \n        # Simulate performance metrics (in production, get from monitoring)\n        metrics['inference_latency_ms'] = 50 + (metrics['gpu_utilization'] * 100)\n        metrics['batch_size'] = 32 if self.current_tier != GPUTier.T4 else 16\n        metrics['throughput_rps'] = 1000 / metrics['inference_latency_ms'] * metrics['batch_size']\n        \n        self.performance_history.append(metrics)\n        return metrics\n        \n    def recommend_scaling_action(self) -> Tuple[str, Optional[GPUTier], Dict]:\n        \"\"\"Recommend vertical scaling action based on metrics\"\"\"\n        if len(self.performance_history) < 5:\n            return 'wait', None, {'reason': 'Insufficient data'}\n            \n        recent_metrics = self.performance_history[-5:]\n        \n        # Calculate averages\n        avg_gpu_util = np.mean([m['gpu_utilization'] for m in recent_metrics])\n        avg_memory_usage = np.mean([m['memory_usage_gb'] for m in recent_metrics])\n        avg_latency = np.mean([m['inference_latency_ms'] for m in recent_metrics])\n        \n        current_memory = self.current_tier.value['memory_gb']\n        current_cost = self.current_tier.value['cost_per_hour']\n        \n        analysis = {\n            'current_tier': self.current_tier.name,\n            'avg_gpu_utilization': avg_gpu_util,\n            'avg_memory_usage_gb': avg_memory_usage,\n            'avg_latency_ms': avg_latency,\n            'memory_utilization': avg_memory_usage / current_memory\n        }\n        \n        # Scaling up conditions\n        if avg_gpu_util > 0.9 or analysis['memory_utilization'] > 0.85:\n            # Need more resources\n            next_tier = self._find_next_tier_up()\n            if next_tier and next_tier.value['cost_per_hour'] <= self.cost_threshold:\n                return 'scale_up', next_tier, {\n                    **analysis,\n                    'reason': 'High resource utilization',\n                    'expected_improvement': self._estimate_improvement(next_tier)\n                }\n                \n        # Scaling down conditions\n        elif avg_gpu_util < 0.3 and analysis['memory_utilization'] < 0.4:\n            # Overprovisioned\n            next_tier = self._find_next_tier_down()\n            if next_tier:\n                return 'scale_down', next_tier, {\n                    **analysis,\n                    'reason': 'Low resource utilization',\n                    'expected_savings': current_cost - next_tier.value['cost_per_hour']\n                }\n                \n        return 'maintain', None, analysis\n        \n    def _find_next_tier_up(self) -> Optional[GPUTier]:\n        \"\"\"Find next higher GPU tier\"\"\"\n        tier_order = [GPUTier.T4, GPUTier.V100, GPUTier.A100_40GB, GPUTier.A100_80GB, GPUTier.H100]\n        current_index = tier_order.index(self.current_tier)\n        \n        if current_index < len(tier_order) - 1:\n            return tier_order[current_index + 1]\n        return None\n        \n    def _find_next_tier_down(self) -> Optional[GPUTier]:\n        \"\"\"Find next lower GPU tier\"\"\"\n        tier_order = [GPUTier.T4, GPUTier.V100, GPUTier.A100_40GB, GPUTier.A100_80GB, GPUTier.H100]\n        current_index = tier_order.index(self.current_tier)\n        \n        if current_index > 0:\n            return tier_order[current_index - 1]\n        return None\n        \n    def _estimate_improvement(self, new_tier: GPUTier) -> Dict:\n        \"\"\"Estimate performance improvement from scaling up\"\"\"\n        current_compute = self.current_tier.value['compute_capability']\n        new_compute = new_tier.value['compute_capability']\n        \n        current_memory = self.current_tier.value['memory_gb']\n        new_memory = new_tier.value['memory_gb']\n        \n        return {\n            'compute_improvement': (new_compute / current_compute - 1) * 100,\n            'memory_increase': (new_memory / current_memory - 1) * 100,\n            'estimated_latency_reduction': min(30, (new_compute / current_compute - 1) * 20),\n            'max_batch_size_increase': (new_memory / current_memory) * 100\n        }\n        \n    async def execute_scaling(self, new_tier: GPUTier) -> Dict:\n        \"\"\"Execute vertical scaling operation\"\"\"\n        with self.scaling_lock:\n            scaling_result = {\n                'start_time': time.time(),\n                'from_tier': self.current_tier.name,\n                'to_tier': new_tier.name,\n                'status': 'in_progress'\n            }\n            \n            try:\n                # Steps for vertical scaling:\n                # 1. Provision new instance with higher tier GPU\n                # 2. Load model on new instance\n                # 3. Warm up the model\n                # 4. Switch traffic to new instance\n                # 5. Decommission old instance\n                \n                # Simulate scaling process\n                await asyncio.sleep(120)  # 2 minutes for new instance\n                \n                self.current_tier = new_tier\n                scaling_result['status'] = 'completed'\n                scaling_result['end_time'] = time.time()\n                scaling_result['duration_seconds'] = scaling_result['end_time'] - scaling_result['start_time']\n                \n            except Exception as e:\n                scaling_result['status'] = 'failed'\n                scaling_result['error'] = str(e)\n                \n            return scaling_result",
            "highlightLines": [9, 10, 11, 12, 13, 14, 37, 38, 39, 41, 42, 43, 56, 57, 58, 59, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 90, 119, 120, 121, 122, 123],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "load-balancing",
      "title": "Load Balancing for AI Endpoints",
      "type": "content",
      "order": 3,
      "estimatedDuration": "20 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#load-balancing",
        "codeExamples": [
          {
            "id": "intelligent-load-balancer",
            "title": "Intelligent AI Load Balancer",
            "description": "Advanced load balancing with model-aware routing and request batching",
            "language": "python",
            "code": "import asyncio\nimport aiohttp\nfrom typing import List, Dict, Optional, Tuple\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport time\nimport heapq\nfrom collections import defaultdict\nimport threading\n\n@dataclass\nclass InferenceRequest:\n    request_id: str\n    data: Dict\n    priority: int = 0\n    arrival_time: float = field(default_factory=time.time)\n    required_model: Optional[str] = None\n    max_latency_ms: Optional[float] = None\n    \n    def __lt__(self, other):\n        return self.priority > other.priority  # Higher priority first\n\n@dataclass\nclass ModelEndpoint:\n    endpoint_id: str\n    url: str\n    model_name: str\n    model_version: str\n    capacity: int  # Max batch size\n    current_load: int = 0\n    avg_latency_ms: float = 50.0\n    success_rate: float = 0.99\n    last_updated: float = field(default_factory=time.time)\n    gpu_memory_gb: int = 16\n    supports_batching: bool = True\n\nclass IntelligentLoadBalancer:\n    def __init__(self, endpoints: List[ModelEndpoint]):\n        self.endpoints = {ep.endpoint_id: ep for ep in endpoints}\n        self.request_queue = []  # Priority queue\n        self.batch_window_ms = 50  # Time to wait for batching\n        self.routing_strategy = 'latency_aware'  # or 'round_robin', 'least_loaded'\n        self.metrics = defaultdict(lambda: {'requests': 0, 'latency': [], 'errors': 0})\n        self._lock = threading.Lock()\n        self._stop_batching = False\n        \n        # Start batch processor\n        asyncio.create_task(self._batch_processor())\n        \n    async def route_request(self, request: InferenceRequest) -> Dict:\n        \"\"\"Route single request or add to batch queue\"\"\"\n        \n        # Find compatible endpoints\n        compatible_endpoints = self._find_compatible_endpoints(request)\n        \n        if not compatible_endpoints:\n            return {'error': 'No compatible endpoints available'}\n            \n        # Check if request requires immediate processing\n        if request.max_latency_ms and request.max_latency_ms < self.batch_window_ms:\n            # Route immediately without batching\n            endpoint = self._select_endpoint(compatible_endpoints, [request])\n            return await self._send_request(endpoint, [request])\n            \n        # Add to batch queue\n        with self._lock:\n            heapq.heappush(self.request_queue, request)\n            \n        # Wait for batch processing\n        return {'status': 'queued', 'request_id': request.request_id}\n        \n    def _find_compatible_endpoints(self, request: InferenceRequest) -> List[ModelEndpoint]:\n        \"\"\"Find endpoints that can handle the request\"\"\"\n        compatible = []\n        \n        for endpoint in self.endpoints.values():\n            # Check model compatibility\n            if request.required_model and endpoint.model_name != request.required_model:\n                continue\n                \n            # Check if endpoint is healthy\n            if endpoint.success_rate < 0.9:\n                continue\n                \n            # Check capacity\n            if endpoint.current_load >= endpoint.capacity:\n                continue\n                \n            compatible.append(endpoint)\n            \n        return compatible\n        \n    def _select_endpoint(self, endpoints: List[ModelEndpoint], batch: List[InferenceRequest]) -> ModelEndpoint:\n        \"\"\"Select best endpoint based on routing strategy\"\"\"\n        \n        if self.routing_strategy == 'latency_aware':\n            # Consider both latency and current load\n            scores = []\n            for ep in endpoints:\n                # Lower score is better\n                latency_score = ep.avg_latency_ms\n                load_score = (ep.current_load / ep.capacity) * 100\n                total_score = latency_score + load_score\n                scores.append((total_score, ep))\n                \n            return min(scores, key=lambda x: x[0])[1]\n            \n        elif self.routing_strategy == 'least_loaded':\n            return min(endpoints, key=lambda ep: ep.current_load / ep.capacity)\n            \n        elif self.routing_strategy == 'round_robin':\n            # Simple round-robin (would need counter in production)\n            return endpoints[0]\n            \n        else:\n            # Default to first available\n            return endpoints[0]\n            \n    async def _batch_processor(self):\n        \"\"\"Process requests in batches for efficiency\"\"\"\n        while not self._stop_batching:\n            await asyncio.sleep(self.batch_window_ms / 1000)\n            \n            with self._lock:\n                if not self.request_queue:\n                    continue\n                    \n                # Group requests by model requirement\n                model_batches = defaultdict(list)\n                temp_queue = []\n                \n                while self.request_queue and len(temp_queue) < 100:\n                    request = heapq.heappop(self.request_queue)\n                    model_key = request.required_model or 'default'\n                    model_batches[model_key].append(request)\n                    \n                # Put back any unprocessed requests\n                for req in temp_queue:\n                    heapq.heappush(self.request_queue, req)\n                    \n            # Process each model's batch\n            for model_name, batch in model_batches.items():\n                await self._process_batch(model_name, batch)\n                \n    async def _process_batch(self, model_name: str, batch: List[InferenceRequest]):\n        \"\"\"Process a batch of requests for a specific model\"\"\"\n        \n        # Find compatible endpoints\n        compatible_endpoints = [\n            ep for ep in self.endpoints.values()\n            if ep.model_name == model_name or model_name == 'default'\n        ]\n        \n        if not compatible_endpoints:\n            # Handle error for all requests in batch\n            for request in batch:\n                self.metrics[request.request_id]['errors'] += 1\n            return\n            \n        # Split batch if it exceeds endpoint capacity\n        endpoint = self._select_endpoint(compatible_endpoints, batch)\n        \n        if len(batch) > endpoint.capacity:\n            # Split into smaller batches\n            for i in range(0, len(batch), endpoint.capacity):\n                sub_batch = batch[i:i + endpoint.capacity]\n                await self._send_request(endpoint, sub_batch)\n        else:\n            await self._send_request(endpoint, batch)\n            \n    async def _send_request(self, endpoint: ModelEndpoint, batch: List[InferenceRequest]) -> Dict:\n        \"\"\"Send batch request to endpoint\"\"\"\n        \n        # Update endpoint load\n        endpoint.current_load += len(batch)\n        \n        # Prepare batch data\n        batch_data = {\n            'requests': [\n                {\n                    'id': req.request_id,\n                    'data': req.data,\n                    'priority': req.priority\n                }\n                for req in batch\n            ],\n            'batch_size': len(batch)\n        }\n        \n        start_time = time.time()\n        \n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    f\"{endpoint.url}/batch_inference\",\n                    json=batch_data,\n                    timeout=aiohttp.ClientTimeout(total=30)\n                ) as response:\n                    result = await response.json()\n                    \n            # Update metrics\n            latency_ms = (time.time() - start_time) * 1000\n            endpoint.avg_latency_ms = 0.9 * endpoint.avg_latency_ms + 0.1 * latency_ms\n            \n            for req in batch:\n                self.metrics[req.request_id]['latency'].append(latency_ms)\n                self.metrics[req.request_id]['requests'] += 1\n                \n            return {\n                'status': 'success',\n                'results': result,\n                'endpoint': endpoint.endpoint_id,\n                'latency_ms': latency_ms,\n                'batch_size': len(batch)\n            }\n            \n        except Exception as e:\n            # Update error metrics\n            endpoint.success_rate *= 0.95  # Decay success rate\n            \n            for req in batch:\n                self.metrics[req.request_id]['errors'] += 1\n                \n            return {\n                'status': 'error',\n                'error': str(e),\n                'endpoint': endpoint.endpoint_id\n            }\n            \n        finally:\n            # Update endpoint load\n            endpoint.current_load -= len(batch)\n            endpoint.last_updated = time.time()\n            \n    def get_endpoint_health(self) -> Dict[str, Dict]:\n        \"\"\"Get health status of all endpoints\"\"\"\n        health_status = {}\n        \n        for ep_id, endpoint in self.endpoints.items():\n            health_status[ep_id] = {\n                'model': endpoint.model_name,\n                'version': endpoint.model_version,\n                'current_load': endpoint.current_load,\n                'capacity': endpoint.capacity,\n                'utilization': endpoint.current_load / endpoint.capacity,\n                'avg_latency_ms': endpoint.avg_latency_ms,\n                'success_rate': endpoint.success_rate,\n                'gpu_memory_gb': endpoint.gpu_memory_gb,\n                'last_updated': time.time() - endpoint.last_updated\n            }\n            \n        return health_status",
            "highlightLines": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 40, 41, 42, 47, 48, 56, 57, 58, 59, 60, 61, 64, 65, 66, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 116, 117, 118, 119, 127, 128, 129, 130, 131, 138, 139, 140, 156, 157, 158, 159, 160, 161, 162, 163, 194, 195, 196, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "caching-strategies",
      "title": "Caching Strategies for AI",
      "type": "content",
      "order": 4,
      "estimatedDuration": "20 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#caching-strategies",
        "codeExamples": [
          {
            "id": "ai-response-cache",
            "title": "Intelligent AI Response Caching",
            "description": "Implementing semantic caching for AI inference results",
            "language": "python",
            "code": "import hashlib\nimport json\nimport time\nimport numpy as np\nfrom typing import Dict, Any, Optional, List, Tuple\nimport redis\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport pickle\nfrom dataclasses import dataclass\nimport asyncio\nfrom collections import OrderedDict\n\n@dataclass\nclass CacheEntry:\n    key: str\n    response: Any\n    embedding: Optional[np.ndarray]\n    timestamp: float\n    hit_count: int = 0\n    compute_time_ms: float = 0\n    model_version: str = ''\n    confidence: float = 0.0\n\nclass AIResponseCache:\n    def __init__(self, \n                 cache_size_mb: int = 1024,\n                 ttl_seconds: int = 3600,\n                 similarity_threshold: float = 0.95):\n        \"\"\"\n        Initialize AI response cache with semantic similarity matching\n        \n        Args:\n            cache_size_mb: Maximum cache size in MB\n            ttl_seconds: Time to live for cache entries\n            similarity_threshold: Threshold for semantic similarity matching\n        \"\"\"\n        self.cache_size_mb = cache_size_mb\n        self.ttl_seconds = ttl_seconds\n        self.similarity_threshold = similarity_threshold\n        \n        # Initialize caching backends\n        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n        self.local_cache = OrderedDict()  # LRU cache\n        \n        # Semantic similarity components\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n        self.index = faiss.IndexFlatIP(384)  # Inner product for cosine similarity\n        self.key_to_embedding = {}\n        \n        # Metrics\n        self.metrics = {\n            'hits': 0,\n            'misses': 0,\n            'semantic_hits': 0,\n            'evictions': 0,\n            'total_compute_saved_ms': 0\n        }\n        \n    def _generate_cache_key(self, request: Dict[str, Any]) -> str:\n        \"\"\"Generate deterministic cache key from request\"\"\"\n        # Sort keys for consistency\n        normalized = json.dumps(request, sort_keys=True)\n        return hashlib.sha256(normalized.encode()).hexdigest()\n        \n    def _should_cache(self, response: Any, compute_time_ms: float) -> bool:\n        \"\"\"Determine if response should be cached\"\"\"\n        # Don't cache errors\n        if isinstance(response, dict) and 'error' in response:\n            return False\n            \n        # Don't cache fast computations (likely already cached upstream)\n        if compute_time_ms < 10:\n            return False\n            \n        # Check response size\n        response_size = len(json.dumps(response).encode())\n        if response_size > 1024 * 1024:  # 1MB limit per entry\n            return False\n            \n        return True\n        \n    async def get_or_compute(self, \n                           request: Dict[str, Any],\n                           compute_fn,\n                           model_version: str = 'v1.0') -> Tuple[Any, bool]:\n        \"\"\"Get from cache or compute and cache result\"\"\"\n        \n        # Try exact match first\n        cache_key = self._generate_cache_key(request)\n        cached_response = await self._get_exact_match(cache_key)\n        \n        if cached_response:\n            self.metrics['hits'] += 1\n            self.metrics['total_compute_saved_ms'] += cached_response.compute_time_ms\n            return cached_response.response, True\n            \n        # Try semantic similarity match\n        semantic_match = await self._find_semantic_match(request)\n        \n        if semantic_match:\n            self.metrics['semantic_hits'] += 1\n            self.metrics['total_compute_saved_ms'] += semantic_match.compute_time_ms\n            return semantic_match.response, True\n            \n        # Cache miss - compute result\n        self.metrics['misses'] += 1\n        start_time = time.time()\n        \n        response = await compute_fn(request)\n        \n        compute_time_ms = (time.time() - start_time) * 1000\n        \n        # Cache the result if appropriate\n        if self._should_cache(response, compute_time_ms):\n            await self._cache_response(\n                cache_key, request, response, compute_time_ms, model_version\n            )\n            \n        return response, False\n        \n    async def _get_exact_match(self, cache_key: str) -> Optional[CacheEntry]:\n        \"\"\"Get exact match from cache\"\"\"\n        \n        # Check local cache first\n        if cache_key in self.local_cache:\n            entry = self.local_cache[cache_key]\n            if time.time() - entry.timestamp < self.ttl_seconds:\n                entry.hit_count += 1\n                # Move to end (LRU)\n                self.local_cache.move_to_end(cache_key)\n                return entry\n            else:\n                # Expired\n                del self.local_cache[cache_key]\n                \n        # Check Redis\n        redis_data = self.redis_client.get(f\"ai_cache:{cache_key}\")\n        if redis_data:\n            entry = pickle.loads(redis_data)\n            if time.time() - entry.timestamp < self.ttl_seconds:\n                entry.hit_count += 1\n                # Add to local cache\n                self._add_to_local_cache(cache_key, entry)\n                return entry\n            else:\n                # Expired\n                self.redis_client.delete(f\"ai_cache:{cache_key}\")\n                \n        return None\n        \n    async def _find_semantic_match(self, request: Dict[str, Any]) -> Optional[CacheEntry]:\n        \"\"\"Find semantically similar cached response\"\"\"\n        \n        if self.index.ntotal == 0:\n            return None\n            \n        # Generate embedding for request\n        request_text = json.dumps(request, sort_keys=True)\n        request_embedding = self.encoder.encode([request_text])[0]\n        request_embedding = request_embedding / np.linalg.norm(request_embedding)\n        \n        # Search for similar embeddings\n        distances, indices = self.index.search(\n            request_embedding.reshape(1, -1), \n            k=min(10, self.index.ntotal)\n        )\n        \n        # Check similarity threshold\n        if distances[0][0] < self.similarity_threshold:\n            return None\n            \n        # Find the corresponding cache key\n        for embedding_key, embedding in self.key_to_embedding.items():\n            if np.allclose(embedding, self.index.reconstruct(int(indices[0][0]))):\n                return await self._get_exact_match(embedding_key)\n                \n        return None\n        \n    async def _cache_response(self, \n                            cache_key: str,\n                            request: Dict[str, Any],\n                            response: Any,\n                            compute_time_ms: float,\n                            model_version: str):\n        \"\"\"Cache the response with semantic embedding\"\"\"\n        \n        # Generate embedding\n        request_text = json.dumps(request, sort_keys=True)\n        embedding = self.encoder.encode([request_text])[0]\n        embedding = embedding / np.linalg.norm(embedding)\n        \n        # Create cache entry\n        entry = CacheEntry(\n            key=cache_key,\n            response=response,\n            embedding=embedding,\n            timestamp=time.time(),\n            compute_time_ms=compute_time_ms,\n            model_version=model_version,\n            confidence=response.get('confidence', 0.0) if isinstance(response, dict) else 0.0\n        )\n        \n        # Add to caches\n        self._add_to_local_cache(cache_key, entry)\n        \n        # Add to Redis with TTL\n        self.redis_client.setex(\n            f\"ai_cache:{cache_key}\",\n            self.ttl_seconds,\n            pickle.dumps(entry)\n        )\n        \n        # Add embedding to FAISS index\n        self.index.add(embedding.reshape(1, -1))\n        self.key_to_embedding[cache_key] = embedding\n        \n    def _add_to_local_cache(self, cache_key: str, entry: CacheEntry):\n        \"\"\"Add entry to local LRU cache with size management\"\"\"\n        \n        # Check cache size\n        current_size = sum(\n            len(pickle.dumps(e)) for e in self.local_cache.values()\n        )\n        \n        # Evict if necessary\n        while current_size > self.cache_size_mb * 1024 * 1024 and self.local_cache:\n            # Remove least recently used\n            evicted_key, evicted_entry = self.local_cache.popitem(last=False)\n            current_size -= len(pickle.dumps(evicted_entry))\n            self.metrics['evictions'] += 1\n            \n        # Add new entry\n        self.local_cache[cache_key] = entry\n        \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive cache statistics\"\"\"\n        total_requests = self.metrics['hits'] + self.metrics['misses']\n        \n        return {\n            'total_requests': total_requests,\n            'hit_rate': self.metrics['hits'] / total_requests if total_requests > 0 else 0,\n            'semantic_hit_rate': self.metrics['semantic_hits'] / total_requests if total_requests > 0 else 0,\n            'exact_hits': self.metrics['hits'] - self.metrics['semantic_hits'],\n            'misses': self.metrics['misses'],\n            'evictions': self.metrics['evictions'],\n            'compute_time_saved_hours': self.metrics['total_compute_saved_ms'] / (1000 * 3600),\n            'local_cache_entries': len(self.local_cache),\n            'semantic_index_size': self.index.ntotal,\n            'cache_size_mb': sum(len(pickle.dumps(e)) for e in self.local_cache.values()) / (1024 * 1024)\n        }\n        \n    async def invalidate_by_model(self, model_version: str):\n        \"\"\"Invalidate all cache entries for a specific model version\"\"\"\n        \n        # Clear from local cache\n        keys_to_remove = [\n            k for k, v in self.local_cache.items() \n            if v.model_version == model_version\n        ]\n        \n        for key in keys_to_remove:\n            del self.local_cache[key]\n            \n        # Clear from Redis (would need to maintain index in production)\n        # For now, clear all and rebuild\n        for key in self.redis_client.scan_iter(\"ai_cache:*\"):\n            entry_data = self.redis_client.get(key)\n            if entry_data:\n                entry = pickle.loads(entry_data)\n                if entry.model_version == model_version:\n                    self.redis_client.delete(key)",
            "highlightLines": [26, 27, 28, 29, 41, 42, 45, 46, 47, 48, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 108, 109, 110, 111, 112, 113, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 206, 207, 208, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "queue-architecture",
      "title": "Queue-Based Architecture",
      "type": "content",
      "order": 5,
      "estimatedDuration": "20 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#queue-architecture",
        "codeExamples": [
          {
            "id": "ai-queue-system",
            "title": "Queue-Based AI Request Processing",
            "description": "Implementing robust queue architecture for handling AI inference bursts",
            "language": "python",
            "code": "import asyncio\nimport aioredis\nfrom typing import Dict, Any, List, Optional, Callable\nimport json\nimport time\nfrom dataclasses import dataclass, asdict\nimport uuid\nfrom enum import Enum\nimport logging\nfrom collections import defaultdict\nimport numpy as np\n\nclass Priority(Enum):\n    CRITICAL = 0\n    HIGH = 1\n    NORMAL = 2\n    LOW = 3\n    BATCH = 4\n\n@dataclass\nclass AIRequest:\n    request_id: str\n    payload: Dict[str, Any]\n    priority: Priority\n    created_at: float\n    deadline_ms: Optional[float] = None\n    model_name: str = 'default'\n    batch_compatible: bool = True\n    max_retries: int = 3\n    retry_count: int = 0\n    callback_url: Optional[str] = None\n    \n    def to_json(self) -> str:\n        data = asdict(self)\n        data['priority'] = self.priority.value\n        return json.dumps(data)\n        \n    @classmethod\n    def from_json(cls, data: str) -> 'AIRequest':\n        obj = json.loads(data)\n        obj['priority'] = Priority(obj['priority'])\n        return cls(**obj)\n\nclass AIQueueSystem:\n    def __init__(self, \n                 redis_url: str = 'redis://localhost',\n                 num_workers: int = 4,\n                 batch_size: int = 32,\n                 batch_timeout_ms: int = 100):\n        \"\"\"\n        Initialize queue-based AI processing system\n        \n        Args:\n            redis_url: Redis connection URL\n            num_workers: Number of worker processes\n            batch_size: Maximum batch size for processing\n            batch_timeout_ms: Maximum time to wait for batch formation\n        \"\"\"\n        self.redis_url = redis_url\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n        self.batch_timeout_ms = batch_timeout_ms\n        \n        self.redis = None\n        self.workers = []\n        self.running = False\n        \n        # Queue names by priority\n        self.queue_names = {\n            Priority.CRITICAL: 'ai:queue:critical',\n            Priority.HIGH: 'ai:queue:high',\n            Priority.NORMAL: 'ai:queue:normal',\n            Priority.LOW: 'ai:queue:low',\n            Priority.BATCH: 'ai:queue:batch'\n        }\n        \n        # Metrics\n        self.metrics = defaultdict(lambda: {\n            'processed': 0,\n            'failed': 0,\n            'retried': 0,\n            'queue_time_ms': [],\n            'processing_time_ms': []\n        })\n        \n        self.logger = logging.getLogger(__name__)\n        \n    async def initialize(self):\n        \"\"\"Initialize Redis connection and start workers\"\"\"\n        self.redis = await aioredis.from_url(self.redis_url)\n        self.running = True\n        \n        # Start worker coroutines\n        for i in range(self.num_workers):\n            worker = asyncio.create_task(self._worker(i))\n            self.workers.append(worker)\n            \n        # Start batch processor\n        self.workers.append(asyncio.create_task(self._batch_processor()))\n        \n        # Start metrics reporter\n        self.workers.append(asyncio.create_task(self._metrics_reporter()))\n        \n        self.logger.info(f\"Queue system initialized with {self.num_workers} workers\")\n        \n    async def submit_request(self, \n                           request: AIRequest,\n                           wait_for_result: bool = False) -> Dict[str, Any]:\n        \"\"\"Submit request to appropriate queue\"\"\"\n        \n        # Validate request\n        if request.deadline_ms and request.deadline_ms < 100:\n            return {'error': 'Deadline too short', 'min_deadline_ms': 100}\n            \n        # Store request data\n        request_key = f\"ai:request:{request.request_id}\"\n        await self.redis.setex(\n            request_key,\n            3600,  # 1 hour TTL\n            request.to_json()\n        )\n        \n        # Add to appropriate queue\n        queue_name = self.queue_names[request.priority]\n        \n        if request.priority == Priority.CRITICAL:\n            # Use LPUSH for LIFO processing of critical requests\n            await self.redis.lpush(queue_name, request.request_id)\n        else:\n            # Use RPUSH for FIFO processing\n            await self.redis.rpush(queue_name, request.request_id)\n            \n        # Update queue metrics\n        queue_length = await self.redis.llen(queue_name)\n        await self.redis.hset('ai:metrics:queues', queue_name, queue_length)\n        \n        if wait_for_result:\n            # Wait for result (with timeout)\n            result = await self._wait_for_result(request.request_id, request.deadline_ms)\n            return result\n        else:\n            return {\n                'status': 'queued',\n                'request_id': request.request_id,\n                'queue': queue_name,\n                'position': queue_length\n            }\n            \n    async def _worker(self, worker_id: int):\n        \"\"\"Worker coroutine for processing requests\"\"\"\n        self.logger.info(f\"Worker {worker_id} started\")\n        \n        while self.running:\n            try:\n                # Check queues in priority order\n                request_id = None\n                queue_name = None\n                \n                for priority in Priority:\n                    queue_name = self.queue_names[priority]\n                    \n                    # Use BLPOP with timeout for blocking pop\n                    result = await self.redis.blpop(queue_name, timeout=1)\n                    \n                    if result:\n                        _, request_id = result\n                        request_id = request_id.decode()\n                        break\n                        \n                if not request_id:\n                    continue\n                    \n                # Process request\n                await self._process_request(request_id, worker_id)\n                \n            except Exception as e:\n                self.logger.error(f\"Worker {worker_id} error: {e}\")\n                await asyncio.sleep(1)\n                \n    async def _process_request(self, request_id: str, worker_id: int):\n        \"\"\"Process a single request\"\"\"\n        start_time = time.time()\n        \n        # Retrieve request data\n        request_key = f\"ai:request:{request_id}\"\n        request_data = await self.redis.get(request_key)\n        \n        if not request_data:\n            self.logger.warning(f\"Request {request_id} not found\")\n            return\n            \n        request = AIRequest.from_json(request_data)\n        \n        # Check deadline\n        if request.deadline_ms:\n            elapsed_ms = (time.time() - request.created_at) * 1000\n            if elapsed_ms > request.deadline_ms:\n                await self._mark_failed(request, \"Deadline exceeded\")\n                return\n                \n        try:\n            # Simulate AI processing\n            result = await self._run_inference(request)\n            \n            # Store result\n            result_key = f\"ai:result:{request_id}\"\n            await self.redis.setex(\n                result_key,\n                3600,  # 1 hour TTL\n                json.dumps(result)\n            )\n            \n            # Update metrics\n            processing_time = (time.time() - start_time) * 1000\n            queue_time = (start_time - request.created_at) * 1000\n            \n            self.metrics[request.priority.name]['processed'] += 1\n            self.metrics[request.priority.name]['queue_time_ms'].append(queue_time)\n            self.metrics[request.priority.name]['processing_time_ms'].append(processing_time)\n            \n            # Send callback if configured\n            if request.callback_url:\n                await self._send_callback(request.callback_url, result)\n                \n        except Exception as e:\n            self.logger.error(f\"Processing error for {request_id}: {e}\")\n            \n            # Retry logic\n            if request.retry_count < request.max_retries:\n                request.retry_count += 1\n                await self.submit_request(request)\n                self.metrics[request.priority.name]['retried'] += 1\n            else:\n                await self._mark_failed(request, str(e))\n                \n    async def _batch_processor(self):\n        \"\"\"Process batch-compatible requests in batches\"\"\"\n        while self.running:\n            try:\n                # Collect batch requests\n                batch_queue = self.queue_names[Priority.BATCH]\n                batch_requests = []\n                \n                # Wait for first request\n                result = await self.redis.blpop(batch_queue, timeout=1)\n                if not result:\n                    continue\n                    \n                _, request_id = result\n                batch_requests.append(request_id.decode())\n                \n                # Collect more requests up to batch size\n                deadline = time.time() + (self.batch_timeout_ms / 1000)\n                \n                while len(batch_requests) < self.batch_size and time.time() < deadline:\n                    result = await self.redis.lpop(batch_queue)\n                    if result:\n                        batch_requests.append(result.decode())\n                    else:\n                        await asyncio.sleep(0.01)\n                        \n                # Process batch\n                if batch_requests:\n                    await self._process_batch(batch_requests)\n                    \n            except Exception as e:\n                self.logger.error(f\"Batch processor error: {e}\")\n                await asyncio.sleep(1)\n                \n    async def _process_batch(self, request_ids: List[str]):\n        \"\"\"Process a batch of requests together\"\"\"\n        self.logger.info(f\"Processing batch of {len(request_ids)} requests\")\n        \n        # Retrieve all requests\n        requests = []\n        for request_id in request_ids:\n            request_data = await self.redis.get(f\"ai:request:{request_id}\")\n            if request_data:\n                requests.append(AIRequest.from_json(request_data))\n                \n        if not requests:\n            return\n            \n        try:\n            # Batch inference\n            results = await self._run_batch_inference(requests)\n            \n            # Store results\n            for request, result in zip(requests, results):\n                result_key = f\"ai:result:{request.request_id}\"\n                await self.redis.setex(result_key, 3600, json.dumps(result))\n                \n                # Update metrics\n                self.metrics[Priority.BATCH.name]['processed'] += 1\n                \n        except Exception as e:\n            self.logger.error(f\"Batch processing error: {e}\")\n            # Requeue failed requests\n            for request in requests:\n                if request.retry_count < request.max_retries:\n                    request.retry_count += 1\n                    await self.submit_request(request)\n                    \n    async def _run_inference(self, request: AIRequest) -> Dict[str, Any]:\n        \"\"\"Simulate AI inference\"\"\"\n        # Simulate processing time based on model\n        processing_time = np.random.normal(100, 20)  # 100ms average\n        await asyncio.sleep(processing_time / 1000)\n        \n        return {\n            'request_id': request.request_id,\n            'model': request.model_name,\n            'result': f\"Processed {request.payload}\",\n            'confidence': np.random.uniform(0.8, 0.99),\n            'processing_time_ms': processing_time\n        }\n        \n    async def _run_batch_inference(self, requests: List[AIRequest]) -> List[Dict[str, Any]]:\n        \"\"\"Simulate batch AI inference\"\"\"\n        # Batch processing is more efficient\n        processing_time = np.random.normal(50, 10) * len(requests)  # Less per request\n        await asyncio.sleep(processing_time / 1000)\n        \n        return [\n            {\n                'request_id': req.request_id,\n                'model': req.model_name,\n                'result': f\"Batch processed {req.payload}\",\n                'confidence': np.random.uniform(0.8, 0.99),\n                'batch_size': len(requests)\n            }\n            for req in requests\n        ]\n        \n    async def get_queue_status(self) -> Dict[str, Any]:\n        \"\"\"Get current queue status and metrics\"\"\"\n        status = {\n            'queues': {},\n            'workers': len(self.workers),\n            'metrics': {}\n        }\n        \n        # Queue lengths\n        for priority, queue_name in self.queue_names.items():\n            length = await self.redis.llen(queue_name)\n            status['queues'][priority.name] = length\n            \n        # Processing metrics\n        for priority_name, metrics in self.metrics.items():\n            if metrics['processing_time_ms']:\n                status['metrics'][priority_name] = {\n                    'processed': metrics['processed'],\n                    'failed': metrics['failed'],\n                    'retried': metrics['retried'],\n                    'avg_queue_time_ms': np.mean(metrics['queue_time_ms']),\n                    'avg_processing_time_ms': np.mean(metrics['processing_time_ms']),\n                    'p95_processing_time_ms': np.percentile(metrics['processing_time_ms'], 95)\n                }\n                \n        return status",
            "highlightLines": [13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 47, 48, 49, 50, 51, 52, 53, 54, 55, 68, 69, 70, 71, 72, 73, 74, 122, 123, 124, 125, 126, 127, 128, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 187, 188, 189, 190, 191, 192, 193, 220, 221, 222, 223, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "auto-scaling",
      "title": "Auto-scaling Policies",
      "type": "content",
      "order": 6,
      "estimatedDuration": "15 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#auto-scaling"
      }
    },
    {
      "id": "cost-optimization",
      "title": "Cost Optimization at Scale",
      "type": "content",
      "order": 7,
      "estimatedDuration": "15 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#cost-optimization"
      }
    },
    {
      "id": "multi-region",
      "title": "Multi-region Deployment",
      "type": "content",
      "order": 8,
      "estimatedDuration": "10 minutes",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#multi-region"
      }
    },
    {
      "id": "hands-on-exercise",
      "title": "Build a Scalable AI Service",
      "type": "exercise",
      "order": 9,
      "estimatedDuration": "45 minutes",
      "content": {
        "type": "exercise",
        "title": "Implement Scalable AI Service",
        "description": "Build a complete scalable AI service with auto-scaling, caching, and queue management",
        "exerciseType": "coding",
        "difficulty": "hard",
        "instructions": [
          "Design a scalable architecture for an AI text generation service",
          "Implement horizontal scaling with load balancing",
          "Add intelligent caching for common requests",
          "Create a queue system for handling traffic bursts",
          "Configure auto-scaling based on queue depth and latency",
          "Add cost tracking and optimization features",
          "Test the system under various load conditions"
        ],
        "startingCode": "from typing import Dict, Any, List, Optional\nimport asyncio\nimport time\nimport hashlib\n\nclass ScalableAIService:\n    def __init__(self, \n                 initial_instances: int = 2,\n                 max_instances: int = 10,\n                 cache_size_mb: int = 512):\n        self.instances = initial_instances\n        self.max_instances = max_instances\n        self.cache_size_mb = cache_size_mb\n        \n        # TODO: Initialize components\n        self.load_balancer = None\n        self.cache = None\n        self.queue = None\n        self.auto_scaler = None\n        \n    async def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process an AI request with caching and queuing\"\"\"\n        # TODO: Check cache first\n        # TODO: Route through load balancer\n        # TODO: Handle queue overflow\n        # TODO: Track metrics for auto-scaling\n        pass\n        \n    def scale_decision(self) -> str:\n        \"\"\"Make scaling decision based on metrics\"\"\"\n        # TODO: Analyze queue depth\n        # TODO: Check average latency\n        # TODO: Consider cost constraints\n        # TODO: Return 'scale_up', 'scale_down', or 'maintain'\n        pass\n        \n    async def apply_scaling(self, action: str):\n        \"\"\"Apply scaling decision\"\"\"\n        # TODO: Add or remove instances\n        # TODO: Update load balancer\n        # TODO: Ensure graceful scaling\n        pass\n\n# Implement and test your scalable AI service\nif __name__ == \"__main__\":\n    service = ScalableAIService()\n    \n    # TODO: Simulate varying load patterns\n    # TODO: Measure performance metrics\n    # TODO: Verify auto-scaling behavior",
        "solution": "from typing import Dict, Any, List, Optional, Tuple\nimport asyncio\nimport time\nimport hashlib\nimport json\nimport numpy as np\nfrom collections import defaultdict, deque, OrderedDict\nfrom dataclasses import dataclass, field\nimport heapq\n\n@dataclass\nclass Instance:\n    instance_id: str\n    capacity: int = 100\n    current_load: int = 0\n    latency_history: deque = field(default_factory=lambda: deque(maxlen=100))\n    \n    @property\n    def utilization(self) -> float:\n        return self.current_load / self.capacity\n        \n    @property\n    def avg_latency(self) -> float:\n        return np.mean(self.latency_history) if self.latency_history else 0\n\n@dataclass\nclass QueuedRequest:\n    request_id: str\n    data: Dict[str, Any]\n    priority: int\n    timestamp: float = field(default_factory=time.time)\n    \n    def __lt__(self, other):\n        return self.priority > other.priority\n\nclass ScalableAIService:\n    def __init__(self, \n                 initial_instances: int = 2,\n                 max_instances: int = 10,\n                 cache_size_mb: int = 512):\n        self.min_instances = initial_instances\n        self.max_instances = max_instances\n        self.cache_size_mb = cache_size_mb\n        \n        # Initialize components\n        self.instances = {\n            f\"instance-{i}\": Instance(f\"instance-{i}\") \n            for i in range(initial_instances)\n        }\n        \n        # Cache implementation\n        self.cache = OrderedDict()  # LRU cache\n        self.cache_hits = 0\n        self.cache_misses = 0\n        \n        # Queue implementation\n        self.request_queue = []  # Priority queue\n        self.queue_processor_running = True\n        \n        # Metrics\n        self.metrics = {\n            'requests_processed': 0,\n            'queue_depth_history': deque(maxlen=100),\n            'latency_history': deque(maxlen=1000),\n            'scaling_events': []\n        }\n        \n        # Auto-scaling parameters\n        self.scale_up_threshold = 0.8  # 80% utilization\n        self.scale_down_threshold = 0.3  # 30% utilization\n        self.scale_cooldown = 60  # seconds\n        self.last_scale_time = 0\n        \n        # Cost tracking\n        self.instance_cost_per_hour = 1.0  # $1/hour per instance\n        self.total_cost = 0\n        \n        # Start background tasks\n        asyncio.create_task(self._queue_processor())\n        asyncio.create_task(self._auto_scaling_loop())\n        asyncio.create_task(self._metrics_collector())\n        \n    def _generate_cache_key(self, request: Dict[str, Any]) -> str:\n        \"\"\"Generate cache key from request\"\"\"\n        return hashlib.md5(json.dumps(request, sort_keys=True).encode()).hexdigest()\n        \n    async def process_request(self, request: Dict[str, Any], priority: int = 5) -> Dict[str, Any]:\n        \"\"\"Process an AI request with caching and queuing\"\"\"\n        request_id = str(time.time())\n        \n        # Check cache first\n        cache_key = self._generate_cache_key(request)\n        if cache_key in self.cache:\n            self.cache_hits += 1\n            # Move to end (LRU)\n            self.cache.move_to_end(cache_key)\n            cached_result = self.cache[cache_key]\n            return {\n                **cached_result,\n                'cached': True,\n                'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses)\n            }\n            \n        self.cache_misses += 1\n        \n        # Add to queue\n        queued_request = QueuedRequest(\n            request_id=request_id,\n            data=request,\n            priority=priority\n        )\n        heapq.heappush(self.request_queue, queued_request)\n        \n        # Try to process immediately if capacity available\n        instance = self._select_instance()\n        if instance and instance.utilization < 1.0:\n            return await self._process_on_instance(instance, request, cache_key)\n            \n        # Otherwise, wait for queue processing\n        return {\n            'status': 'queued',\n            'request_id': request_id,\n            'queue_position': len(self.request_queue),\n            'estimated_wait_ms': self._estimate_wait_time()\n        }\n        \n    def _select_instance(self) -> Optional[Instance]:\n        \"\"\"Select instance with lowest utilization\"\"\"\n        available_instances = [\n            inst for inst in self.instances.values() \n            if inst.utilization < 1.0\n        ]\n        \n        if not available_instances:\n            return None\n            \n        return min(available_instances, key=lambda x: x.utilization)\n        \n    async def _process_on_instance(self, \n                                  instance: Instance, \n                                  request: Dict[str, Any],\n                                  cache_key: str) -> Dict[str, Any]:\n        \"\"\"Process request on specific instance\"\"\"\n        instance.current_load += 1\n        start_time = time.time()\n        \n        try:\n            # Simulate AI processing\n            processing_time = np.random.normal(100, 20)  # 100ms average\n            await asyncio.sleep(processing_time / 1000)\n            \n            result = {\n                'result': f\"Processed: {request}\",\n                'confidence': np.random.uniform(0.8, 0.99),\n                'instance_id': instance.instance_id,\n                'processing_time_ms': processing_time\n            }\n            \n            # Update metrics\n            latency = (time.time() - start_time) * 1000\n            instance.latency_history.append(latency)\n            self.metrics['latency_history'].append(latency)\n            self.metrics['requests_processed'] += 1\n            \n            # Cache result\n            self._add_to_cache(cache_key, result)\n            \n            return result\n            \n        finally:\n            instance.current_load -= 1\n            \n    def _add_to_cache(self, key: str, value: Dict[str, Any]):\n        \"\"\"Add to LRU cache with size limit\"\"\"\n        # Estimate size (simplified)\n        entry_size = len(json.dumps(value).encode())\n        max_cache_size = self.cache_size_mb * 1024 * 1024\n        \n        # Evict if necessary\n        current_size = sum(len(json.dumps(v).encode()) for v in self.cache.values())\n        while current_size + entry_size > max_cache_size and self.cache:\n            evicted_key, _ = self.cache.popitem(last=False)\n            current_size = sum(len(json.dumps(v).encode()) for v in self.cache.values())\n            \n        self.cache[key] = value\n        \n    async def _queue_processor(self):\n        \"\"\"Process queued requests\"\"\"\n        while self.queue_processor_running:\n            if not self.request_queue:\n                await asyncio.sleep(0.01)\n                continue\n                \n            # Get highest priority request\n            instance = self._select_instance()\n            if instance:\n                request = heapq.heappop(self.request_queue)\n                cache_key = self._generate_cache_key(request.data)\n                \n                # Process asynchronously\n                asyncio.create_task(\n                    self._process_on_instance(instance, request.data, cache_key)\n                )\n                \n            await asyncio.sleep(0.001)\n            \n    async def _auto_scaling_loop(self):\n        \"\"\"Auto-scaling control loop\"\"\"\n        while True:\n            await asyncio.sleep(10)  # Check every 10 seconds\n            \n            action = self.scale_decision()\n            if action != 'maintain':\n                await self.apply_scaling(action)\n                \n    def scale_decision(self) -> str:\n        \"\"\"Make scaling decision based on metrics\"\"\"\n        # Check cooldown\n        if time.time() - self.last_scale_time < self.scale_cooldown:\n            return 'maintain'\n            \n        # Calculate average utilization\n        utilizations = [inst.utilization for inst in self.instances.values()]\n        avg_utilization = np.mean(utilizations)\n        \n        # Queue depth consideration\n        queue_depth = len(self.request_queue)\n        self.metrics['queue_depth_history'].append(queue_depth)\n        avg_queue_depth = np.mean(self.metrics['queue_depth_history'])\n        \n        # Latency consideration\n        if self.metrics['latency_history']:\n            p95_latency = np.percentile(self.metrics['latency_history'], 95)\n        else:\n            p95_latency = 0\n            \n        # Scaling logic\n        current_instances = len(self.instances)\n        \n        # Scale up conditions\n        if (avg_utilization > self.scale_up_threshold or \n            avg_queue_depth > 50 or \n            p95_latency > 200):  # 200ms threshold\n            if current_instances < self.max_instances:\n                return 'scale_up'\n                \n        # Scale down conditions\n        elif (avg_utilization < self.scale_down_threshold and \n              avg_queue_depth < 10 and \n              p95_latency < 100):  # Good performance\n            if current_instances > self.min_instances:\n                # Check cost optimization\n                if self._can_reduce_cost():\n                    return 'scale_down'\n                    \n        return 'maintain'\n        \n    def _can_reduce_cost(self) -> bool:\n        \"\"\"Check if scaling down would optimize cost\"\"\"\n        current_cost = len(self.instances) * self.instance_cost_per_hour\n        projected_cost = (len(self.instances) - 1) * self.instance_cost_per_hour\n        \n        # Ensure we can still handle load with fewer instances\n        total_capacity = sum(inst.capacity for inst in self.instances.values())\n        current_load = sum(inst.current_load for inst in self.instances.values())\n        \n        if current_load / (total_capacity - 100) < 0.7:  # 70% utilization after scale down\n            return True\n        return False\n        \n    async def apply_scaling(self, action: str):\n        \"\"\"Apply scaling decision\"\"\"\n        current_instances = len(self.instances)\n        \n        if action == 'scale_up':\n            new_instance_id = f\"instance-{current_instances}\"\n            self.instances[new_instance_id] = Instance(new_instance_id)\n            \n            self.metrics['scaling_events'].append({\n                'timestamp': time.time(),\n                'action': 'scale_up',\n                'instances': current_instances + 1,\n                'reason': 'High utilization or latency'\n            })\n            \n            print(f\"Scaled up to {len(self.instances)} instances\")\n            \n        elif action == 'scale_down':\n            # Remove instance with lowest utilization\n            instance_to_remove = min(\n                self.instances.values(), \n                key=lambda x: x.utilization\n            )\n            \n            # Wait for instance to drain\n            while instance_to_remove.current_load > 0:\n                await asyncio.sleep(0.1)\n                \n            del self.instances[instance_to_remove.instance_id]\n            \n            self.metrics['scaling_events'].append({\n                'timestamp': time.time(),\n                'action': 'scale_down',\n                'instances': len(self.instances),\n                'reason': 'Low utilization'\n            })\n            \n            print(f\"Scaled down to {len(self.instances)} instances\")\n            \n        self.last_scale_time = time.time()\n        \n    async def _metrics_collector(self):\n        \"\"\"Collect and track metrics\"\"\"\n        while True:\n            await asyncio.sleep(60)  # Every minute\n            \n            # Calculate cost\n            self.total_cost += len(self.instances) * self.instance_cost_per_hour / 60\n            \n            # Log metrics\n            if self.metrics['requests_processed'] > 0:\n                print(f\"\\nMetrics Summary:\")\n                print(f\"Instances: {len(self.instances)}\")\n                print(f\"Requests processed: {self.metrics['requests_processed']}\")\n                print(f\"Cache hit rate: {self.cache_hits / (self.cache_hits + self.cache_misses):.2%}\")\n                print(f\"Avg queue depth: {np.mean(self.metrics['queue_depth_history']):.1f}\")\n                print(f\"P95 latency: {np.percentile(self.metrics['latency_history'], 95):.1f}ms\")\n                print(f\"Total cost: ${self.total_cost:.2f}\")\n                print(f\"Cost per request: ${self.total_cost / self.metrics['requests_processed']:.4f}\")\n                \n    def _estimate_wait_time(self) -> float:\n        \"\"\"Estimate queue wait time\"\"\"\n        if not self.metrics['latency_history']:\n            return 0\n            \n        avg_processing_time = np.mean(self.metrics['latency_history'])\n        total_capacity = sum(inst.capacity for inst in self.instances.values())\n        \n        return (len(self.request_queue) / total_capacity) * avg_processing_time\n\n# Test implementation\nasync def simulate_load():\n    service = ScalableAIService(initial_instances=2, max_instances=8)\n    \n    # Simulate varying load patterns\n    load_patterns = [\n        (100, 10, 5),   # requests, duration_seconds, priority\n        (500, 30, 3),   # High load\n        (50, 20, 7),    # Low load\n        (1000, 60, 1),  # Burst\n        (200, 40, 5),   # Normal\n    ]\n    \n    for requests, duration, priority in load_patterns:\n        print(f\"\\nSimulating {requests} requests over {duration}s...\")\n        \n        tasks = []\n        for i in range(requests):\n            delay = np.random.uniform(0, duration)\n            task = asyncio.create_task(\n                process_after_delay(service, delay, {\n                    'input': f'request_{i}',\n                    'data': np.random.rand(10).tolist()\n                }, priority)\n            )\n            tasks.append(task)\n            \n        await asyncio.gather(*tasks)\n        await asyncio.sleep(5)  # Cool down\n        \n    # Final metrics\n    print(\"\\n=== Final Performance Report ===\")\n    print(f\"Total requests: {service.metrics['requests_processed']}\")\n    print(f\"Scaling events: {len(service.metrics['scaling_events'])}\")\n    print(f\"Final cost: ${service.total_cost:.2f}\")\n    \nasync def process_after_delay(service, delay, request, priority):\n    await asyncio.sleep(delay)\n    return await service.process_request(request, priority)\n\nif __name__ == \"__main__\":\n    asyncio.run(simulate_load())",
        "hints": [
          "Use a priority queue for request handling",
          "Implement LRU caching with size limits",
          "Track utilization metrics for each instance",
          "Consider queue depth and latency in scaling decisions",
          "Implement graceful instance draining when scaling down",
          "Use asyncio for concurrent request processing"
        ],
        "validation": [
          {
            "type": "contains",
            "value": "class ScalableAIService",
            "message": "Define the scalable AI service class"
          },
          {
            "type": "contains",
            "value": "process_request",
            "message": "Implement request processing method"
          },
          {
            "type": "contains",
            "value": "scale_decision",
            "message": "Implement scaling decision logic"
          },
          {
            "type": "contains",
            "value": "_select_instance",
            "message": "Implement load balancing logic"
          },
          {
            "type": "contains",
            "value": "cache",
            "message": "Implement caching mechanism"
          }
        ]
      }
    },
    {
      "id": "quiz",
      "title": "Scaling Strategies Quiz",
      "type": "quiz",
      "order": 10,
      "estimatedDuration": "15 minutes",
      "content": {
        "type": "quiz",
        "title": "AI Scaling Knowledge Check",
        "description": "Test your understanding of scaling strategies and load management for AI systems",
        "questions": [
          {
            "id": "q1",
            "type": "multiple-choice",
            "question": "What are the unique challenges of scaling AI systems compared to traditional web applications?",
            "points": 3,
            "options": [
              {
                "id": "a",
                "text": "High memory requirements for model loading",
                "explanation": "Correct! AI models can require several GB of memory."
              },
              {
                "id": "b",
                "text": "GPU resource constraints and allocation",
                "explanation": "Correct! GPUs are expensive and have exclusive allocation requirements."
              },
              {
                "id": "c",
                "text": "Database connection pooling",
                "explanation": "Incorrect. This is a common challenge for all web applications."
              },
              {
                "id": "d",
                "text": "Slow cold start times due to model loading",
                "explanation": "Correct! Loading large models can take 30+ seconds."
              }
            ],
            "correctAnswers": ["a", "b", "d"],
            "randomizeOptions": true
          },
          {
            "id": "q2",
            "type": "true-false",
            "question": "Horizontal scaling is always more cost-effective than vertical scaling for AI workloads.",
            "points": 2,
            "correctAnswer": false,
            "explanation": "False! Due to GPU constraints and model loading overhead, vertical scaling (using more powerful GPUs) can often be more cost-effective for AI workloads."
          },
          {
            "id": "q3",
            "type": "multiple-choice",
            "question": "Which caching strategy is most effective for AI inference results?",
            "points": 2,
            "options": [
              {
                "id": "a",
                "text": "Time-based expiration only",
                "explanation": "Incomplete. This misses opportunities for semantic matching."
              },
              {
                "id": "b",
                "text": "Exact key matching with semantic similarity fallback",
                "explanation": "Correct! This provides the best hit rate for AI responses."
              },
              {
                "id": "c",
                "text": "Random eviction policy",
                "explanation": "Incorrect. This is inefficient for AI workloads."
              },
              {
                "id": "d",
                "text": "No caching due to unique requests",
                "explanation": "Incorrect. Many AI requests have semantic similarity."
              }
            ],
            "correctAnswers": ["b"],
            "randomizeOptions": true
          },
          {
            "id": "q4",
            "type": "multiple-choice",
            "question": "What metrics should trigger auto-scaling for AI services?",
            "points": 3,
            "options": [
              {
                "id": "a",
                "text": "GPU utilization percentage",
                "explanation": "Correct! GPU utilization is a key indicator of capacity."
              },
              {
                "id": "b",
                "text": "Queue depth and wait time",
                "explanation": "Correct! Queue metrics indicate demand exceeding capacity."
              },
              {
                "id": "c",
                "text": "Inference latency percentiles (p95, p99)",
                "explanation": "Correct! Latency degradation indicates overload."
              },
              {
                "id": "d",
                "text": "CPU utilization only",
                "explanation": "Incorrect. CPU is rarely the bottleneck for AI inference."
              }
            ],
            "correctAnswers": ["a", "b", "c"],
            "randomizeOptions": true
          },
          {
            "id": "q5",
            "type": "true-false",
            "question": "Queue-based architectures help handle traffic bursts by decoupling request acceptance from processing.",
            "points": 1,
            "correctAnswer": true,
            "explanation": "True! Queues allow the system to accept requests even when processing capacity is temporarily exceeded."
          }
        ],
        "passingScore": 75,
        "allowRetries": true,
        "showCorrectAnswers": true,
        "randomizeQuestions": false
      }
    }
  ],
  "resources": [
    {
      "id": "kubernetes-gpu-guide",
      "title": "Kubernetes GPU Scheduling Guide",
      "type": "documentation",
      "url": "https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/",
      "description": "Official guide for GPU resource management in Kubernetes",
      "required": true
    },
    {
      "id": "aws-sagemaker-scaling",
      "title": "AWS SageMaker Auto Scaling",
      "type": "guide",
      "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html",
      "description": "Auto-scaling ML models with AWS SageMaker",
      "required": false
    },
    {
      "id": "ray-serve-scaling",
      "title": "Ray Serve Scaling Guide",
      "type": "tutorial",
      "url": "https://docs.ray.io/en/latest/serve/scaling-guide.html",
      "description": "Scaling AI applications with Ray Serve",
      "required": true
    },
    {
      "id": "nvidia-mig-guide",
      "title": "NVIDIA Multi-Instance GPU Guide",
      "type": "reference",
      "url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/",
      "description": "Partitioning GPUs for better utilization",
      "required": false
    },
    {
      "id": "redis-ai-caching",
      "title": "Redis for AI Caching Patterns",
      "type": "tutorial",
      "url": "https://redis.io/docs/stack/ml/",
      "description": "Using Redis for AI model caching and inference",
      "required": true
    }
  ],
  "assessmentCriteria": {
    "minimumScore": 75,
    "requiredSections": ["scaling-challenges", "scaling-strategies", "load-balancing", "caching-strategies", "queue-architecture", "hands-on-exercise"],
    "timeTracking": true,
    "completionCertificate": false
  }
}