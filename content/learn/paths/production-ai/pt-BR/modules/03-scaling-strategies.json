{
  "metadata": {
    "id": "scaling-strategies",
    "pathId": "production-ai",
    "title": "Estratégias de Escalonamento e Gerenciamento de Carga",
    "description": "Domine a arte de escalonar sistemas de IA em produção, incluindo alocação de GPU, otimização de carregamento de modelo, estratégias de cache, arquiteturas baseadas em filas e políticas de auto-escalonamento específicas para cargas de trabalho de IA.",
    "duration": "120 minutos",
    "type": "concept",
    "difficulty": "avançado",
    "order": 3,
    "prerequisites": [
      "Compreensão de implantação de modelos de IA",
      "Conhecimento básico de sistemas distribuídos",
      "Familiaridade com containerização e orquestração",
      "Experiência com infraestrutura de produção"
    ],
    "objectives": [
      "Entender os desafios únicos de escalonar sistemas de IA vs aplicações tradicionais",
      "Projetar estratégias de escalonamento horizontal e vertical para cargas de trabalho de IA",
      "Implementar balanceamento de carga inteligente para endpoints de IA",
      "Construir estratégias de cache otimizadas para inferência de IA",
      "Arquitetar sistemas baseados em filas para lidar com picos de tráfego",
      "Configurar políticas de auto-escalonamento baseadas em métricas específicas de IA",
      "Otimizar custos mantendo desempenho em escala",
      "Implantar serviços de IA multi-região com considerações de residência de dados"
    ],
    "tags": [
      "escalonamento",
      "gerenciamento-carga",
      "otimização-gpu",
      "cache",
      "arquitetura-filas",
      "auto-escalonamento",
      "otimização-custos",
      "multi-região"
    ],
    "version": "1.0.0",
    "lastUpdated": "2025-06-28",
    "author": "Equipe de Engenharia de IA",
    "estimatedCompletionTime": 360
  },
  "sections": [
    {
      "id": "scaling-challenges",
      "title": "Desafios do Escalonamento de IA",
      "type": "content",
      "order": 1,
      "estimatedDuration": "20 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#scaling-challenges",
        "codeExamples": [
          {
            "id": "resource-comparison",
            "title": "Requisitos de Recursos: IA vs App Tradicional",
            "description": "Comparando necessidades de recursos e características de escalonamento",
            "language": "python",
            "code": "# Aplicação Web Tradicional\nclass TraditionalApp:\n    def __init__(self):\n        self.memory_mb = 512  # Memória típica de app web\n        self.cpu_cores = 0.5  # Alocação fracional de CPU\n        self.startup_time_s = 2  # Inicialização rápida\n        self.stateless = True  # Fácil de escalonar horizontalmente\n        \n    def handle_request(self, request):\n        # Operação simples limitada por CPU\n        return process_request(request)\n\n# Aplicação de IA\nclass AIApplication:\n    def __init__(self):\n        self.memory_gb = 16  # Grande pegada de memória do modelo\n        self.gpu_required = True  # Dependência de GPU\n        self.model_load_time_s = 30  # Carregamento lento do modelo\n        self.batch_processing = True  # Batching eficiente necessário\n        \n    def load_model(self):\n        # Carregar modelo multi-GB na memória GPU\n        self.model = load_large_model('model.bin')  # Arquivo de 5-10GB\n        self.model.to('cuda')  # Alocação de memória GPU\n        \n    def inference(self, batch):\n        # Inferência acelerada por GPU\n        with torch.no_grad():\n            return self.model(batch)\n\n# Implicações de Escalonamento\nclass ScalingComparison:\n    def __init__(self):\n        self.traditional_scaling = {\n            'horizontal_ease': 'Alto',  # Fácil adicionar instâncias\n            'vertical_limit': 'Baixo',  # Não precisa de muitos recursos\n            'cold_start': 'Rápido',  # Rápido para criar\n            'resource_sharing': 'Eficiente',  # Múltiplas apps por host\n            'cost_model': 'Linear'  # Custos previsíveis\n        }\n        \n        self.ai_scaling = {\n            'horizontal_ease': 'Médio',  # Sobrecarga de carregamento de modelo\n            'vertical_limit': 'Alto',  # Restrições de memória GPU\n            'cold_start': 'Lento',  # Tempo de carregamento de modelo\n            'resource_sharing': 'Limitado',  # Exclusividade de GPU\n            'cost_model': 'Função-degrau'  # Níveis de instância GPU\n        }",
            "highlightLines": [14, 15, 16, 17, 18, 22, 23, 24, 33, 34, 35, 36, 37, 41, 42, 43, 44, 45],
            "runnable": false
          },
          {
            "id": "gpu-allocation-challenge",
            "title": "Desafios de Memória e Alocação de GPU",
            "description": "Entendendo restrições de recursos GPU no escalonamento",
            "language": "python",
            "code": "import torch\nimport psutil\nimport GPUtil\nfrom typing import Dict, List, Optional\nimport threading\nimport queue\n\nclass GPUResourceManager:\n    def __init__(self):\n        self.gpus = GPUtil.getGPUs()\n        self.allocation_lock = threading.Lock()\n        self.gpu_allocations = {gpu.id: [] for gpu in self.gpus}\n        \n    def get_gpu_status(self) -> List[Dict]:\n        \"\"\"Obter status atual de utilização da GPU\"\"\"\n        status = []\n        for gpu in GPUtil.getGPUs():\n            status.append({\n                'id': gpu.id,\n                'name': gpu.name,\n                'memory_used_mb': gpu.memoryUsed,\n                'memory_total_mb': gpu.memoryTotal,\n                'memory_free_mb': gpu.memoryFree,\n                'utilization_percent': gpu.load * 100,\n                'temperature_c': gpu.temperature\n            })\n        return status\n        \n    def allocate_model_to_gpu(self, model_size_mb: int) -> Optional[int]:\n        \"\"\"Encontrar e alocar GPU para implantação do modelo\"\"\"\n        with self.allocation_lock:\n            for gpu in GPUtil.getGPUs():\n                if gpu.memoryFree > model_size_mb * 1.2:  # 20% de margem\n                    self.gpu_allocations[gpu.id].append(model_size_mb)\n                    return gpu.id\n            return None  # Nenhuma GPU disponível\n            \n    def estimate_model_capacity(self) -> Dict:\n        \"\"\"Estimar quantos modelos cabem nas GPUs disponíveis\"\"\"\n        capacity = {}\n        \n        # Tamanhos comuns de modelo (MB)\n        model_sizes = {\n            'small_bert': 420,      # DistilBERT\n            'base_bert': 440,       # BERT-base\n            'large_bert': 1340,     # BERT-large\n            'gpt2': 548,           # GPT-2 base\n            'gpt2_large': 3000,    # GPT-2 large\n            't5_small': 242,       # T5-small\n            't5_base': 892,        # T5-base\n            't5_large': 3000,      # T5-large\n            'llama_7b': 13000,     # LLaMA 7B\n            'llama_13b': 26000     # LLaMA 13B\n        }\n        \n        for gpu in GPUtil.getGPUs():\n            gpu_capacity = {}\n            available_memory = gpu.memoryFree * 0.9  # Manter 10% de buffer\n            \n            for model_name, size_mb in model_sizes.items():\n                # Considerar sobrecarga de inferência (ativações, etc.)\n                total_required = size_mb * 1.5\n                gpu_capacity[model_name] = int(available_memory // total_required)\n                \n            capacity[f'GPU_{gpu.id}_{gpu.name}'] = {\n                'memory_available_mb': available_memory,\n                'model_capacity': gpu_capacity\n            }\n            \n        return capacity\n\n# Exemplo de Desafio de Escalonamento de Modelo\nclass ModelScalingChallenge:\n    def __init__(self, model_name: str, model_size_mb: int):\n        self.model_name = model_name\n        self.model_size_mb = model_size_mb\n        self.instances = []\n        self.gpu_manager = GPUResourceManager()\n        \n    def scale_up(self, target_instances: int) -> Dict:\n        \"\"\"Tentar escalonar para número alvo de instâncias\"\"\"\n        results = {\n            'requested': target_instances,\n            'successful': 0,\n            'failed': 0,\n            'failure_reasons': []\n        }\n        \n        current_count = len(self.instances)\n        needed = target_instances - current_count\n        \n        for i in range(needed):\n            gpu_id = self.gpu_manager.allocate_model_to_gpu(self.model_size_mb)\n            if gpu_id is not None:\n                self.instances.append({\n                    'instance_id': f'{self.model_name}-{len(self.instances)}',\n                    'gpu_id': gpu_id,\n                    'status': 'loading'\n                })\n                results['successful'] += 1\n            else:\n                results['failed'] += 1\n                results['failure_reasons'].append(\n                    f'Memória GPU insuficiente para instância {i+1}'\n                )\n                \n        return results",
            "highlightLines": [29, 30, 31, 32, 33, 34, 35, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 60, 61, 62, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "scaling-strategies",
      "title": "Escalonamento Horizontal vs Vertical",
      "type": "content",
      "order": 2,
      "estimatedDuration": "25 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#scaling-strategies",
        "codeExamples": [
          {
            "id": "horizontal-scaling",
            "title": "Implementação de Escalonamento Horizontal",
            "description": "Implementando escalonamento horizontal para serviços de IA com replicação de modelo",
            "language": "python",
            "code": "import asyncio\nimport aiohttp\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom dataclasses import dataclass\nimport time\nimport hashlib\n\n@dataclass\nclass ModelInstance:\n    instance_id: str\n    endpoint: str\n    gpu_id: int\n    model_version: str\n    status: str\n    load_percentage: float\n    last_health_check: float\n\nclass HorizontalScalingOrchestrator:\n    def __init__(self, model_name: str, base_port: int = 8000):\n        self.model_name = model_name\n        self.base_port = base_port\n        self.instances: List[ModelInstance] = []\n        self.health_check_interval = 30  # segundos\n        self.scale_threshold_high = 0.8  # 80% de utilização\n        self.scale_threshold_low = 0.3   # 30% de utilização\n        \n    async def deploy_instance(self, gpu_id: int) -> ModelInstance:\n        \"\"\"Implantar nova instância do modelo na GPU especificada\"\"\"\n        instance_id = f\"{self.model_name}-{len(self.instances)}-{int(time.time())}\"\n        port = self.base_port + len(self.instances)\n        \n        # Manifesto de implantação Kubernetes\n        deployment_config = {\n            'apiVersion': 'apps/v1',\n            'kind': 'Deployment',\n            'metadata': {\n                'name': instance_id,\n                'labels': {'app': self.model_name, 'instance': instance_id}\n            },\n            'spec': {\n                'replicas': 1,\n                'selector': {'matchLabels': {'instance': instance_id}},\n                'template': {\n                    'metadata': {'labels': {'instance': instance_id}},\n                    'spec': {\n                        'containers': [{\n                            'name': 'model-server',\n                            'image': f'{self.model_name}:latest',\n                            'ports': [{'containerPort': 8080}],\n                            'env': [\n                                {'name': 'CUDA_VISIBLE_DEVICES', 'value': str(gpu_id)},\n                                {'name': 'MODEL_NAME', 'value': self.model_name},\n                                {'name': 'INSTANCE_ID', 'value': instance_id}\n                            ],\n                            'resources': {\n                                'limits': {\n                                    'nvidia.com/gpu': 1,\n                                    'memory': '16Gi',\n                                    'cpu': '4'\n                                },\n                                'requests': {\n                                    'nvidia.com/gpu': 1,\n                                    'memory': '12Gi',\n                                    'cpu': '2'\n                                }\n                            },\n                            'readinessProbe': {\n                                'httpGet': {'path': '/health', 'port': 8080},\n                                'initialDelaySeconds': 60,\n                                'periodSeconds': 10\n                            },\n                            'livenessProbe': {\n                                'httpGet': {'path': '/health', 'port': 8080},\n                                'initialDelaySeconds': 90,\n                                'periodSeconds': 30\n                            }\n                        }],\n                        'nodeSelector': {'gpu-type': 'nvidia-a100'}\n                    }\n                }\n            }\n        }\n        \n        # Implantar e aguardar prontidão\n        instance = ModelInstance(\n            instance_id=instance_id,\n            endpoint=f'http://{instance_id}:8080',\n            gpu_id=gpu_id,\n            model_version='v1.0',\n            status='deploying',\n            load_percentage=0.0,\n            last_health_check=time.time()\n        )\n        \n        self.instances.append(instance)\n        \n        # Simular implantação (em produção, usar API K8s)\n        await asyncio.sleep(60)  # Tempo de carregamento do modelo\n        instance.status = 'ready'\n        \n        return instance\n        \n    async def scale_decision(self) -> Dict[str, any]:\n        \"\"\"Tomar decisões de escalonamento baseadas em métricas de carga\"\"\"\n        if not self.instances:\n            return {'action': 'scale_up', 'reason': 'Nenhuma instância rodando'}\n            \n        # Calcular carga média entre instâncias\n        active_instances = [i for i in self.instances if i.status == 'ready']\n        if not active_instances:\n            return {'action': 'wait', 'reason': 'Nenhuma instância pronta'}\n            \n        avg_load = np.mean([i.load_percentage for i in active_instances])\n        \n        decision = {\n            'current_instances': len(active_instances),\n            'average_load': avg_load,\n            'action': 'none',\n            'reason': ''\n        }\n        \n        if avg_load > self.scale_threshold_high:\n            decision['action'] = 'scale_up'\n            decision['reason'] = f'Carga alta: {avg_load:.1%}'\n            decision['recommended_instances'] = min(\n                len(active_instances) + 2,  # Adicionar 2 instâncias\n                8  # Máximo de instâncias\n            )\n        elif avg_load < self.scale_threshold_low and len(active_instances) > 1:\n            decision['action'] = 'scale_down'\n            decision['reason'] = f'Carga baixa: {avg_load:.1%}'\n            decision['recommended_instances'] = max(\n                len(active_instances) - 1,  # Remover 1 instância\n                1  # Mínimo de instâncias\n            )\n            \n        return decision\n\n# Balanceador de Carga para Escalonamento Horizontal\nclass AILoadBalancer:\n    def __init__(self, instances: List[ModelInstance]):\n        self.instances = instances\n        self.request_counter = 0\n        self.routing_strategy = 'least_loaded'  # ou 'round_robin', 'hash_based'\n        \n    def select_instance(self, request_id: str) -> Optional[ModelInstance]:\n        \"\"\"Selecionar melhor instância para requisição\"\"\"\n        ready_instances = [i for i in self.instances if i.status == 'ready']\n        \n        if not ready_instances:\n            return None\n            \n        if self.routing_strategy == 'round_robin':\n            instance = ready_instances[self.request_counter % len(ready_instances)]\n            self.request_counter += 1\n            return instance\n            \n        elif self.routing_strategy == 'least_loaded':\n            return min(ready_instances, key=lambda x: x.load_percentage)\n            \n        elif self.routing_strategy == 'hash_based':\n            # Hash consistente para afinidade de sessão\n            hash_value = int(hashlib.md5(request_id.encode()).hexdigest(), 16)\n            return ready_instances[hash_value % len(ready_instances)]\n            \n    async def route_request(self, request_data: Dict) -> Dict:\n        \"\"\"Rotear requisição de inferência para instância selecionada\"\"\"\n        instance = self.select_instance(request_data.get('request_id', str(time.time())))\n        \n        if not instance:\n            return {'error': 'Nenhuma instância disponível'}\n            \n        # Encaminhar requisição para instância selecionada\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{instance.endpoint}/inference\",\n                json=request_data\n            ) as response:\n                result = await response.json()\n                \n        # Atualizar carga da instância (simplificado)\n        instance.load_percentage = min(instance.load_percentage + 0.1, 1.0)\n        \n        return {\n            'result': result,\n            'routed_to': instance.instance_id,\n            'instance_load': instance.load_percentage\n        }",
            "highlightLines": [25, 26, 34, 35, 36, 37, 38, 51, 52, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65, 110, 111, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 146, 147, 148, 149, 153, 154, 155, 157, 158, 159, 160],
            "runnable": false
          },
          {
            "id": "vertical-scaling",
            "title": "Estratégia de Escalonamento Vertical",
            "description": "Implementando escalonamento vertical para cargas de trabalho intensivas em GPU",
            "language": "python",
            "code": "import torch\nimport psutil\nimport GPUtil\nfrom typing import Dict, List, Optional, Tuple\nimport threading\nimport time\nfrom enum import Enum\n\nclass GPUTier(Enum):\n    T4 = {'memory_gb': 16, 'compute_capability': 7.5, 'cost_per_hour': 0.35}\n    V100 = {'memory_gb': 32, 'compute_capability': 7.0, 'cost_per_hour': 2.48}\n    A100_40GB = {'memory_gb': 40, 'compute_capability': 8.0, 'cost_per_hour': 3.67}\n    A100_80GB = {'memory_gb': 80, 'compute_capability': 8.0, 'cost_per_hour': 5.12}\n    H100 = {'memory_gb': 80, 'compute_capability': 9.0, 'cost_per_hour': 8.00}\n\nclass VerticalScalingManager:\n    def __init__(self, initial_gpu_tier: GPUTier):\n        self.current_tier = initial_gpu_tier\n        self.performance_history = []\n        self.scaling_lock = threading.Lock()\n        self.cost_threshold = 10.0  # Max $/hora\n        \n    def analyze_performance_metrics(self) -> Dict:\n        \"\"\"Analisar desempenho atual e utilização de recursos\"\"\"\n        metrics = {\n            'timestamp': time.time(),\n            'gpu_utilization': 0.0,\n            'memory_usage_gb': 0.0,\n            'inference_latency_ms': 0.0,\n            'batch_size': 0,\n            'throughput_rps': 0.0\n        }\n        \n        # Obter métricas de GPU\n        gpus = GPUtil.getGPUs()\n        if gpus:\n            gpu = gpus[0]\n            metrics['gpu_utilization'] = gpu.load\n            metrics['memory_usage_gb'] = gpu.memoryUsed / 1024\n            \n        # Simular métricas de desempenho (em produção, obter do monitoramento)\n        metrics['inference_latency_ms'] = 50 + (metrics['gpu_utilization'] * 100)\n        metrics['batch_size'] = 32 if self.current_tier != GPUTier.T4 else 16\n        metrics['throughput_rps'] = 1000 / metrics['inference_latency_ms'] * metrics['batch_size']\n        \n        self.performance_history.append(metrics)\n        return metrics\n        \n    def recommend_scaling_action(self) -> Tuple[str, Optional[GPUTier], Dict]:\n        \"\"\"Recomendar ação de escalonamento vertical baseada em métricas\"\"\"\n        if len(self.performance_history) < 5:\n            return 'wait', None, {'reason': 'Dados insuficientes'}\n            \n        recent_metrics = self.performance_history[-5:]\n        \n        # Calcular médias\n        avg_gpu_util = np.mean([m['gpu_utilization'] for m in recent_metrics])\n        avg_memory_usage = np.mean([m['memory_usage_gb'] for m in recent_metrics])\n        avg_latency = np.mean([m['inference_latency_ms'] for m in recent_metrics])\n        \n        current_memory = self.current_tier.value['memory_gb']\n        current_cost = self.current_tier.value['cost_per_hour']\n        \n        analysis = {\n            'current_tier': self.current_tier.name,\n            'avg_gpu_utilization': avg_gpu_util,\n            'avg_memory_usage_gb': avg_memory_usage,\n            'avg_latency_ms': avg_latency,\n            'memory_utilization': avg_memory_usage / current_memory\n        }\n        \n        # Condições de scale up\n        if avg_gpu_util > 0.9 or analysis['memory_utilization'] > 0.85:\n            # Precisa de mais recursos\n            next_tier = self._find_next_tier_up()\n            if next_tier and next_tier.value['cost_per_hour'] <= self.cost_threshold:\n                return 'scale_up', next_tier, {\n                    **analysis,\n                    'reason': 'Alta utilização de recursos',\n                    'expected_improvement': self._estimate_improvement(next_tier)\n                }\n                \n        # Condições de scale down\n        elif avg_gpu_util < 0.3 and analysis['memory_utilization'] < 0.4:\n            # Superprovisionado\n            next_tier = self._find_next_tier_down()\n            if next_tier:\n                return 'scale_down', next_tier, {\n                    **analysis,\n                    'reason': 'Baixa utilização de recursos',\n                    'expected_savings': current_cost - next_tier.value['cost_per_hour']\n                }\n                \n        return 'maintain', None, analysis\n        \n    def _find_next_tier_up(self) -> Optional[GPUTier]:\n        \"\"\"Encontrar próximo nível superior de GPU\"\"\"\n        tier_order = [GPUTier.T4, GPUTier.V100, GPUTier.A100_40GB, GPUTier.A100_80GB, GPUTier.H100]\n        current_index = tier_order.index(self.current_tier)\n        \n        if current_index < len(tier_order) - 1:\n            return tier_order[current_index + 1]\n        return None\n        \n    def _find_next_tier_down(self) -> Optional[GPUTier]:\n        \"\"\"Encontrar próximo nível inferior de GPU\"\"\"\n        tier_order = [GPUTier.T4, GPUTier.V100, GPUTier.A100_40GB, GPUTier.A100_80GB, GPUTier.H100]\n        current_index = tier_order.index(self.current_tier)\n        \n        if current_index > 0:\n            return tier_order[current_index - 1]\n        return None\n        \n    def _estimate_improvement(self, new_tier: GPUTier) -> Dict:\n        \"\"\"Estimar melhoria de desempenho do scale up\"\"\"\n        current_compute = self.current_tier.value['compute_capability']\n        new_compute = new_tier.value['compute_capability']\n        \n        current_memory = self.current_tier.value['memory_gb']\n        new_memory = new_tier.value['memory_gb']\n        \n        return {\n            'compute_improvement': (new_compute / current_compute - 1) * 100,\n            'memory_increase': (new_memory / current_memory - 1) * 100,\n            'estimated_latency_reduction': min(30, (new_compute / current_compute - 1) * 20),\n            'max_batch_size_increase': (new_memory / current_memory) * 100\n        }\n        \n    async def execute_scaling(self, new_tier: GPUTier) -> Dict:\n        \"\"\"Executar operação de escalonamento vertical\"\"\"\n        with self.scaling_lock:\n            scaling_result = {\n                'start_time': time.time(),\n                'from_tier': self.current_tier.name,\n                'to_tier': new_tier.name,\n                'status': 'in_progress'\n            }\n            \n            try:\n                # Passos para escalonamento vertical:\n                # 1. Provisionar nova instância com GPU de nível superior\n                # 2. Carregar modelo na nova instância\n                # 3. Aquecer o modelo\n                # 4. Alternar tráfego para nova instância\n                # 5. Descomissionar instância antiga\n                \n                # Simular processo de escalonamento\n                await asyncio.sleep(120)  # 2 minutos para nova instância\n                \n                self.current_tier = new_tier\n                scaling_result['status'] = 'completed'\n                scaling_result['end_time'] = time.time()\n                scaling_result['duration_seconds'] = scaling_result['end_time'] - scaling_result['start_time']\n                \n            except Exception as e:\n                scaling_result['status'] = 'failed'\n                scaling_result['error'] = str(e)\n                \n            return scaling_result",
            "highlightLines": [9, 10, 11, 12, 13, 14, 37, 38, 39, 41, 42, 43, 56, 57, 58, 59, 61, 62, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 90, 119, 120, 121, 122, 123],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "load-balancing",
      "title": "Balanceamento de Carga para Endpoints de IA",
      "type": "content",
      "order": 3,
      "estimatedDuration": "20 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#load-balancing",
        "codeExamples": [
          {
            "id": "intelligent-load-balancer",
            "title": "Balanceador de Carga Inteligente para IA",
            "description": "Balanceamento de carga avançado com roteamento consciente de modelo e agrupamento de requisições",
            "language": "python",
            "code": "import asyncio\nimport aiohttp\nfrom typing import List, Dict, Optional, Tuple\nimport numpy as np\nfrom dataclasses import dataclass, field\nimport time\nimport heapq\nfrom collections import defaultdict\nimport threading\n\n@dataclass\nclass InferenceRequest:\n    request_id: str\n    data: Dict\n    priority: int = 0\n    arrival_time: float = field(default_factory=time.time)\n    required_model: Optional[str] = None\n    max_latency_ms: Optional[float] = None\n    \n    def __lt__(self, other):\n        return self.priority > other.priority  # Prioridade maior primeiro\n\n@dataclass\nclass ModelEndpoint:\n    endpoint_id: str\n    url: str\n    model_name: str\n    model_version: str\n    capacity: int  # Tamanho máximo do batch\n    current_load: int = 0\n    avg_latency_ms: float = 50.0\n    success_rate: float = 0.99\n    last_updated: float = field(default_factory=time.time)\n    gpu_memory_gb: int = 16\n    supports_batching: bool = True\n\nclass IntelligentLoadBalancer:\n    def __init__(self, endpoints: List[ModelEndpoint]):\n        self.endpoints = {ep.endpoint_id: ep for ep in endpoints}\n        self.request_queue = []  # Fila de prioridade\n        self.batch_window_ms = 50  # Tempo para aguardar batching\n        self.routing_strategy = 'latency_aware'  # ou 'round_robin', 'least_loaded'\n        self.metrics = defaultdict(lambda: {'requests': 0, 'latency': [], 'errors': 0})\n        self._lock = threading.Lock()\n        self._stop_batching = False\n        \n        # Iniciar processador de batch\n        asyncio.create_task(self._batch_processor())\n        \n    async def route_request(self, request: InferenceRequest) -> Dict:\n        \"\"\"Rotear requisição única ou adicionar à fila de batch\"\"\"\n        \n        # Encontrar endpoints compatíveis\n        compatible_endpoints = self._find_compatible_endpoints(request)\n        \n        if not compatible_endpoints:\n            return {'error': 'Nenhum endpoint compatível disponível'}\n            \n        # Verificar se requisição requer processamento imediato\n        if request.max_latency_ms and request.max_latency_ms < self.batch_window_ms:\n            # Rotear imediatamente sem batching\n            endpoint = self._select_endpoint(compatible_endpoints, [request])\n            return await self._send_request(endpoint, [request])\n            \n        # Adicionar à fila de batch\n        with self._lock:\n            heapq.heappush(self.request_queue, request)\n            \n        # Aguardar processamento em batch\n        return {'status': 'queued', 'request_id': request.request_id}\n        \n    def _find_compatible_endpoints(self, request: InferenceRequest) -> List[ModelEndpoint]:\n        \"\"\"Encontrar endpoints que podem lidar com a requisição\"\"\"\n        compatible = []\n        \n        for endpoint in self.endpoints.values():\n            # Verificar compatibilidade do modelo\n            if request.required_model and endpoint.model_name != request.required_model:\n                continue\n                \n            # Verificar se endpoint está saudável\n            if endpoint.success_rate < 0.9:\n                continue\n                \n            # Verificar capacidade\n            if endpoint.current_load >= endpoint.capacity:\n                continue\n                \n            compatible.append(endpoint)\n            \n        return compatible\n        \n    def _select_endpoint(self, endpoints: List[ModelEndpoint], batch: List[InferenceRequest]) -> ModelEndpoint:\n        \"\"\"Selecionar melhor endpoint baseado na estratégia de roteamento\"\"\"\n        \n        if self.routing_strategy == 'latency_aware':\n            # Considerar tanto latência quanto carga atual\n            scores = []\n            for ep in endpoints:\n                # Score menor é melhor\n                latency_score = ep.avg_latency_ms\n                load_score = (ep.current_load / ep.capacity) * 100\n                total_score = latency_score + load_score\n                scores.append((total_score, ep))\n                \n            return min(scores, key=lambda x: x[0])[1]\n            \n        elif self.routing_strategy == 'least_loaded':\n            return min(endpoints, key=lambda ep: ep.current_load / ep.capacity)\n            \n        elif self.routing_strategy == 'round_robin':\n            # Round-robin simples (precisaria de contador em produção)\n            return endpoints[0]\n            \n        else:\n            # Padrão para primeiro disponível\n            return endpoints[0]\n            \n    async def _batch_processor(self):\n        \"\"\"Processar requisições em batches para eficiência\"\"\"\n        while not self._stop_batching:\n            await asyncio.sleep(self.batch_window_ms / 1000)\n            \n            with self._lock:\n                if not self.request_queue:\n                    continue\n                    \n                # Agrupar requisições por requisito de modelo\n                model_batches = defaultdict(list)\n                temp_queue = []\n                \n                while self.request_queue and len(temp_queue) < 100:\n                    request = heapq.heappop(self.request_queue)\n                    model_key = request.required_model or 'default'\n                    model_batches[model_key].append(request)\n                    \n                # Devolver requisições não processadas\n                for req in temp_queue:\n                    heapq.heappush(self.request_queue, req)\n                    \n            # Processar batch de cada modelo\n            for model_name, batch in model_batches.items():\n                await self._process_batch(model_name, batch)\n                \n    async def _process_batch(self, model_name: str, batch: List[InferenceRequest]):\n        \"\"\"Processar batch de requisições para modelo específico\"\"\"\n        \n        # Encontrar endpoints compatíveis\n        compatible_endpoints = [\n            ep for ep in self.endpoints.values()\n            if ep.model_name == model_name or model_name == 'default'\n        ]\n        \n        if not compatible_endpoints:\n            # Lidar com erro para todas as requisições no batch\n            for request in batch:\n                self.metrics[request.request_id]['errors'] += 1\n            return\n            \n        # Dividir batch se exceder capacidade do endpoint\n        endpoint = self._select_endpoint(compatible_endpoints, batch)\n        \n        if len(batch) > endpoint.capacity:\n            # Dividir em batches menores\n            for i in range(0, len(batch), endpoint.capacity):\n                sub_batch = batch[i:i + endpoint.capacity]\n                await self._send_request(endpoint, sub_batch)\n        else:\n            await self._send_request(endpoint, batch)\n            \n    async def _send_request(self, endpoint: ModelEndpoint, batch: List[InferenceRequest]) -> Dict:\n        \"\"\"Enviar requisição em batch para endpoint\"\"\"\n        \n        # Atualizar carga do endpoint\n        endpoint.current_load += len(batch)\n        \n        # Preparar dados do batch\n        batch_data = {\n            'requests': [\n                {\n                    'id': req.request_id,\n                    'data': req.data,\n                    'priority': req.priority\n                }\n                for req in batch\n            ],\n            'batch_size': len(batch)\n        }\n        \n        start_time = time.time()\n        \n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    f\"{endpoint.url}/batch_inference\",\n                    json=batch_data,\n                    timeout=aiohttp.ClientTimeout(total=30)\n                ) as response:\n                    result = await response.json()\n                    \n            # Atualizar métricas\n            latency_ms = (time.time() - start_time) * 1000\n            endpoint.avg_latency_ms = 0.9 * endpoint.avg_latency_ms + 0.1 * latency_ms\n            \n            for req in batch:\n                self.metrics[req.request_id]['latency'].append(latency_ms)\n                self.metrics[req.request_id]['requests'] += 1\n                \n            return {\n                'status': 'success',\n                'results': result,\n                'endpoint': endpoint.endpoint_id,\n                'latency_ms': latency_ms,\n                'batch_size': len(batch)\n            }\n            \n        except Exception as e:\n            # Atualizar métricas de erro\n            endpoint.success_rate *= 0.95  # Decaimento da taxa de sucesso\n            \n            for req in batch:\n                self.metrics[req.request_id]['errors'] += 1\n                \n            return {\n                'status': 'error',\n                'error': str(e),\n                'endpoint': endpoint.endpoint_id\n            }\n            \n        finally:\n            # Atualizar carga do endpoint\n            endpoint.current_load -= len(batch)\n            endpoint.last_updated = time.time()\n            \n    def get_endpoint_health(self) -> Dict[str, Dict]:\n        \"\"\"Obter status de saúde de todos os endpoints\"\"\"\n        health_status = {}\n        \n        for ep_id, endpoint in self.endpoints.items():\n            health_status[ep_id] = {\n                'model': endpoint.model_name,\n                'version': endpoint.model_version,\n                'current_load': endpoint.current_load,\n                'capacity': endpoint.capacity,\n                'utilization': endpoint.current_load / endpoint.capacity,\n                'avg_latency_ms': endpoint.avg_latency_ms,\n                'success_rate': endpoint.success_rate,\n                'gpu_memory_gb': endpoint.gpu_memory_gb,\n                'last_updated': time.time() - endpoint.last_updated\n            }\n            \n        return health_status",
            "highlightLines": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 40, 41, 42, 47, 48, 56, 57, 58, 59, 60, 61, 64, 65, 66, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 116, 117, 118, 119, 127, 128, 129, 130, 131, 138, 139, 140, 156, 157, 158, 159, 160, 161, 162, 163, 194, 195, 196, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "caching-strategies",
      "title": "Estratégias de Cache para IA",
      "type": "content",
      "order": 4,
      "estimatedDuration": "20 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#caching-strategies",
        "codeExamples": [
          {
            "id": "ai-response-cache",
            "title": "Cache Inteligente de Respostas de IA",
            "description": "Implementando cache semântico para resultados de inferência de IA",
            "language": "python",
            "code": "import hashlib\nimport json\nimport time\nimport numpy as np\nfrom typing import Dict, Any, Optional, List, Tuple\nimport redis\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport pickle\nfrom dataclasses import dataclass\nimport asyncio\nfrom collections import OrderedDict\n\n@dataclass\nclass CacheEntry:\n    key: str\n    response: Any\n    embedding: Optional[np.ndarray]\n    timestamp: float\n    hit_count: int = 0\n    compute_time_ms: float = 0\n    model_version: str = ''\n    confidence: float = 0.0\n\nclass AIResponseCache:\n    def __init__(self, \n                 cache_size_mb: int = 1024,\n                 ttl_seconds: int = 3600,\n                 similarity_threshold: float = 0.95):\n        \"\"\"\n        Inicializar cache de resposta de IA com correspondência de similaridade semântica\n        \n        Args:\n            cache_size_mb: Tamanho máximo do cache em MB\n            ttl_seconds: Tempo de vida para entradas do cache\n            similarity_threshold: Limite para correspondência de similaridade semântica\n        \"\"\"\n        self.cache_size_mb = cache_size_mb\n        self.ttl_seconds = ttl_seconds\n        self.similarity_threshold = similarity_threshold\n        \n        # Inicializar backends de cache\n        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n        self.local_cache = OrderedDict()  # Cache LRU\n        \n        # Componentes de similaridade semântica\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n        self.index = faiss.IndexFlatIP(384)  # Produto interno para similaridade cosseno\n        self.key_to_embedding = {}\n        \n        # Métricas\n        self.metrics = {\n            'hits': 0,\n            'misses': 0,\n            'semantic_hits': 0,\n            'evictions': 0,\n            'total_compute_saved_ms': 0\n        }\n        \n    def _generate_cache_key(self, request: Dict[str, Any]) -> str:\n        \"\"\"Gerar chave de cache determinística da requisição\"\"\"\n        # Ordenar chaves para consistência\n        normalized = json.dumps(request, sort_keys=True)\n        return hashlib.sha256(normalized.encode()).hexdigest()\n        \n    def _should_cache(self, response: Any, compute_time_ms: float) -> bool:\n        \"\"\"Determinar se resposta deve ser cacheada\"\"\"\n        # Não cachear erros\n        if isinstance(response, dict) and 'error' in response:\n            return False\n            \n        # Não cachear computações rápidas (provavelmente já cacheadas upstream)\n        if compute_time_ms < 10:\n            return False\n            \n        # Verificar tamanho da resposta\n        response_size = len(json.dumps(response).encode())\n        if response_size > 1024 * 1024:  # Limite de 1MB por entrada\n            return False\n            \n        return True\n        \n    async def get_or_compute(self, \n                           request: Dict[str, Any],\n                           compute_fn,\n                           model_version: str = 'v1.0') -> Tuple[Any, bool]:\n        \"\"\"Obter do cache ou computar e cachear resultado\"\"\"\n        \n        # Tentar correspondência exata primeiro\n        cache_key = self._generate_cache_key(request)\n        cached_response = await self._get_exact_match(cache_key)\n        \n        if cached_response:\n            self.metrics['hits'] += 1\n            self.metrics['total_compute_saved_ms'] += cached_response.compute_time_ms\n            return cached_response.response, True\n            \n        # Tentar correspondência de similaridade semântica\n        semantic_match = await self._find_semantic_match(request)\n        \n        if semantic_match:\n            self.metrics['semantic_hits'] += 1\n            self.metrics['total_compute_saved_ms'] += semantic_match.compute_time_ms\n            return semantic_match.response, True\n            \n        # Cache miss - computar resultado\n        self.metrics['misses'] += 1\n        start_time = time.time()\n        \n        response = await compute_fn(request)\n        \n        compute_time_ms = (time.time() - start_time) * 1000\n        \n        # Cachear resultado se apropriado\n        if self._should_cache(response, compute_time_ms):\n            await self._cache_response(\n                cache_key, request, response, compute_time_ms, model_version\n            )\n            \n        return response, False\n        \n    async def _get_exact_match(self, cache_key: str) -> Optional[CacheEntry]:\n        \"\"\"Obter correspondência exata do cache\"\"\"\n        \n        # Verificar cache local primeiro\n        if cache_key in self.local_cache:\n            entry = self.local_cache[cache_key]\n            if time.time() - entry.timestamp < self.ttl_seconds:\n                entry.hit_count += 1\n                # Mover para final (LRU)\n                self.local_cache.move_to_end(cache_key)\n                return entry\n            else:\n                # Expirado\n                del self.local_cache[cache_key]\n                \n        # Verificar Redis\n        redis_data = self.redis_client.get(f\"ai_cache:{cache_key}\")\n        if redis_data:\n            entry = pickle.loads(redis_data)\n            if time.time() - entry.timestamp < self.ttl_seconds:\n                entry.hit_count += 1\n                # Adicionar ao cache local\n                self._add_to_local_cache(cache_key, entry)\n                return entry\n            else:\n                # Expirado\n                self.redis_client.delete(f\"ai_cache:{cache_key}\")\n                \n        return None\n        \n    async def _find_semantic_match(self, request: Dict[str, Any]) -> Optional[CacheEntry]:\n        \"\"\"Encontrar resposta cacheada semanticamente similar\"\"\"\n        \n        if self.index.ntotal == 0:\n            return None\n            \n        # Gerar embedding para requisição\n        request_text = json.dumps(request, sort_keys=True)\n        request_embedding = self.encoder.encode([request_text])[0]\n        request_embedding = request_embedding / np.linalg.norm(request_embedding)\n        \n        # Buscar embeddings similares\n        distances, indices = self.index.search(\n            request_embedding.reshape(1, -1), \n            k=min(10, self.index.ntotal)\n        )\n        \n        # Verificar limite de similaridade\n        if distances[0][0] < self.similarity_threshold:\n            return None\n            \n        # Encontrar a chave de cache correspondente\n        for embedding_key, embedding in self.key_to_embedding.items():\n            if np.allclose(embedding, self.index.reconstruct(int(indices[0][0]))):\n                return await self._get_exact_match(embedding_key)\n                \n        return None\n        \n    async def _cache_response(self, \n                            cache_key: str,\n                            request: Dict[str, Any],\n                            response: Any,\n                            compute_time_ms: float,\n                            model_version: str):\n        \"\"\"Cachear resposta com embedding semântico\"\"\"\n        \n        # Gerar embedding\n        request_text = json.dumps(request, sort_keys=True)\n        embedding = self.encoder.encode([request_text])[0]\n        embedding = embedding / np.linalg.norm(embedding)\n        \n        # Criar entrada de cache\n        entry = CacheEntry(\n            key=cache_key,\n            response=response,\n            embedding=embedding,\n            timestamp=time.time(),\n            compute_time_ms=compute_time_ms,\n            model_version=model_version,\n            confidence=response.get('confidence', 0.0) if isinstance(response, dict) else 0.0\n        )\n        \n        # Adicionar aos caches\n        self._add_to_local_cache(cache_key, entry)\n        \n        # Adicionar ao Redis com TTL\n        self.redis_client.setex(\n            f\"ai_cache:{cache_key}\",\n            self.ttl_seconds,\n            pickle.dumps(entry)\n        )\n        \n        # Adicionar embedding ao índice FAISS\n        self.index.add(embedding.reshape(1, -1))\n        self.key_to_embedding[cache_key] = embedding\n        \n    def _add_to_local_cache(self, cache_key: str, entry: CacheEntry):\n        \"\"\"Adicionar entrada ao cache LRU local com gerenciamento de tamanho\"\"\"\n        \n        # Verificar tamanho do cache\n        current_size = sum(\n            len(pickle.dumps(e)) for e in self.local_cache.values()\n        )\n        \n        # Despejar se necessário\n        while current_size > self.cache_size_mb * 1024 * 1024 and self.local_cache:\n            # Remover menos recentemente usado\n            evicted_key, evicted_entry = self.local_cache.popitem(last=False)\n            current_size -= len(pickle.dumps(evicted_entry))\n            self.metrics['evictions'] += 1\n            \n        # Adicionar nova entrada\n        self.local_cache[cache_key] = entry\n        \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Obter estatísticas abrangentes do cache\"\"\"\n        total_requests = self.metrics['hits'] + self.metrics['misses']\n        \n        return {\n            'total_requests': total_requests,\n            'hit_rate': self.metrics['hits'] / total_requests if total_requests > 0 else 0,\n            'semantic_hit_rate': self.metrics['semantic_hits'] / total_requests if total_requests > 0 else 0,\n            'exact_hits': self.metrics['hits'] - self.metrics['semantic_hits'],\n            'misses': self.metrics['misses'],\n            'evictions': self.metrics['evictions'],\n            'compute_time_saved_hours': self.metrics['total_compute_saved_ms'] / (1000 * 3600),\n            'local_cache_entries': len(self.local_cache),\n            'semantic_index_size': self.index.ntotal,\n            'cache_size_mb': sum(len(pickle.dumps(e)) for e in self.local_cache.values()) / (1024 * 1024)\n        }\n        \n    async def invalidate_by_model(self, model_version: str):\n        \"\"\"Invalidar todas as entradas de cache para versão específica do modelo\"\"\"\n        \n        # Limpar do cache local\n        keys_to_remove = [\n            k for k, v in self.local_cache.items() \n            if v.model_version == model_version\n        ]\n        \n        for key in keys_to_remove:\n            del self.local_cache[key]\n            \n        # Limpar do Redis (precisaria manter índice em produção)\n        # Por enquanto, limpar tudo e reconstruir\n        for key in self.redis_client.scan_iter(\"ai_cache:*\"):\n            entry_data = self.redis_client.get(key)\n            if entry_data:\n                entry = pickle.loads(entry_data)\n                if entry.model_version == model_version:\n                    self.redis_client.delete(key)",
            "highlightLines": [26, 27, 28, 29, 41, 42, 45, 46, 47, 48, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 108, 109, 110, 111, 112, 113, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 206, 207, 208, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "queue-architecture",
      "title": "Arquitetura Baseada em Filas",
      "type": "content",
      "order": 5,
      "estimatedDuration": "20 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#queue-architecture",
        "codeExamples": [
          {
            "id": "ai-queue-system",
            "title": "Processamento de Requisições de IA Baseado em Filas",
            "description": "Implementando arquitetura robusta de filas para lidar com rajadas de inferência de IA",
            "language": "python",
            "code": "import asyncio\nimport aioredis\nfrom typing import Dict, Any, List, Optional, Callable\nimport json\nimport time\nfrom dataclasses import dataclass, asdict\nimport uuid\nfrom enum import Enum\nimport logging\nfrom collections import defaultdict\nimport numpy as np\n\nclass Priority(Enum):\n    CRITICAL = 0\n    HIGH = 1\n    NORMAL = 2\n    LOW = 3\n    BATCH = 4\n\n@dataclass\nclass AIRequest:\n    request_id: str\n    payload: Dict[str, Any]\n    priority: Priority\n    created_at: float\n    deadline_ms: Optional[float] = None\n    model_name: str = 'default'\n    batch_compatible: bool = True\n    max_retries: int = 3\n    retry_count: int = 0\n    callback_url: Optional[str] = None\n    \n    def to_json(self) -> str:\n        data = asdict(self)\n        data['priority'] = self.priority.value\n        return json.dumps(data)\n        \n    @classmethod\n    def from_json(cls, data: str) -> 'AIRequest':\n        obj = json.loads(data)\n        obj['priority'] = Priority(obj['priority'])\n        return cls(**obj)\n\nclass AIQueueSystem:\n    def __init__(self, \n                 redis_url: str = 'redis://localhost',\n                 num_workers: int = 4,\n                 batch_size: int = 32,\n                 batch_timeout_ms: int = 100):\n        \"\"\"\n        Inicializar sistema de processamento de IA baseado em filas\n        \n        Args:\n            redis_url: URL de conexão Redis\n            num_workers: Número de processos worker\n            batch_size: Tamanho máximo do batch para processamento\n            batch_timeout_ms: Tempo máximo para aguardar formação de batch\n        \"\"\"\n        self.redis_url = redis_url\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n        self.batch_timeout_ms = batch_timeout_ms\n        \n        self.redis = None\n        self.workers = []\n        self.running = False\n        \n        # Nomes de filas por prioridade\n        self.queue_names = {\n            Priority.CRITICAL: 'ai:queue:critical',\n            Priority.HIGH: 'ai:queue:high',\n            Priority.NORMAL: 'ai:queue:normal',\n            Priority.LOW: 'ai:queue:low',\n            Priority.BATCH: 'ai:queue:batch'\n        }\n        \n        # Métricas\n        self.metrics = defaultdict(lambda: {\n            'processed': 0,\n            'failed': 0,\n            'retried': 0,\n            'queue_time_ms': [],\n            'processing_time_ms': []\n        })\n        \n        self.logger = logging.getLogger(__name__)\n        \n    async def initialize(self):\n        \"\"\"Inicializar conexão Redis e iniciar workers\"\"\"\n        self.redis = await aioredis.from_url(self.redis_url)\n        self.running = True\n        \n        # Iniciar corrotinas worker\n        for i in range(self.num_workers):\n            worker = asyncio.create_task(self._worker(i))\n            self.workers.append(worker)\n            \n        # Iniciar processador de batch\n        self.workers.append(asyncio.create_task(self._batch_processor()))\n        \n        # Iniciar reporter de métricas\n        self.workers.append(asyncio.create_task(self._metrics_reporter()))\n        \n        self.logger.info(f\"Sistema de filas inicializado com {self.num_workers} workers\")\n        \n    async def submit_request(self, \n                           request: AIRequest,\n                           wait_for_result: bool = False) -> Dict[str, Any]:\n        \"\"\"Submeter requisição para fila apropriada\"\"\"\n        \n        # Validar requisição\n        if request.deadline_ms and request.deadline_ms < 100:\n            return {'error': 'Prazo muito curto', 'min_deadline_ms': 100}\n            \n        # Armazenar dados da requisição\n        request_key = f\"ai:request:{request.request_id}\"\n        await self.redis.setex(\n            request_key,\n            3600,  # TTL de 1 hora\n            request.to_json()\n        )\n        \n        # Adicionar à fila apropriada\n        queue_name = self.queue_names[request.priority]\n        \n        if request.priority == Priority.CRITICAL:\n            # Usar LPUSH para processamento LIFO de requisições críticas\n            await self.redis.lpush(queue_name, request.request_id)\n        else:\n            # Usar RPUSH para processamento FIFO\n            await self.redis.rpush(queue_name, request.request_id)\n            \n        # Atualizar métricas de fila\n        queue_length = await self.redis.llen(queue_name)\n        await self.redis.hset('ai:metrics:queues', queue_name, queue_length)\n        \n        if wait_for_result:\n            # Aguardar resultado (com timeout)\n            result = await self._wait_for_result(request.request_id, request.deadline_ms)\n            return result\n        else:\n            return {\n                'status': 'queued',\n                'request_id': request.request_id,\n                'queue': queue_name,\n                'position': queue_length\n            }\n            \n    async def _worker(self, worker_id: int):\n        \"\"\"Corrotina worker para processar requisições\"\"\"\n        self.logger.info(f\"Worker {worker_id} iniciado\")\n        \n        while self.running:\n            try:\n                # Verificar filas em ordem de prioridade\n                request_id = None\n                queue_name = None\n                \n                for priority in Priority:\n                    queue_name = self.queue_names[priority]\n                    \n                    # Usar BLPOP com timeout para pop bloqueante\n                    result = await self.redis.blpop(queue_name, timeout=1)\n                    \n                    if result:\n                        _, request_id = result\n                        request_id = request_id.decode()\n                        break\n                        \n                if not request_id:\n                    continue\n                    \n                # Processar requisição\n                await self._process_request(request_id, worker_id)\n                \n            except Exception as e:\n                self.logger.error(f\"Erro do worker {worker_id}: {e}\")\n                await asyncio.sleep(1)\n                \n    async def _process_request(self, request_id: str, worker_id: int):\n        \"\"\"Processar uma única requisição\"\"\"\n        start_time = time.time()\n        \n        # Recuperar dados da requisição\n        request_key = f\"ai:request:{request_id}\"\n        request_data = await self.redis.get(request_key)\n        \n        if not request_data:\n            self.logger.warning(f\"Requisição {request_id} não encontrada\")\n            return\n            \n        request = AIRequest.from_json(request_data)\n        \n        # Verificar prazo\n        if request.deadline_ms:\n            elapsed_ms = (time.time() - request.created_at) * 1000\n            if elapsed_ms > request.deadline_ms:\n                await self._mark_failed(request, \"Prazo excedido\")\n                return\n                \n        try:\n            # Simular processamento de IA\n            result = await self._run_inference(request)\n            \n            # Armazenar resultado\n            result_key = f\"ai:result:{request_id}\"\n            await self.redis.setex(\n                result_key,\n                3600,  # TTL de 1 hora\n                json.dumps(result)\n            )\n            \n            # Atualizar métricas\n            processing_time = (time.time() - start_time) * 1000\n            queue_time = (start_time - request.created_at) * 1000\n            \n            self.metrics[request.priority.name]['processed'] += 1\n            self.metrics[request.priority.name]['queue_time_ms'].append(queue_time)\n            self.metrics[request.priority.name]['processing_time_ms'].append(processing_time)\n            \n            # Enviar callback se configurado\n            if request.callback_url:\n                await self._send_callback(request.callback_url, result)\n                \n        except Exception as e:\n            self.logger.error(f\"Erro de processamento para {request_id}: {e}\")\n            \n            # Lógica de retry\n            if request.retry_count < request.max_retries:\n                request.retry_count += 1\n                await self.submit_request(request)\n                self.metrics[request.priority.name]['retried'] += 1\n            else:\n                await self._mark_failed(request, str(e))\n                \n    async def _batch_processor(self):\n        \"\"\"Processar requisições compatíveis com batch em batches\"\"\"\n        while self.running:\n            try:\n                # Coletar requisições de batch\n                batch_queue = self.queue_names[Priority.BATCH]\n                batch_requests = []\n                \n                # Aguardar primeira requisição\n                result = await self.redis.blpop(batch_queue, timeout=1)\n                if not result:\n                    continue\n                    \n                _, request_id = result\n                batch_requests.append(request_id.decode())\n                \n                # Coletar mais requisições até o tamanho do batch\n                deadline = time.time() + (self.batch_timeout_ms / 1000)\n                \n                while len(batch_requests) < self.batch_size and time.time() < deadline:\n                    result = await self.redis.lpop(batch_queue)\n                    if result:\n                        batch_requests.append(result.decode())\n                    else:\n                        await asyncio.sleep(0.01)\n                        \n                # Processar batch\n                if batch_requests:\n                    await self._process_batch(batch_requests)\n                    \n            except Exception as e:\n                self.logger.error(f\"Erro do processador de batch: {e}\")\n                await asyncio.sleep(1)\n                \n    async def _process_batch(self, request_ids: List[str]):\n        \"\"\"Processar batch de requisições juntas\"\"\"\n        self.logger.info(f\"Processando batch de {len(request_ids)} requisições\")\n        \n        # Recuperar todas as requisições\n        requests = []\n        for request_id in request_ids:\n            request_data = await self.redis.get(f\"ai:request:{request_id}\")\n            if request_data:\n                requests.append(AIRequest.from_json(request_data))\n                \n        if not requests:\n            return\n            \n        try:\n            # Inferência em batch\n            results = await self._run_batch_inference(requests)\n            \n            # Armazenar resultados\n            for request, result in zip(requests, results):\n                result_key = f\"ai:result:{request.request_id}\"\n                await self.redis.setex(result_key, 3600, json.dumps(result))\n                \n                # Atualizar métricas\n                self.metrics[Priority.BATCH.name]['processed'] += 1\n                \n        except Exception as e:\n            self.logger.error(f\"Erro de processamento em batch: {e}\")\n            # Reencaminhar requisições falhadas\n            for request in requests:\n                if request.retry_count < request.max_retries:\n                    request.retry_count += 1\n                    await self.submit_request(request)\n                    \n    async def _run_inference(self, request: AIRequest) -> Dict[str, Any]:\n        \"\"\"Simular inferência de IA\"\"\"\n        # Simular tempo de processamento baseado no modelo\n        processing_time = np.random.normal(100, 20)  # Média de 100ms\n        await asyncio.sleep(processing_time / 1000)\n        \n        return {\n            'request_id': request.request_id,\n            'model': request.model_name,\n            'result': f\"Processado {request.payload}\",\n            'confidence': np.random.uniform(0.8, 0.99),\n            'processing_time_ms': processing_time\n        }\n        \n    async def _run_batch_inference(self, requests: List[AIRequest]) -> List[Dict[str, Any]]:\n        \"\"\"Simular inferência de IA em batch\"\"\"\n        # Processamento em batch é mais eficiente\n        processing_time = np.random.normal(50, 10) * len(requests)  # Menos por requisição\n        await asyncio.sleep(processing_time / 1000)\n        \n        return [\n            {\n                'request_id': req.request_id,\n                'model': req.model_name,\n                'result': f\"Processado em batch {req.payload}\",\n                'confidence': np.random.uniform(0.8, 0.99),\n                'batch_size': len(requests)\n            }\n            for req in requests\n        ]\n        \n    async def get_queue_status(self) -> Dict[str, Any]:\n        \"\"\"Obter status atual da fila e métricas\"\"\"\n        status = {\n            'queues': {},\n            'workers': len(self.workers),\n            'metrics': {}\n        }\n        \n        # Tamanhos das filas\n        for priority, queue_name in self.queue_names.items():\n            length = await self.redis.llen(queue_name)\n            status['queues'][priority.name] = length\n            \n        # Métricas de processamento\n        for priority_name, metrics in self.metrics.items():\n            if metrics['processing_time_ms']:\n                status['metrics'][priority_name] = {\n                    'processed': metrics['processed'],\n                    'failed': metrics['failed'],\n                    'retried': metrics['retried'],\n                    'avg_queue_time_ms': np.mean(metrics['queue_time_ms']),\n                    'avg_processing_time_ms': np.mean(metrics['processing_time_ms']),\n                    'p95_processing_time_ms': np.percentile(metrics['processing_time_ms'], 95)\n                }\n                \n        return status",
            "highlightLines": [13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 47, 48, 49, 50, 51, 52, 53, 54, 55, 68, 69, 70, 71, 72, 73, 74, 122, 123, 124, 125, 126, 127, 128, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 187, 188, 189, 190, 191, 192, 193, 220, 221, 222, 223, 224, 225, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290],
            "runnable": false
          }
        ]
      }
    },
    {
      "id": "auto-scaling",
      "title": "Políticas de Auto-escalonamento",
      "type": "content",
      "order": 6,
      "estimatedDuration": "15 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#auto-scaling"
      }
    },
    {
      "id": "cost-optimization",
      "title": "Otimização de Custos em Escala",
      "type": "content",
      "order": 7,
      "estimatedDuration": "15 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#cost-optimization"
      }
    },
    {
      "id": "multi-region",
      "title": "Implantação Multi-região",
      "type": "content",
      "order": 8,
      "estimatedDuration": "10 minutos",
      "content": {
        "type": "mdx",
        "source": "03-scaling-strategies.mdx#multi-region"
      }
    },
    {
      "id": "hands-on-exercise",
      "title": "Construa um Serviço de IA Escalável",
      "type": "exercise",
      "order": 9,
      "estimatedDuration": "45 minutos",
      "content": {
        "type": "exercise",
        "title": "Implementar Serviço de IA Escalável",
        "description": "Construa um serviço completo de IA escalável com auto-escalonamento, cache e gerenciamento de filas",
        "exerciseType": "coding",
        "difficulty": "hard",
        "instructions": [
          "Projete uma arquitetura escalável para um serviço de geração de texto de IA",
          "Implemente escalonamento horizontal com balanceamento de carga",
          "Adicione cache inteligente para requisições comuns",
          "Crie um sistema de filas para lidar com picos de tráfego",
          "Configure auto-escalonamento baseado em profundidade da fila e latência",
          "Adicione recursos de rastreamento e otimização de custos",
          "Teste o sistema sob várias condições de carga"
        ],
        "startingCode": "from typing import Dict, Any, List, Optional\nimport asyncio\nimport time\nimport hashlib\n\nclass ScalableAIService:\n    def __init__(self, \n                 initial_instances: int = 2,\n                 max_instances: int = 10,\n                 cache_size_mb: int = 512):\n        self.instances = initial_instances\n        self.max_instances = max_instances\n        self.cache_size_mb = cache_size_mb\n        \n        # TODO: Inicializar componentes\n        self.load_balancer = None\n        self.cache = None\n        self.queue = None\n        self.auto_scaler = None\n        \n    async def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Processar requisição de IA com cache e filas\"\"\"\n        # TODO: Verificar cache primeiro\n        # TODO: Rotear através do balanceador de carga\n        # TODO: Lidar com overflow da fila\n        # TODO: Rastrear métricas para auto-escalonamento\n        pass\n        \n    def scale_decision(self) -> str:\n        \"\"\"Tomar decisão de escalonamento baseada em métricas\"\"\"\n        # TODO: Analisar profundidade da fila\n        # TODO: Verificar latência média\n        # TODO: Considerar restrições de custo\n        # TODO: Retornar 'scale_up', 'scale_down', ou 'maintain'\n        pass\n        \n    async def apply_scaling(self, action: str):\n        \"\"\"Aplicar decisão de escalonamento\"\"\"\n        # TODO: Adicionar ou remover instâncias\n        # TODO: Atualizar balanceador de carga\n        # TODO: Garantir escalonamento gracioso\n        pass\n\n# Implementar e testar seu serviço de IA escalável\nif __name__ == \"__main__\":\n    service = ScalableAIService()\n    \n    # TODO: Simular padrões de carga variados\n    # TODO: Medir métricas de desempenho\n    # TODO: Verificar comportamento de auto-escalonamento",
        "solution": "from typing import Dict, Any, List, Optional, Tuple\nimport asyncio\nimport time\nimport hashlib\nimport json\nimport numpy as np\nfrom collections import defaultdict, deque, OrderedDict\nfrom dataclasses import dataclass, field\nimport heapq\n\n@dataclass\nclass Instance:\n    instance_id: str\n    capacity: int = 100\n    current_load: int = 0\n    latency_history: deque = field(default_factory=lambda: deque(maxlen=100))\n    \n    @property\n    def utilization(self) -> float:\n        return self.current_load / self.capacity\n        \n    @property\n    def avg_latency(self) -> float:\n        return np.mean(self.latency_history) if self.latency_history else 0\n\n@dataclass\nclass QueuedRequest:\n    request_id: str\n    data: Dict[str, Any]\n    priority: int\n    timestamp: float = field(default_factory=time.time)\n    \n    def __lt__(self, other):\n        return self.priority > other.priority\n\nclass ScalableAIService:\n    def __init__(self, \n                 initial_instances: int = 2,\n                 max_instances: int = 10,\n                 cache_size_mb: int = 512):\n        self.min_instances = initial_instances\n        self.max_instances = max_instances\n        self.cache_size_mb = cache_size_mb\n        \n        # Inicializar componentes\n        self.instances = {\n            f\"instance-{i}\": Instance(f\"instance-{i}\") \n            for i in range(initial_instances)\n        }\n        \n        # Implementação de cache\n        self.cache = OrderedDict()  # Cache LRU\n        self.cache_hits = 0\n        self.cache_misses = 0\n        \n        # Implementação de fila\n        self.request_queue = []  # Fila de prioridade\n        self.queue_processor_running = True\n        \n        # Métricas\n        self.metrics = {\n            'requests_processed': 0,\n            'queue_depth_history': deque(maxlen=100),\n            'latency_history': deque(maxlen=1000),\n            'scaling_events': []\n        }\n        \n        # Parâmetros de auto-escalonamento\n        self.scale_up_threshold = 0.8  # 80% de utilização\n        self.scale_down_threshold = 0.3  # 30% de utilização\n        self.scale_cooldown = 60  # segundos\n        self.last_scale_time = 0\n        \n        # Rastreamento de custos\n        self.instance_cost_per_hour = 1.0  # $1/hora por instância\n        self.total_cost = 0\n        \n        # Iniciar tarefas em background\n        asyncio.create_task(self._queue_processor())\n        asyncio.create_task(self._auto_scaling_loop())\n        asyncio.create_task(self._metrics_collector())\n        \n    def _generate_cache_key(self, request: Dict[str, Any]) -> str:\n        \"\"\"Gerar chave de cache da requisição\"\"\"\n        return hashlib.md5(json.dumps(request, sort_keys=True).encode()).hexdigest()\n        \n    async def process_request(self, request: Dict[str, Any], priority: int = 5) -> Dict[str, Any]:\n        \"\"\"Processar requisição de IA com cache e filas\"\"\"\n        request_id = str(time.time())\n        \n        # Verificar cache primeiro\n        cache_key = self._generate_cache_key(request)\n        if cache_key in self.cache:\n            self.cache_hits += 1\n            # Mover para final (LRU)\n            self.cache.move_to_end(cache_key)\n            cached_result = self.cache[cache_key]\n            return {\n                **cached_result,\n                'cached': True,\n                'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses)\n            }\n            \n        self.cache_misses += 1\n        \n        # Adicionar à fila\n        queued_request = QueuedRequest(\n            request_id=request_id,\n            data=request,\n            priority=priority\n        )\n        heapq.heappush(self.request_queue, queued_request)\n        \n        # Tentar processar imediatamente se capacidade disponível\n        instance = self._select_instance()\n        if instance and instance.utilization < 1.0:\n            return await self._process_on_instance(instance, request, cache_key)\n            \n        # Caso contrário, aguardar processamento da fila\n        return {\n            'status': 'queued',\n            'request_id': request_id,\n            'queue_position': len(self.request_queue),\n            'estimated_wait_ms': self._estimate_wait_time()\n        }\n        \n    def _select_instance(self) -> Optional[Instance]:\n        \"\"\"Selecionar instância com menor utilização\"\"\"\n        available_instances = [\n            inst for inst in self.instances.values() \n            if inst.utilization < 1.0\n        ]\n        \n        if not available_instances:\n            return None\n            \n        return min(available_instances, key=lambda x: x.utilization)\n        \n    async def _process_on_instance(self, \n                                  instance: Instance, \n                                  request: Dict[str, Any],\n                                  cache_key: str) -> Dict[str, Any]:\n        \"\"\"Processar requisição em instância específica\"\"\"\n        instance.current_load += 1\n        start_time = time.time()\n        \n        try:\n            # Simular processamento de IA\n            processing_time = np.random.normal(100, 20)  # Média de 100ms\n            await asyncio.sleep(processing_time / 1000)\n            \n            result = {\n                'result': f\"Processado: {request}\",\n                'confidence': np.random.uniform(0.8, 0.99),\n                'instance_id': instance.instance_id,\n                'processing_time_ms': processing_time\n            }\n            \n            # Atualizar métricas\n            latency = (time.time() - start_time) * 1000\n            instance.latency_history.append(latency)\n            self.metrics['latency_history'].append(latency)\n            self.metrics['requests_processed'] += 1\n            \n            # Cachear resultado\n            self._add_to_cache(cache_key, result)\n            \n            return result\n            \n        finally:\n            instance.current_load -= 1\n            \n    def _add_to_cache(self, key: str, value: Dict[str, Any]):\n        \"\"\"Adicionar ao cache LRU com limite de tamanho\"\"\"\n        # Estimar tamanho (simplificado)\n        entry_size = len(json.dumps(value).encode())\n        max_cache_size = self.cache_size_mb * 1024 * 1024\n        \n        # Despejar se necessário\n        current_size = sum(len(json.dumps(v).encode()) for v in self.cache.values())\n        while current_size + entry_size > max_cache_size and self.cache:\n            evicted_key, _ = self.cache.popitem(last=False)\n            current_size = sum(len(json.dumps(v).encode()) for v in self.cache.values())\n            \n        self.cache[key] = value\n        \n    async def _queue_processor(self):\n        \"\"\"Processar requisições enfileiradas\"\"\"\n        while self.queue_processor_running:\n            if not self.request_queue:\n                await asyncio.sleep(0.01)\n                continue\n                \n            # Obter requisição de maior prioridade\n            instance = self._select_instance()\n            if instance:\n                request = heapq.heappop(self.request_queue)\n                cache_key = self._generate_cache_key(request.data)\n                \n                # Processar assincronamente\n                asyncio.create_task(\n                    self._process_on_instance(instance, request.data, cache_key)\n                )\n                \n            await asyncio.sleep(0.001)\n            \n    async def _auto_scaling_loop(self):\n        \"\"\"Loop de controle de auto-escalonamento\"\"\"\n        while True:\n            await asyncio.sleep(10)  # Verificar a cada 10 segundos\n            \n            action = self.scale_decision()\n            if action != 'maintain':\n                await self.apply_scaling(action)\n                \n    def scale_decision(self) -> str:\n        \"\"\"Tomar decisão de escalonamento baseada em métricas\"\"\"\n        # Verificar cooldown\n        if time.time() - self.last_scale_time < self.scale_cooldown:\n            return 'maintain'\n            \n        # Calcular utilização média\n        utilizations = [inst.utilization for inst in self.instances.values()]\n        avg_utilization = np.mean(utilizations)\n        \n        # Consideração da profundidade da fila\n        queue_depth = len(self.request_queue)\n        self.metrics['queue_depth_history'].append(queue_depth)\n        avg_queue_depth = np.mean(self.metrics['queue_depth_history'])\n        \n        # Consideração de latência\n        if self.metrics['latency_history']:\n            p95_latency = np.percentile(self.metrics['latency_history'], 95)\n        else:\n            p95_latency = 0\n            \n        # Lógica de escalonamento\n        current_instances = len(self.instances)\n        \n        # Condições de scale up\n        if (avg_utilization > self.scale_up_threshold or \n            avg_queue_depth > 50 or \n            p95_latency > 200):  # Limite de 200ms\n            if current_instances < self.max_instances:\n                return 'scale_up'\n                \n        # Condições de scale down\n        elif (avg_utilization < self.scale_down_threshold and \n              avg_queue_depth < 10 and \n              p95_latency < 100):  # Bom desempenho\n            if current_instances > self.min_instances:\n                # Verificar otimização de custo\n                if self._can_reduce_cost():\n                    return 'scale_down'\n                    \n        return 'maintain'\n        \n    def _can_reduce_cost(self) -> bool:\n        \"\"\"Verificar se scale down otimizaria custo\"\"\"\n        current_cost = len(self.instances) * self.instance_cost_per_hour\n        projected_cost = (len(self.instances) - 1) * self.instance_cost_per_hour\n        \n        # Garantir que ainda pode lidar com carga com menos instâncias\n        total_capacity = sum(inst.capacity for inst in self.instances.values())\n        current_load = sum(inst.current_load for inst in self.instances.values())\n        \n        if current_load / (total_capacity - 100) < 0.7:  # 70% de utilização após scale down\n            return True\n        return False\n        \n    async def apply_scaling(self, action: str):\n        \"\"\"Aplicar decisão de escalonamento\"\"\"\n        current_instances = len(self.instances)\n        \n        if action == 'scale_up':\n            new_instance_id = f\"instance-{current_instances}\"\n            self.instances[new_instance_id] = Instance(new_instance_id)\n            \n            self.metrics['scaling_events'].append({\n                'timestamp': time.time(),\n                'action': 'scale_up',\n                'instances': current_instances + 1,\n                'reason': 'Alta utilização ou latência'\n            })\n            \n            print(f\"Escalonado para cima para {len(self.instances)} instâncias\")\n            \n        elif action == 'scale_down':\n            # Remover instância com menor utilização\n            instance_to_remove = min(\n                self.instances.values(), \n                key=lambda x: x.utilization\n            )\n            \n            # Aguardar instância drenar\n            while instance_to_remove.current_load > 0:\n                await asyncio.sleep(0.1)\n                \n            del self.instances[instance_to_remove.instance_id]\n            \n            self.metrics['scaling_events'].append({\n                'timestamp': time.time(),\n                'action': 'scale_down',\n                'instances': len(self.instances),\n                'reason': 'Baixa utilização'\n            })\n            \n            print(f\"Escalonado para baixo para {len(self.instances)} instâncias\")\n            \n        self.last_scale_time = time.time()\n        \n    async def _metrics_collector(self):\n        \"\"\"Coletar e rastrear métricas\"\"\"\n        while True:\n            await asyncio.sleep(60)  # A cada minuto\n            \n            # Calcular custo\n            self.total_cost += len(self.instances) * self.instance_cost_per_hour / 60\n            \n            # Registrar métricas\n            if self.metrics['requests_processed'] > 0:\n                print(f\"\\nResumo de Métricas:\")\n                print(f\"Instâncias: {len(self.instances)}\")\n                print(f\"Requisições processadas: {self.metrics['requests_processed']}\")\n                print(f\"Taxa de acerto do cache: {self.cache_hits / (self.cache_hits + self.cache_misses):.2%}\")\n                print(f\"Profundidade média da fila: {np.mean(self.metrics['queue_depth_history']):.1f}\")\n                print(f\"Latência P95: {np.percentile(self.metrics['latency_history'], 95):.1f}ms\")\n                print(f\"Custo total: ${self.total_cost:.2f}\")\n                print(f\"Custo por requisição: ${self.total_cost / self.metrics['requests_processed']:.4f}\")\n                \n    def _estimate_wait_time(self) -> float:\n        \"\"\"Estimar tempo de espera da fila\"\"\"\n        if not self.metrics['latency_history']:\n            return 0\n            \n        avg_processing_time = np.mean(self.metrics['latency_history'])\n        total_capacity = sum(inst.capacity for inst in self.instances.values())\n        \n        return (len(self.request_queue) / total_capacity) * avg_processing_time\n\n# Testar implementação\nasync def simulate_load():\n    service = ScalableAIService(initial_instances=2, max_instances=8)\n    \n    # Simular padrões de carga variados\n    load_patterns = [\n        (100, 10, 5),   # requisições, duração_segundos, prioridade\n        (500, 30, 3),   # Carga alta\n        (50, 20, 7),    # Carga baixa\n        (1000, 60, 1),  # Rajada\n        (200, 40, 5),   # Normal\n    ]\n    \n    for requests, duration, priority in load_patterns:\n        print(f\"\\nSimulando {requests} requisições em {duration}s...\")\n        \n        tasks = []\n        for i in range(requests):\n            delay = np.random.uniform(0, duration)\n            task = asyncio.create_task(\n                process_after_delay(service, delay, {\n                    'input': f'request_{i}',\n                    'data': np.random.rand(10).tolist()\n                }, priority)\n            )\n            tasks.append(task)\n            \n        await asyncio.gather(*tasks)\n        await asyncio.sleep(5)  # Cool down\n        \n    # Métricas finais\n    print(\"\\n=== Relatório Final de Desempenho ===\")\n    print(f\"Total de requisições: {service.metrics['requests_processed']}\")\n    print(f\"Eventos de escalonamento: {len(service.metrics['scaling_events'])}\")\n    print(f\"Custo final: ${service.total_cost:.2f}\")\n    \nasync def process_after_delay(service, delay, request, priority):\n    await asyncio.sleep(delay)\n    return await service.process_request(request, priority)\n\nif __name__ == \"__main__\":\n    asyncio.run(simulate_load())",
        "hints": [
          "Use uma fila de prioridade para tratamento de requisições",
          "Implemente cache LRU com limites de tamanho",
          "Rastreie métricas de utilização para cada instância",
          "Considere profundidade da fila e latência em decisões de escalonamento",
          "Implemente drenagem graciosa de instância ao escalonar para baixo",
          "Use asyncio para processamento concorrente de requisições"
        ],
        "validation": [
          {
            "type": "contains",
            "value": "class ScalableAIService",
            "message": "Defina a classe do serviço de IA escalável"
          },
          {
            "type": "contains",
            "value": "process_request",
            "message": "Implemente método de processamento de requisições"
          },
          {
            "type": "contains",
            "value": "scale_decision",
            "message": "Implemente lógica de decisão de escalonamento"
          },
          {
            "type": "contains",
            "value": "_select_instance",
            "message": "Implemente lógica de balanceamento de carga"
          },
          {
            "type": "contains",
            "value": "cache",
            "message": "Implemente mecanismo de cache"
          }
        ]
      }
    },
    {
      "id": "quiz",
      "title": "Quiz de Estratégias de Escalonamento",
      "type": "quiz",
      "order": 10,
      "estimatedDuration": "15 minutos",
      "content": {
        "type": "quiz",
        "title": "Verificação de Conhecimento sobre Escalonamento de IA",
        "description": "Teste sua compreensão de estratégias de escalonamento e gerenciamento de carga para sistemas de IA",
        "questions": [
          {
            "id": "q1",
            "type": "multiple-choice",
            "question": "Quais são os desafios únicos de escalonar sistemas de IA comparado a aplicações web tradicionais?",
            "points": 3,
            "options": [
              {
                "id": "a",
                "text": "Altos requisitos de memória para carregamento de modelo",
                "explanation": "Correto! Modelos de IA podem requerer vários GB de memória."
              },
              {
                "id": "b",
                "text": "Restrições e alocação de recursos GPU",
                "explanation": "Correto! GPUs são caras e têm requisitos de alocação exclusiva."
              },
              {
                "id": "c",
                "text": "Pool de conexões de banco de dados",
                "explanation": "Incorreto. Este é um desafio comum para todas as aplicações web."
              },
              {
                "id": "d",
                "text": "Tempos lentos de cold start devido ao carregamento do modelo",
                "explanation": "Correto! Carregar modelos grandes pode levar 30+ segundos."
              }
            ],
            "correctAnswers": ["a", "b", "d"],
            "randomizeOptions": true
          },
          {
            "id": "q2",
            "type": "true-false",
            "question": "Escalonamento horizontal é sempre mais econômico que escalonamento vertical para cargas de trabalho de IA.",
            "points": 2,
            "correctAnswer": false,
            "explanation": "Falso! Devido a restrições de GPU e sobrecarga de carregamento de modelo, escalonamento vertical (usar GPUs mais poderosas) pode frequentemente ser mais econômico para cargas de trabalho de IA."
          },
          {
            "id": "q3",
            "type": "multiple-choice",
            "question": "Qual estratégia de cache é mais eficaz para resultados de inferência de IA?",
            "points": 2,
            "options": [
              {
                "id": "a",
                "text": "Apenas expiração baseada em tempo",
                "explanation": "Incompleto. Isso perde oportunidades de correspondência semântica."
              },
              {
                "id": "b",
                "text": "Correspondência exata de chave com fallback de similaridade semântica",
                "explanation": "Correto! Isso fornece a melhor taxa de acerto para respostas de IA."
              },
              {
                "id": "c",
                "text": "Política de despejo aleatória",
                "explanation": "Incorreto. Isso é ineficiente para cargas de trabalho de IA."
              },
              {
                "id": "d",
                "text": "Sem cache devido a requisições únicas",
                "explanation": "Incorreto. Muitas requisições de IA têm similaridade semântica."
              }
            ],
            "correctAnswers": ["b"],
            "randomizeOptions": true
          },
          {
            "id": "q4",
            "type": "multiple-choice",
            "question": "Quais métricas devem disparar auto-escalonamento para serviços de IA?",
            "points": 3,
            "options": [
              {
                "id": "a",
                "text": "Porcentagem de utilização de GPU",
                "explanation": "Correto! Utilização de GPU é um indicador-chave de capacidade."
              },
              {
                "id": "b",
                "text": "Profundidade da fila e tempo de espera",
                "explanation": "Correto! Métricas de fila indicam demanda excedendo capacidade."
              },
              {
                "id": "c",
                "text": "Percentis de latência de inferência (p95, p99)",
                "explanation": "Correto! Degradação de latência indica sobrecarga."
              },
              {
                "id": "d",
                "text": "Apenas utilização de CPU",
                "explanation": "Incorreto. CPU raramente é o gargalo para inferência de IA."
              }
            ],
            "correctAnswers": ["a", "b", "c"],
            "randomizeOptions": true
          },
          {
            "id": "q5",
            "type": "true-false",
            "question": "Arquiteturas baseadas em filas ajudam a lidar com picos de tráfego ao desacoplar aceitação de requisições do processamento.",
            "points": 1,
            "correctAnswer": true,
            "explanation": "Verdadeiro! Filas permitem que o sistema aceite requisições mesmo quando a capacidade de processamento está temporariamente excedida."
          }
        ],
        "passingScore": 75,
        "allowRetries": true,
        "showCorrectAnswers": true,
        "randomizeQuestions": false
      }
    }
  ],
  "resources": [
    {
      "id": "kubernetes-gpu-guide",
      "title": "Guia de Agendamento de GPU do Kubernetes",
      "type": "documentation",
      "url": "https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/",
      "description": "Guia oficial para gerenciamento de recursos GPU no Kubernetes",
      "required": true
    },
    {
      "id": "aws-sagemaker-scaling",
      "title": "Auto Escalonamento do AWS SageMaker",
      "type": "guide",
      "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html",
      "description": "Auto-escalonamento de modelos ML com AWS SageMaker",
      "required": false
    },
    {
      "id": "ray-serve-scaling",
      "title": "Guia de Escalonamento do Ray Serve",
      "type": "tutorial",
      "url": "https://docs.ray.io/en/latest/serve/scaling-guide.html",
      "description": "Escalonando aplicações de IA com Ray Serve",
      "required": true
    },
    {
      "id": "nvidia-mig-guide",
      "title": "Guia de GPU Multi-Instância da NVIDIA",
      "type": "reference",
      "url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/",
      "description": "Particionamento de GPUs para melhor utilização",
      "required": false
    },
    {
      "id": "redis-ai-caching",
      "title": "Padrões de Cache de IA com Redis",
      "type": "tutorial",
      "url": "https://redis.io/docs/stack/ml/",
      "description": "Usando Redis para cache de modelo de IA e inferência",
      "required": true
    }
  ],
  "assessmentCriteria": {
    "minimumScore": 75,
    "requiredSections": ["scaling-challenges", "scaling-strategies", "load-balancing", "caching-strategies", "queue-architecture", "hands-on-exercise"],
    "timeTracking": true,
    "completionCertificate": false
  }
}