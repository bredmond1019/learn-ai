---
title: "Build a Research Agent"
description: "Capstone project that integrates all agentic workflow concepts to build a comprehensive research agent capable of autonomous research, analysis, and reporting with human oversight."
duration: "180 minutes"
difficulty: "advanced"
order: 5
pathId: "agentic-workflows"
moduleId: "build-research-agent"
type: "project"
objectives:
  - "Integrate agent architecture patterns, planning, tool orchestration, and human-in-the-loop design"
  - "Build a complete research agent with autonomous research capabilities"
  - "Implement sophisticated planning and goal management for research tasks"
  - "Create robust tool orchestration for web scraping, data analysis, and report generation"
  - "Design effective human oversight and quality control mechanisms"
  - "Deploy a functional research assistant with explainable decision-making"
prerequisites:
  - "Agent Architecture Patterns"
  - "Building Planning Systems"
  - "Tool Orchestration"
  - "Human-in-the-Loop Design"
  - "Experience with web APIs and data processing"
tags:
  - "research-automation"
  - "information-gathering"
  - "agent-integration"
  - "quality-control"
  - "report-generation"
  - "capstone-project"
version: "1.0.0"
lastUpdated: "2025-06-20"
author: "AI Engineering Team"
estimatedCompletionTime: 270
---

# Build a Research Agent

Welcome to the capstone project of the Agentic AI Workflows learning path! In this module, you'll integrate everything you've learned to build a sophisticated research agent capable of autonomous research, analysis, and reporting with appropriate human oversight. This project demonstrates how all the concepts—agent architectures, planning systems, tool orchestration, and human-in-the-loop design—work together in a real-world application.

## Learning Objectives

By the end of this module, you will:

- Integrate agent architecture patterns, planning, tool orchestration, and human-in-the-loop design
- Build a complete research agent with autonomous research capabilities
- Implement sophisticated planning and goal management for research tasks
- Create robust tool orchestration for web scraping, data analysis, and report generation
- Design effective human oversight and quality control mechanisms
- Deploy a functional research assistant with explainable decision-making

## Project Overview

Our research agent will be capable of:

1. **Autonomous Research**: Planning and executing comprehensive research on given topics
2. **Multi-Source Information Gathering**: Using web search, academic databases, and APIs
3. **Intelligent Analysis**: Processing and synthesizing information from multiple sources
4. **Quality Control**: Fact-checking and validating research findings
5. **Human Collaboration**: Involving humans at critical decision points
6. **Report Generation**: Creating structured, professional research deliverables
7. **Explainable Process**: Providing clear explanations of research methodology and findings

## Research Agent Architecture

### Core System Design

Our research agent follows a hybrid architecture combining reactive and deliberative approaches:

```python
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import asyncio
import time
import logging
import json
from datetime import datetime, timedelta

# Import all previous modules (simplified for demonstration)
# In practice, these would be in separate files
from agent_architecture import Agent, ReActAgent
from planning_systems import STRIPSPlanner, HTNPlanner, GoalManager
from tool_orchestration import ToolRegistry, ToolSelector, WorkflowOrchestrator
from human_in_loop import HumanAICoordinator, InterventionManager, ExplainableAgent

class ResearchPhase(Enum):
    PLANNING = "planning"
    INFORMATION_GATHERING = "information_gathering"
    ANALYSIS = "analysis"
    SYNTHESIS = "synthesis"
    QUALITY_CONTROL = "quality_control"
    REPORTING = "reporting"
    REVIEW = "review"

class ResearchTaskType(Enum):
    LITERATURE_REVIEW = "literature_review"
    MARKET_ANALYSIS = "market_analysis"
    COMPETITIVE_INTELLIGENCE = "competitive_intelligence"
    TECHNICAL_ANALYSIS = "technical_analysis"
    TREND_ANALYSIS = "trend_analysis"
    FACT_CHECKING = "fact_checking"

@dataclass
class ResearchTopic:
    """Represents a research topic with requirements and constraints"""
    topic_id: str
    title: str
    description: str
    research_type: ResearchTaskType
    scope: str  # "broad", "focused", "deep"
    deadline: datetime
    quality_requirements: Dict[str, Any]
    constraints: Dict[str, Any] = field(default_factory=dict)
    target_audience: str = "general"
    deliverable_format: str = "report"  # "report", "presentation", "summary"
    
class ResearchAgent:
    """Main research agent integrating all subsystems"""
    
    def __init__(self, agent_name: str = "ResearchAgent"):
        self.agent_name = agent_name
        
        # Core subsystems
        self.tool_registry = ToolRegistry()
        self.tool_selector = ToolSelector(self.tool_registry)
        self.orchestrator = WorkflowOrchestrator(self.tool_registry, self.tool_selector)
        self.planner = ResearchPlanner()
        self.quality_controller = QualityController()
        self.report_generator = ReportGenerator()
        self.intervention_manager = InterventionManager()
        self.explainable_agent = ExplainableAgent(agent_name)
        
        # Research state
        self.active_research: Dict[str, Any] = {}
        self.research_history: List[Dict[str, Any]] = []
        self.knowledge_base: Dict[str, Any] = {}
        
        # Initialize tools and capabilities
        self._setup_research_tools()
        self._setup_intervention_points()
        
    def _setup_research_tools(self):
        """Initialize research-specific tools"""
        
        # Web research tools
        web_search_tool = WebSearchTool()
        academic_search_tool = AcademicSearchTool()
        web_scraper_tool = WebScrapingTool()
        
        # Data processing tools
        text_analyzer_tool = TextAnalysisTool()
        data_processor_tool = DataProcessingTool()
        fact_checker_tool = FactCheckingTool()
        
        # Report generation tools
        summarizer_tool = SummarizerTool()
        visualizer_tool = VisualizationTool()
        document_generator_tool = DocumentGeneratorTool()
        
        # Register all tools
        tools = [
            web_search_tool, academic_search_tool, web_scraper_tool,
            text_analyzer_tool, data_processor_tool, fact_checker_tool,
            summarizer_tool, visualizer_tool, document_generator_tool
        ]
        
        for tool in tools:
            asyncio.create_task(self.tool_registry.register_tool(tool))
    
    def _setup_intervention_points(self):
        """Setup human intervention points for research quality"""
        
        # Low quality sources intervention
        low_quality_point = InterventionPoint(
            trigger_type=InterventionTrigger.LOW_CONFIDENCE,
            condition=lambda ctx: ctx.get("source_credibility", 1.0) < 0.6,
            urgency=InterventionUrgency.MEDIUM,
            description="Low credibility sources detected",
            escalation_path=["research_supervisor", "domain_expert"],
            timeout=1800.0,  # 30 minutes
            fallback_action="conservative",
            required_expertise=["research_methodology", "source_evaluation"]
        )
        
        # Conflicting information intervention
        conflict_point = InterventionPoint(
            trigger_type=InterventionTrigger.ERROR_DETECTION,
            condition=lambda ctx: ctx.get("information_conflicts", []),
            urgency=InterventionUrgency.HIGH,
            description="Conflicting information detected across sources",
            escalation_path=["fact_checker", "domain_expert", "research_supervisor"],
            timeout=3600.0,  # 1 hour
            fallback_action="defer",
            required_expertise=["fact_checking", "domain_knowledge"]
        )
        
        # Research scope expansion intervention
        scope_point = InterventionPoint(
            trigger_type=InterventionTrigger.STAKEHOLDER_REQUEST,
            condition=lambda ctx: ctx.get("scope_expansion_needed", False),
            urgency=InterventionUrgency.MEDIUM,
            description="Research scope may need expansion",
            escalation_path=["project_manager", "stakeholder"],
            timeout=7200.0,  # 2 hours
            fallback_action="proceed_as_planned",
            required_expertise=["project_management", "research_planning"]
        )
        
        self.intervention_manager.register_intervention_point(low_quality_point)
        self.intervention_manager.register_intervention_point(conflict_point)
        self.intervention_manager.register_intervention_point(scope_point)
        
        # Register human experts
        self.intervention_manager.register_human_expert(
            "research_supervisor", ["research_methodology", "quality_control"], True
        )
        self.intervention_manager.register_human_expert(
            "domain_expert", ["domain_knowledge", "source_evaluation"], True
        )
        self.intervention_manager.register_human_expert(
            "fact_checker", ["fact_checking", "verification"], True
        )
    
    async def conduct_research(self, research_topic: ResearchTopic) -> Dict[str, Any]:
        """Main method to conduct comprehensive research"""
        
        research_id = f"research_{int(time.time() * 1000)}"
        
        try:
            # Initialize research session
            research_session = {
                "research_id": research_id,
                "topic": research_topic,
                "start_time": datetime.now(),
                "current_phase": ResearchPhase.PLANNING,
                "findings": {},
                "sources": [],
                "quality_metrics": {},
                "interventions": [],
                "deliverables": {}
            }
            
            self.active_research[research_id] = research_session
            
            # Phase 1: Research Planning
            await self._planning_phase(research_session)
            
            # Phase 2: Information Gathering
            await self._information_gathering_phase(research_session)
            
            # Phase 3: Analysis and Synthesis
            await self._analysis_phase(research_session)
            
            # Phase 4: Quality Control
            await self._quality_control_phase(research_session)
            
            # Phase 5: Report Generation
            await self._reporting_phase(research_session)
            
            # Phase 6: Human Review
            await self._review_phase(research_session)
            
            # Finalize research
            research_session["end_time"] = datetime.now()
            research_session["total_duration"] = (
                research_session["end_time"] - research_session["start_time"]
            ).total_seconds()
            
            # Move to history
            self.research_history.append(research_session)
            if research_id in self.active_research:
                del self.active_research[research_id]
            
            logging.info(f"Research {research_id} completed successfully")
            
            return {
                "success": True,
                "research_id": research_id,
                "deliverables": research_session["deliverables"],
                "quality_metrics": research_session["quality_metrics"],
                "duration": research_session["total_duration"]
            }
            
        except Exception as e:
            logging.error(f"Research {research_id} failed: {e}")
            return {
                "success": False,
                "research_id": research_id,
                "error": str(e),
                "partial_results": research_session.get("findings", {})
            }
    
    async def _planning_phase(self, research_session: Dict[str, Any]):
        """Phase 1: Plan the research approach"""
        
        research_session["current_phase"] = ResearchPhase.PLANNING
        logging.info(f"Starting planning phase for {research_session['research_id']}")
        
        topic = research_session["topic"]
        
        # Create research plan
        research_plan = await self.planner.create_research_plan(topic)
        research_session["plan"] = research_plan
        
        # Set up goals and milestones
        goals = self.planner.extract_research_goals(topic, research_plan)
        research_session["goals"] = goals
        
        # Check for planning interventions
        planning_context = {
            "research_type": topic.research_type.value,
            "scope": topic.scope,
            "complexity": research_plan.get("complexity", "medium"),
            "estimated_duration": research_plan.get("estimated_duration", 0)
        }
        
        intervention = await self.intervention_manager.check_intervention_needed(planning_context)
        if intervention:
            research_session["interventions"].append(intervention)
        
        logging.info(f"Planning phase completed for {research_session['research_id']}")
    
    async def _information_gathering_phase(self, research_session: Dict[str, Any]):
        """Phase 2: Gather information from multiple sources"""
        
        research_session["current_phase"] = ResearchPhase.INFORMATION_GATHERING
        logging.info(f"Starting information gathering for {research_session['research_id']}")
        
        topic = research_session["topic"]
        plan = research_session["plan"]
        
        # Execute information gathering workflow
        gathering_workflow = self._create_information_gathering_workflow(topic, plan)
        workflow_result = await self.orchestrator.execute_workflow(gathering_workflow)
        
        # Process and store findings
        raw_information = self._extract_workflow_outputs(workflow_result)
        processed_information = await self._process_raw_information(raw_information)
        
        research_session["findings"]["raw_information"] = raw_information
        research_session["findings"]["processed_information"] = processed_information
        research_session["sources"] = self._extract_sources(raw_information)
        
        # Check information quality
        quality_context = {
            "source_count": len(research_session["sources"]),
            "source_credibility": self._calculate_average_credibility(research_session["sources"]),
            "information_coverage": self._assess_information_coverage(processed_information, topic)
        }
        
        intervention = await self.intervention_manager.check_intervention_needed(quality_context)
        if intervention:
            research_session["interventions"].append(intervention)
        
        logging.info(f"Information gathering completed for {research_session['research_id']}")
    
    async def _analysis_phase(self, research_session: Dict[str, Any]):
        """Phase 3: Analyze and synthesize information"""
        
        research_session["current_phase"] = ResearchPhase.ANALYSIS
        logging.info(f"Starting analysis phase for {research_session['research_id']}")
        
        processed_info = research_session["findings"]["processed_information"]
        topic = research_session["topic"]
        
        # Perform various types of analysis
        analysis_results = {}
        
        # Trend analysis
        if topic.research_type in [ResearchTaskType.MARKET_ANALYSIS, ResearchTaskType.TREND_ANALYSIS]:
            analysis_results["trends"] = await self._analyze_trends(processed_info)
        
        # Comparative analysis
        if topic.research_type == ResearchTaskType.COMPETITIVE_INTELLIGENCE:
            analysis_results["competitive_landscape"] = await self._analyze_competition(processed_info)
        
        # Technical analysis
        if topic.research_type == ResearchTaskType.TECHNICAL_ANALYSIS:
            analysis_results["technical_insights"] = await self._analyze_technical_aspects(processed_info)
        
        # General synthesis
        analysis_results["synthesis"] = await self._synthesize_information(processed_info, topic)
        
        # Identify key insights and conclusions
        analysis_results["key_insights"] = await self._extract_key_insights(analysis_results)
        analysis_results["conclusions"] = await self._draw_conclusions(analysis_results, topic)
        
        research_session["findings"]["analysis"] = analysis_results
        
        # Check for conflicting information
        conflicts = self._detect_information_conflicts(analysis_results)
        if conflicts:
            conflict_context = {
                "information_conflicts": conflicts,
                "conflict_severity": len(conflicts)
            }
            
            intervention = await self.intervention_manager.check_intervention_needed(conflict_context)
            if intervention:
                research_session["interventions"].append(intervention)
        
        logging.info(f"Analysis phase completed for {research_session['research_id']}")
    
    async def _quality_control_phase(self, research_session: Dict[str, Any]):
        """Phase 4: Quality control and validation"""
        
        research_session["current_phase"] = ResearchPhase.QUALITY_CONTROL
        logging.info(f"Starting quality control for {research_session['research_id']}")
        
        # Run comprehensive quality checks
        quality_results = await self.quality_controller.assess_research_quality(
            research_session["findings"],
            research_session["sources"],
            research_session["topic"]
        )
        
        research_session["quality_metrics"] = quality_results
        
        # Check if quality meets requirements
        topic = research_session["topic"]
        quality_requirements = topic.quality_requirements
        
        quality_issues = self._identify_quality_issues(quality_results, quality_requirements)
        
        if quality_issues:
            # Attempt to address quality issues
            improvements = await self._improve_research_quality(research_session, quality_issues)
            research_session["quality_improvements"] = improvements
        
        logging.info(f"Quality control completed for {research_session['research_id']}")
    
    async def _reporting_phase(self, research_session: Dict[str, Any]):
        """Phase 5: Generate research deliverables"""
        
        research_session["current_phase"] = ResearchPhase.REPORTING
        logging.info(f"Starting report generation for {research_session['research_id']}")
        
        topic = research_session["topic"]
        findings = research_session["findings"]
        
        # Generate deliverables based on requirements
        deliverables = {}
        
        if topic.deliverable_format in ["report", "comprehensive"]:
            report = await self.report_generator.generate_research_report(
                topic, findings, research_session["sources"]
            )
            deliverables["report"] = report
        
        if topic.deliverable_format in ["presentation", "comprehensive"]:
            presentation = await self.report_generator.generate_presentation(
                topic, findings, research_session["sources"]
            )
            deliverables["presentation"] = presentation
        
        if topic.deliverable_format in ["summary", "comprehensive"]:
            summary = await self.report_generator.generate_executive_summary(
                topic, findings, research_session["sources"]
            )
            deliverables["summary"] = summary
        
        # Generate visualizations if applicable
        if findings.get("analysis", {}).get("trends"):
            visualizations = await self.report_generator.generate_visualizations(
                findings["analysis"]
            )
            deliverables["visualizations"] = visualizations
        
        research_session["deliverables"] = deliverables
        
        logging.info(f"Report generation completed for {research_session['research_id']}")
    
    async def _review_phase(self, research_session: Dict[str, Any]):
        """Phase 6: Human review and final approval"""
        
        research_session["current_phase"] = ResearchPhase.REVIEW
        logging.info(f"Starting review phase for {research_session['research_id']}")
        
        # Prepare review package
        review_package = {
            "research_summary": self._create_research_summary(research_session),
            "quality_metrics": research_session["quality_metrics"],
            "deliverables": research_session["deliverables"],
            "methodology": research_session.get("plan", {}),
            "sources": research_session["sources"]
        }
        
        # Check if human review is required
        topic = research_session["topic"]
        quality_score = research_session["quality_metrics"].get("overall_score", 0.5)
        
        requires_review = (
            quality_score < 0.8 or
            topic.research_type in [ResearchTaskType.FACT_CHECKING] or
            len(research_session["interventions"]) > 0 or
            topic.quality_requirements.get("human_review_required", False)
        )
        
        if requires_review:
            # Request human review
            review_context = {
                "requires_human_review": True,
                "quality_score": quality_score,
                "research_complexity": topic.scope
            }
            
            intervention = await self.intervention_manager.check_intervention_needed(review_context)
            if intervention:
                research_session["interventions"].append(intervention)
                
                # Wait for human feedback
                # In practice, this would integrate with a real review system
                review_feedback = await self._simulate_human_review(review_package)
                research_session["review_feedback"] = review_feedback
                
                # Apply review feedback
                if review_feedback.get("requires_changes"):
                    await self._apply_review_feedback(research_session, review_feedback)
        
        logging.info(f"Review phase completed for {research_session['research_id']}")
    
    def get_research_status(self, research_id: str) -> Optional[Dict[str, Any]]:
        """Get current status of research project"""
        
        if research_id in self.active_research:
            session = self.active_research[research_id]
            return {
                "research_id": research_id,
                "status": "active",
                "current_phase": session["current_phase"].value,
                "progress": self._calculate_progress(session),
                "estimated_completion": self._estimate_completion_time(session),
                "quality_score": session.get("quality_metrics", {}).get("overall_score", 0),
                "intervention_count": len(session.get("interventions", []))
            }
        
        # Check completed research
        for session in self.research_history:
            if session["research_id"] == research_id:
                return {
                    "research_id": research_id,
                    "status": "completed",
                    "completion_time": session["end_time"],
                    "duration": session["total_duration"],
                    "quality_score": session.get("quality_metrics", {}).get("overall_score", 0),
                    "deliverables": list(session.get("deliverables", {}).keys())
                }
        
        return None
    
    def _calculate_progress(self, session: Dict[str, Any]) -> float:
        """Calculate research progress percentage"""
        phase_weights = {
            ResearchPhase.PLANNING: 0.15,
            ResearchPhase.INFORMATION_GATHERING: 0.30,
            ResearchPhase.ANALYSIS: 0.25,
            ResearchPhase.QUALITY_CONTROL: 0.15,
            ResearchPhase.REPORTING: 0.10,
            ResearchPhase.REVIEW: 0.05
        }
        
        current_phase = session["current_phase"]
        completed_weight = 0.0
        
        for phase, weight in phase_weights.items():
            if phase.value < current_phase.value:
                completed_weight += weight
            elif phase == current_phase:
                completed_weight += weight * 0.5  # Assume 50% completion of current phase
                break
        
        return min(1.0, completed_weight)

# Research-specific tool implementations
class WebSearchTool(Tool):
    """Enhanced web search tool for research"""
    
    def __init__(self):
        metadata = ToolMetadata(
            name="web_search",
            version="2.0.0",
            description="Advanced web search with credibility scoring",
            category=ToolCategory.WEB_SCRAPING,
            capability=ToolCapability(
                input_types=["query", "search_parameters"],
                output_types=["search_results", "credibility_scores"],
                required_params=["query"],
                optional_params=["num_results", "date_range", "source_filter"],
                max_concurrent_calls=3,
                average_execution_time=2.0
            )
        )
        super().__init__(metadata)
    
    async def execute(self, **kwargs) -> Any:
        """Execute advanced web search"""
        start_time = time.time()
        self.call_count += 1
        
        try:
            query = kwargs.get("query")
            num_results = kwargs.get("num_results", 10)
            date_range = kwargs.get("date_range", "any")
            
            # Simulate advanced search (in practice, use real search APIs)
            await asyncio.sleep(1.0)
            
            # Mock search results with credibility scoring
            results = []
            for i in range(num_results):
                result = {
                    "title": f"Search Result {i+1} for '{query}'",
                    "url": f"https://example{i+1}.com/article",
                    "snippet": f"Relevant information about {query}...",
                    "source": f"example{i+1}.com",
                    "date": "2024-01-15",
                    "credibility_score": 0.7 + (i % 3) * 0.1,  # Vary credibility
                    "relevance_score": 0.9 - (i * 0.05)
                }
                results.append(result)
            
            execution_time = time.time() - start_time
            self.total_execution_time += execution_time
            
            return {
                "query": query,
                "results": results,
                "total_results": len(results),
                "average_credibility": sum(r["credibility_score"] for r in results) / len(results),
                "execution_time": execution_time
            }
            
        except Exception as e:
            self.error_count += 1
            execution_time = time.time() - start_time
            self.total_execution_time += execution_time
            raise e
    
    async def health_check(self) -> bool:
        """Check if web search is available"""
        try:
            await asyncio.sleep(0.1)
            return True
        except:
            return False

# Additional research tools would be implemented similarly...
# For brevity, I'll include key methods in the main class

class ResearchPlanner:
    """Specialized planner for research tasks"""
    
    async def create_research_plan(self, topic: ResearchTopic) -> Dict[str, Any]:
        """Create comprehensive research plan"""
        
        plan = {
            "research_id": f"plan_{int(time.time())}",
            "topic": topic.title,
            "research_type": topic.research_type.value,
            "approach": self._determine_research_approach(topic),
            "phases": self._plan_research_phases(topic),
            "resources": self._identify_required_resources(topic),
            "timeline": self._create_timeline(topic),
            "quality_checkpoints": self._define_quality_checkpoints(topic),
            "risk_mitigation": self._identify_risks_and_mitigation(topic)
        }
        
        return plan
    
    def _determine_research_approach(self, topic: ResearchTopic) -> str:
        """Determine the best research approach for the topic"""
        
        if topic.research_type == ResearchTaskType.LITERATURE_REVIEW:
            return "systematic_review"
        elif topic.research_type == ResearchTaskType.MARKET_ANALYSIS:
            return "multi_source_analysis"
        elif topic.research_type == ResearchTaskType.COMPETITIVE_INTELLIGENCE:
            return "competitive_benchmarking"
        elif topic.research_type == ResearchTaskType.FACT_CHECKING:
            return "verification_focused"
        else:
            return "comprehensive_analysis"

class QualityController:
    """Research quality control and validation"""
    
    async def assess_research_quality(self, findings: Dict[str, Any], 
                                    sources: List[Dict[str, Any]], 
                                    topic: ResearchTopic) -> Dict[str, Any]:
        """Assess overall research quality"""
        
        quality_metrics = {}
        
        # Source quality assessment
        quality_metrics["source_quality"] = self._assess_source_quality(sources)
        
        # Information completeness
        quality_metrics["completeness"] = self._assess_completeness(findings, topic)
        
        # Consistency check
        quality_metrics["consistency"] = self._check_consistency(findings)
        
        # Objectivity assessment
        quality_metrics["objectivity"] = self._assess_objectivity(findings)
        
        # Currency/recency
        quality_metrics["currency"] = self._assess_currency(sources)
        
        # Calculate overall score
        weights = {
            "source_quality": 0.25,
            "completeness": 0.25,
            "consistency": 0.20,
            "objectivity": 0.15,
            "currency": 0.15
        }
        
        overall_score = sum(
            quality_metrics[metric] * weights[metric] 
            for metric in weights.keys()
        )
        
        quality_metrics["overall_score"] = overall_score
        quality_metrics["quality_grade"] = self._assign_quality_grade(overall_score)
        
        return quality_metrics
    
    def _assess_source_quality(self, sources: List[Dict[str, Any]]) -> float:
        """Assess the quality of research sources"""
        if not sources:
            return 0.0
        
        total_credibility = sum(source.get("credibility_score", 0.5) for source in sources)
        return total_credibility / len(sources)
    
    def _assign_quality_grade(self, score: float) -> str:
        """Assign quality grade based on score"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"

class ReportGenerator:
    """Generate research deliverables"""
    
    async def generate_research_report(self, topic: ResearchTopic, 
                                     findings: Dict[str, Any], 
                                     sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive research report"""
        
        report = {
            "title": f"Research Report: {topic.title}",
            "executive_summary": self._create_executive_summary(findings),
            "methodology": self._describe_methodology(topic),
            "findings": self._format_findings(findings),
            "analysis": self._format_analysis(findings.get("analysis", {})),
            "conclusions": self._format_conclusions(findings.get("analysis", {})),
            "recommendations": self._generate_recommendations(findings, topic),
            "sources": self._format_sources(sources),
            "appendices": self._create_appendices(findings),
            "metadata": {
                "generated_date": datetime.now().isoformat(),
                "research_type": topic.research_type.value,
                "scope": topic.scope,
                "target_audience": topic.target_audience
            }
        }
        
        return report
    
    def _create_executive_summary(self, findings: Dict[str, Any]) -> str:
        """Create executive summary of research"""
        
        summary_parts = []
        
        # Key insights
        if "analysis" in findings and "key_insights" in findings["analysis"]:
            insights = findings["analysis"]["key_insights"]
            summary_parts.append(f"Key Insights: {'; '.join(insights[:3])}")
        
        # Main conclusions
        if "analysis" in findings and "conclusions" in findings["analysis"]:
            conclusions = findings["analysis"]["conclusions"]
            summary_parts.append(f"Conclusions: {'; '.join(conclusions[:2])}")
        
        # Data coverage
        info_count = len(findings.get("processed_information", []))
        summary_parts.append(f"Based on analysis of {info_count} information sources")
        
        return ". ".join(summary_parts) + "."

# Example usage and demonstration
async def demonstrate_research_agent():
    """Demonstrate the complete research agent"""
    
    # Create research agent
    agent = ResearchAgent("AdvancedResearchAgent")
    
    # Define research topic
    research_topic = ResearchTopic(
        topic_id="ai_trends_2024",
        title="AI Technology Trends in 2024",
        description="Comprehensive analysis of emerging AI technologies, market adoption, and future predictions for 2024",
        research_type=ResearchTaskType.TREND_ANALYSIS,
        scope="broad",
        deadline=datetime.now() + timedelta(hours=4),
        quality_requirements={
            "minimum_sources": 10,
            "credibility_threshold": 0.7,
            "human_review_required": True
        },
        target_audience="technology_executives",
        deliverable_format="comprehensive"
    )
    
    print(f"Starting research on: {research_topic.title}")
    
    # Conduct research
    result = await agent.conduct_research(research_topic)
    
    if result["success"]:
        print(f"Research completed successfully!")
        print(f"Research ID: {result['research_id']}")
        print(f"Duration: {result['duration']:.2f} seconds")
        print(f"Quality Score: {result['quality_metrics'].get('overall_score', 0):.2f}")
        print(f"Deliverables: {list(result['deliverables'].keys())}")
    else:
        print(f"Research failed: {result['error']}")
    
    # Get research status
    status = agent.get_research_status(result["research_id"])
    print(f"Final Status: {status}")

# Run the demonstration
# await demonstrate_research_agent()
```

## Interactive Quiz

Test your understanding of building research agents:

### Question 1
Which architecture pattern is most suitable for a research agent that needs both reactive responses and complex planning?

A) Purely reactive
B) Purely deliberative
C) Hybrid architecture
D) Multi-agent system

**Answer: C) Hybrid architecture**

Research agents need reactive capabilities for immediate responses and deliberative planning for complex research strategies, making hybrid architecture ideal.

### Question 2
What is the most critical intervention point for research quality?

A) Planning phase approval
B) Information source credibility validation
C) Report formatting review
D) Timeline management

**Answer: B) Information source credibility validation**

Source credibility directly impacts research quality and reliability, making it the most critical intervention point for maintaining research integrity.

### Question 3
Which quality metric is most important for research deliverables?

A) Completeness of information coverage
B) Speed of research completion
C) Number of sources consulted
D) Length of final report

**Answer: A) Completeness of information coverage**

Completeness ensures that research adequately addresses the topic scope and requirements, providing comprehensive and reliable insights.

## Practical Exercises

### Exercise 1: Basic Research Agent Implementation
**Time: 60 minutes**

Build a simplified research agent with:
1. Basic planning capabilities for research tasks
2. Simple web search and information gathering
3. Basic quality assessment of sources
4. Simple report generation

### Exercise 2: Advanced Research Features
**Time: 90 minutes**

Enhance your research agent with:
1. Multi-source information orchestration
2. Conflict detection and resolution
3. Human intervention points
4. Explainable research methodology
5. Quality control mechanisms

### Exercise 3: Complete Research System Deployment
**Time: 120 minutes**

Deploy a production-ready research agent featuring:
1. Full workflow orchestration
2. Comprehensive quality control
3. Multiple deliverable formats
4. Human review integration
5. Performance monitoring and analytics
6. User interface for research requests

## Summary

Building a research agent integrates all the concepts from this learning path:

- **Agent Architecture**: Hybrid design combining reactive and deliberative approaches
- **Planning Systems**: Specialized research planning with goal management
- **Tool Orchestration**: Coordinating multiple information gathering and analysis tools
- **Human-in-the-Loop**: Strategic intervention points for quality control and oversight
- **Quality Assurance**: Comprehensive validation and fact-checking mechanisms
- **Explainable AI**: Transparent research methodology and decision-making

This capstone project demonstrates how sophisticated agentic systems can augment human capabilities in complex domains like research, providing both automation and appropriate human oversight.

## Congratulations!

You have successfully completed the Agentic AI Workflows learning path! You now have the knowledge and skills to:

- Design and implement sophisticated AI agent architectures
- Build planning systems for complex, multi-step tasks
- Orchestrate multiple tools and services effectively
- Integrate human expertise and oversight appropriately
- Create explainable and trustworthy AI systems

These skills form the foundation for building production-ready agentic AI systems that can handle real-world complexity while maintaining reliability, transparency, and human control.

## Next Steps

Continue your AI engineering journey with the **Production AI Systems** learning path, where you'll learn to deploy, monitor, scale, and secure AI systems in production environments.

---

## Additional Resources

- [Research Methodology Best Practices](https://libguides.usc.edu/writingguide/researchprocess)
- [Information Quality Assessment](https://guides.library.harvard.edu/c.php?g=310264&p=2071003)
- [Automated Research Tools](https://www.semanticscholar.org/)
- [AI Agent Development Patterns](https://www.oreilly.com/library/view/building-intelligent-agents/)
- [Research Ethics and AI](https://www.ieee.org/about/corporate/governance/p7000.html)