# MCP for Python

This article covers how to integrate the Model Context Protocol (MCP) into Python applications for building production-ready AI systems and agents. The course goes beyond simple desktop integrations to show how to build real AI backends using MCP.

**What You'll Learn:**

- Understanding MCP fundamentals and architecture
- Setting up MCP servers using Python SDK
- Integrating MCP servers into Python applications
- Working with both Standard I/O and HTTP transport mechanisms
- Building practical AI systems with OpenAI integration
- Deploying MCP servers with Docker
- Managing server lifecycles in production

## Chapter 1: Introduction and Context

### What is MCP?

**Model Context Protocol (MCP)** is a standardization framework developed by Anthropic (released November 25, 2024) that provides a unified way to connect AI assistants to external systems where data lives, including content repositories, business tools, and development environments.

### The Problem MCP Solves

**Before MCP:**

- Every developer created custom API layers for external integrations (Slack, Google Drive, GitHub)
- Each integration required custom function definitions and schemas
- No standardization across different tools and services
- Constant reinvention of the wheel for common integrations

**After MCP:**

- Unified protocol layer that standardizes tool and resource definitions
- Consistent API across all integrations
- Reusable servers that can be shared across projects
- Standardized schemas, functions, documentation, and arguments

### The MCP Adoption Surge

**Key Timeline:**

- November 2024: MCP released by Anthropic
- February 2025: Major adoption surge begins
- Present: Exponential growth in usage and community adoption

**What Changed:**

- Recognition of MCP as a solid, lightweight protocol
- Hundreds of officially supported servers by major tech companies
- OpenAI (Anthropic's biggest competitor) added MCP support to their Agent SDK
- GitHub star growth surpassing other popular AI frameworks

### Important Distinction

**MCP doesn't introduce new LLM functionality** - it's a standardized way of making tools and resources available to LLMs. Everything possible with MCP was already achievable through custom implementations, but MCP provides standardization and reusability.

## Chapter 2: Understanding MCP at a Technical Level

### Core Architecture Components

**Hosts**

- Programs that want to access data through MCP protocol
- Examples: Claude Desktop, IDEs, custom Python applications
- Contains MCP clients for server communication

**MCP Clients**

- Protocol clients that maintain one-to-one connections with servers
- Handle communication between hosts and servers
- Manage session lifecycle and tool invocation

**MCP Servers**

- Lightweight programs exposing specific capabilities
- Can provide: tools, resources, and prompts
- Connect to local data sources or remote services via APIs

### Transport Mechanisms

**Standard I/O**

- Used for local development on same machine
- Communication via pipes and standard input/output
- Simpler setup but limited to local usage
- Good for desktop applications like Claude Desktop

**Server-Sent Events (SSE) over HTTP**

- Used for distributed systems and remote servers
- Communication via HTTP requests and JSON-RPC
- Enables true client-server architecture
- Required for production deployments and shared resources

### Key Developer Considerations

**Local vs Remote Development:**

- Standard I/O: Everything on same machine, simpler but limited
- HTTP/SSE: Servers can be deployed remotely, accessible via API
- Remote deployment enables sharing tools across multiple applications
- Multiple servers and clients can be connected simultaneously

## Chapter 3: Simple Server Setup

### Basic Server Implementation

**Installation Requirements:**

```bash
pip install mcp

```

**Minimal Server Code (31 lines):**

```python
from mcp import FastMCP

# Create MCP server instance
mcp = FastMCP(
    name="simple-calculator",
    host="localhost",
    port=8050
)

# Define tools using decorators
@mcp.tool
def add(a: int, b: int) -> int:
    """Add two numbers together."""
    return a + b

# Run server with transport mechanism
if __name__ == "__main__":
    transport = "stdio"  # or "sse"

    if transport == "stdio":
        mcp.run(transport="stdio")
    else:
        mcp.run_sse()

```

### Development and Testing

**MCP Inspector:**

- Built-in tool for testing and debugging servers
- Command: `mcp dev server.py`
- Provides web interface for testing tools, resources, and prompts
- Essential for development and debugging

**Server Capabilities:**

- **Tools**: Python functions exposed as callable tools
- **Resources**: Local files and data sources
- **Prompts**: Reusable prompt templates

## Chapter 4: Client Implementation

### Standard I/O Client

**Basic Client Setup:**

```python
import asyncio
from mcp import stdio_client

async def main():
    # Define server parameters
    server_params = {
        "command": "python",
        "args": ["server.py"]
    }

    # Create session and connect
    async with stdio_client(server_params) as session:
        # List available tools
        tools = await session.list_tools()
        print("Available tools:", tools)

        # Call a tool
        result = await session.call_tool("add", {"a": 3, "b": 3})
        print("Result:", result)

# For interactive sessions
import nest_asyncio
nest_asyncio.apply()
asyncio.run(main())

```

### HTTP/SSE Client

**Remote Server Connection:**

```python
from mcp import sse_client

async def main():
    # Connect to remote server
    async with sse_client("http://localhost:8050") as session:
        # Same operations as stdio client
        tools = await session.list_tools()
        result = await session.call_tool("add", {"a": 3, "b": 3})

# Note: Server must be running separately
# python server.py (in another terminal)

```

## Chapter 5: OpenAI Integration

### Advanced Server with Knowledge Base

**Enhanced Server Example:**

```python
import json
from mcp import FastMCP

mcp = FastMCP(name="knowledge-server")

@mcp.tool
def get_knowledge_base() -> str:
    """Retrieve company knowledge base for RAG."""
    with open("data/knowledge.json", "r") as f:
        data = json.load(f)

    # Format for LLM consumption
    formatted = ""
    for item in data:
        formatted += f"Q: {item['question']}\nA: {item['answer']}\n\n"

    return formatted

if __name__ == "__main__":
    mcp.run(transport="stdio")

```

### OpenAI Client Integration

**Complete AI System Implementation:**

```python
import asyncio
from openai import OpenAI
from mcp import stdio_client

class MCPOpenAIClient:
    def __init__(self, model="gpt-4"):
        self.openai_client = OpenAI()
        self.model = model
        self.session = None

    async def connect_to_server(self):
        """Establish connection to MCP server."""
        server_params = {
            "command": "python",
            "args": ["server.py"]
        }
        self.session = stdio_client(server_params)
        await self.session.__aenter__()

    async def get_mcp_tools(self):
        """Convert MCP tools to OpenAI format."""
        tools = await self.session.list_tools()
        openai_tools = []

        for tool in tools:
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            }
            openai_tools.append(openai_tool)

        return openai_tools

    async def process_query(self, query: str):
        """Process user query with MCP tools integration."""
        # Get available tools
        tools = await self.get_mcp_tools()

        # Initial API call
        messages = [{"role": "user", "content": query}]
        response = self.openai_client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )

        # Handle tool calls
        assistant_message = response.choices[0].message
        if assistant_message.tool_calls:
            messages.append(assistant_message)

            # Execute tool calls via MCP
            for tool_call in assistant_message.tool_calls:
                result = await self.session.call_tool(
                    tool_call.function.name,
                    json.loads(tool_call.function.arguments)
                )

                # Add tool result to context
                messages.append({
                    "role": "tool",
                    "content": str(result),
                    "tool_call_id": tool_call.id
                })

            # Final API call with tool results
            final_response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages
            )
            return final_response.choices[0].message.content

        return assistant_message.content

# Usage example
async def main():
    client = MCPOpenAIClient()
    await client.connect_to_server()

    result = await client.process_query(
        "What is our company's vacation policy?"
    )
    print(result)

asyncio.run(main())

```

### Tool Calling Flow

**Step-by-Step Process:**

1. **Query Input**: User asks question
2. **Tool Discovery**: System lists available MCP tools
3. **Schema Conversion**: Convert MCP tools to OpenAI format
4. **Initial LLM Call**: Send query + tool definitions to OpenAI
5. **Tool Decision**: LLM decides whether to use tools
6. **Tool Execution**: If tools needed, execute via MCP session
7. **Context Building**: Add tool results to message context
8. **Final Response**: LLM synthesizes final answer with all context

## Chapter 6: MCP vs Function Calling

### Direct Comparison

**Traditional Function Calling:**

```python
# Tools defined directly in application
def get_knowledge_base():
    """Direct function implementation."""
    return load_knowledge_data()

# Direct integration with OpenAI
tools = [{
    "type": "function",
    "function": {
        "name": "get_knowledge_base",
        "description": "Get knowledge base data"
    }
}]

```

**MCP Approach:**

```python
# Tools defined in separate MCP server
# Retrieved dynamically via MCP protocol
tools = await session.list_tools()  # From MCP server
openai_tools = convert_to_openai_format(tools)

```

### When to Use Each Approach

**Stick with Function Calling When:**

- Simple applications with few tools
- Tools are specific to single application
- No need for tool sharing across projects
- Existing implementation works well

**Consider MCP When:**

- Building multiple AI applications
- Need to share tools across projects
- Want to leverage existing MCP servers
- Planning distributed architecture
- Building enterprise-scale systems

### Migration Considerations

**Key Insight**: There's no immediate need to migrate existing function calling implementations to MCP. MCP adds standardization and reusability but doesn't provide new capabilities that weren't already possible.

## Chapter 7: Docker Deployment

### Containerizing MCP Servers

**Dockerfile Example:**

```
FROM python:3.11-slim

WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy server code
COPY server.py .
COPY data/ ./data/

# Expose port for HTTP transport
EXPOSE 8050

# Run server
CMD ["python", "server.py"]

```

**Build and Run Commands:**

```bash
# Build Docker image
docker build -t mcp-server .

# Run container
docker run -p 8050:8050 mcp-server

```

### Production Deployment Benefits

**Advantages of Docker Deployment:**

- **Portability**: Run on any cloud provider or server
- **Scalability**: Easy horizontal scaling
- **Isolation**: Clean environment separation
- **Consistency**: Same environment across dev/staging/production
- **Resource Sharing**: Multiple applications can connect to same server

**Deployment Options:**

- Virtual machines on AWS/Azure/GCP
- Managed container services (ECS, AKS, GKE)
- Kubernetes clusters for enterprise deployments
- Docker Compose for multi-server setups

## Chapter 8: Lifecycle Management

### Connection Management

**Basic Session Handling:**

```python
# Using context managers for proper cleanup
async with stdio_client(server_params) as session:
    # Session automatically managed
    tools = await session.list_tools()
    # Session closed automatically on exit

```

**Advanced Lifecycle Management:**

```python
from mcp import FastMCP
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(server):
    """Manage server lifecycle events."""
    # Startup
    print("Server starting...")
    database = await connect_to_database()
    server.state.database = database

    yield

    # Shutdown
    print("Server shutting down...")
    await database.close()

# Create server with lifecycle management
mcp = FastMCP(name="production-server", lifespan=lifespan)

```

### Production Considerations

**Important for Enterprise Applications:**

- **Database Connections**: Proper connection pooling and cleanup
- **External Service Integration**: Graceful handling of API connections
- **Error Recovery**: Robust error handling and retry mechanisms
- **Monitoring**: Health checks and performance monitoring
- **Security**: Authentication and authorization for tool access

## Key Takeaways

### Core Benefits of MCP

1. **Standardization**: Unified protocol for tool and resource integration
2. **Reusability**: Share tools across multiple projects and applications
3. **Discoverability**: Dynamic discovery of available capabilities
4. **Composability**: Servers can be clients of other servers
5. **Ecosystem Growth**: Rapidly expanding collection of pre-built servers

### Best Practices

1. **Start Simple**: Begin with Standard I/O for local development
2. **Plan for Scale**: Consider HTTP transport for production systems
3. **Leverage Existing Servers**: Use community-built servers when possible
4. **Focus on Tools**: Tools provide the most immediate value over resources/prompts
5. **Proper Lifecycle Management**: Essential for production deployments

### When MCP Makes Sense

**Ideal Use Cases:**

- Building multiple AI applications that need shared functionality
- Enterprise environments with distributed teams
- Applications requiring integration with many external services
- Systems that benefit from standardized tool definitions
- Projects planning to leverage the growing MCP ecosystem

**When to Avoid:**

- Simple applications with few, specific tools
- Proof-of-concepts or prototypes
- Existing systems working well with function calling
- Applications where added complexity isn't justified

### Future Outlook

MCP represents a significant shift toward standardization in the AI tooling ecosystem. While it doesn't provide new capabilities, it offers substantial benefits for building scalable, maintainable AI systems. The rapid adoption rate and ecosystem growth suggest MCP will become a standard part of professional AI development workflows.

**Next Steps:**

- Experiment with the provided examples
- Build custom servers for your specific use cases
- Explore the growing ecosystem of community servers
- Consider MCP integration for new AI projects
- Stay updated with the evolving MCP specification and tooling