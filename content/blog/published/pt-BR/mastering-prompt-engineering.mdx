---
title: "Dominando Engenharia de Prompts: Do B√°sico √†s T√©cnicas Avan√ßadas"
date: "2024-03-20"
excerpt: "Explore t√©cnicas avan√ßadas de engenharia de prompts que podem melhorar drasticamente suas aplica√ß√µes LLM, com exemplos pr√°ticos e insights do mundo real."
tags: ["LLM", "Engenharia de Prompts", "Engenharia de IA"]
author: "Brandon"
---

A engenharia de prompts evoluiu de uma habilidade desej√°vel para uma compet√™ncia essencial para engenheiros de IA. Como algu√©m que projetou prompts para sistemas de produ√ß√£o processando milh√µes de requests, aprendi que a diferen√ßa entre uma aplica√ß√£o LLM med√≠ocre e excepcional frequentemente reside na qualidade do design de prompts. Deixe-me compartilhar o que aprendi.

## A Ci√™ncia Por Tr√°s de Prompts Eficazes

Entender como LLMs processam prompts √© crucial para criar prompts eficazes. Esses modelos n√£o "entendem" no sentido humano‚Äîeles predizem a continua√ß√£o mais prov√°vel baseada em padr√µes aprendidos durante o treinamento. Este insight molda como devemos abordar o design de prompts.

### A Anatomia de um Grande Prompt

Todo prompt eficaz cont√©m estes elementos:

1. **Defini√ß√£o de Contexto**: Estabelecer o cen√°rio e restri√ß√µes
2. **Instru√ß√µes Claras**: Especificar exatamente o que voc√™ quer
3. **Especifica√ß√£o de Formato**: Definir a estrutura de sa√≠da esperada
4. **Exemplos**: Fornecer ilustra√ß√µes concretas quando necess√°rio
5. **Tratamento de Casos Extremos**: Abordar potenciais ambiguidades

## T√©cnicas Avan√ßadas de Prompting

### 1. Chain-of-Thought (CoT) Prompting

```python
def create_cot_prompt(problem):
    """
    Cria um prompt de cadeia de pensamento que guia o modelo atrav√©s
    do racioc√≠nio passo-a-passo.
    """
    return f"""
Resolva o seguinte problema passo-a-passo:

Problema: `{problem}`

Pense atrav√©s deste problema sistematicamente:

1. Primeiro, identifique o que est√° sendo perguntado
2. Determine que informa√ß√µes voc√™ tem
3. Identifique que informa√ß√µes adicionais voc√™ precisa
4. Trabalhe atrav√©s da solu√ß√£o passo-a-passo
5. Verifique sua resposta

Solu√ß√£o:
"""

# Exemplo de uso
problem = "Uma empresa tem 150 funcion√°rios. Se 30% trabalham remotamente e 40% dos trabalhadores remotos s√£o engenheiros de software, quantos engenheiros de software trabalham remotamente?"

cot_prompt = create_cot_prompt(problem)
print(cot_prompt)
```

### 2. Few-Shot Learning com Exemplos Estrat√©gicos

```python
def create_few_shot_prompt(task_description, examples, new_input):
    """
    Cria um prompt few-shot com exemplos cuidadosamente selecionados.
    """
    prompt = f"Tarefa: {task_description}\n\n"
    
    for i, example in enumerate(examples, 1):
        prompt += f"Exemplo {i}:\n"
        prompt += f"Entrada: {example['input']}\n"
        prompt += f"Sa√≠da: {example['output']}\n\n"
    
    prompt += f"Agora complete:\n"
    prompt += f"Entrada: {new_input}\n"
    prompt += f"Sa√≠da:"
    
    return prompt

# Exemplo para classifica√ß√£o de sentimento
sentiment_examples = [
    {
        "input": "Este produto √© incr√≠vel! Funciona exatamente como anunciado.",
        "output": "Positivo"
    },
    {
        "input": "Terr√≠vel qualidade, quebrou ap√≥s uma semana.",
        "output": "Negativo"
    },
    {
        "input": "√â um produto ok, nada de especial mas funciona.",
        "output": "Neutro"
    }
]

sentiment_prompt = create_few_shot_prompt(
    "Classifique o sentimento do texto como Positivo, Negativo, ou Neutro",
    sentiment_examples,
    "O produto chegou r√°pido mas a qualidade poderia ser melhor."
)
```

### 3. Prompting Hier√°rquico para Tarefas Complexas

```python
class HierarchicalPrompt:
    """
    Quebra tarefas complexas em sub-tarefas gerenci√°veis.
    """
    
    def __init__(self, main_task):
        self.main_task = main_task
        self.subtasks = []
        self.results = {}
    
    def add_subtask(self, name, prompt_template):
        """Adiciona uma sub-tarefa ao fluxo hier√°rquico."""
        self.subtasks.append({
            "name": name,
            "prompt": prompt_template
        })
    
    def execute_subtask(self, subtask_name, **kwargs):
        """Executa uma sub-tarefa espec√≠fica."""
        subtask = next(s for s in self.subtasks if s["name"] == subtask_name)
        return subtask["prompt"].format(**kwargs)
    
    def create_synthesis_prompt(self):
        """Cria prompt para sintetizar resultados de sub-tarefas."""
        return f"""
Tarefa Principal: {self.main_task}

Resultados das Sub-tarefas:
{chr(10).join([f"- {name}: {result}" for name, result in self.results.items()])}

Com base nos resultados acima, forne√ßa uma s√≠ntese abrangente que:
1. Integre todas as descobertas
2. Identifique padr√µes ou contradi√ß√µes
3. Forne√ßa uma conclus√£o clara
4. Sugira pr√≥ximos passos

S√≠ntese:
"""

# Exemplo: An√°lise de mercado
market_analysis = HierarchicalPrompt("An√°lise abrangente de mercado para produto SaaS de IA")

market_analysis.add_subtask("competitor_analysis", """
Analise os seguintes competidores: `{competitors}`

Para cada competidor, identifique:
1. Pontos fortes principais
2. Fraquezas evidentes  
3. Estrat√©gia de pre√ßos
4. P√∫blico-alvo
5. Diferenciadores √∫nicos

An√°lise de Competidores:
""")

market_analysis.add_subtask("market_sizing", """
Estime o tamanho do mercado para `{product_category}` considerando:

1. Mercado Total Endere√ß√°vel (TAM)
2. Mercado Endere√ß√°vel Servic√≠vel (SAM)  
3. Mercado Obt√≠vel Servic√≠vel (SOM)
4. Taxa de crescimento projetada
5. Fatores que influenciam o crescimento

Dimensionamento de Mercado:
""")
```

### 4. Prompt Templates Reutiliz√°veis

```python
class PromptTemplate:
    """
    Sistema de templates para prompts consistentes e reutiliz√°veis.
    """
    
    def __init__(self, name, template, required_vars=None, optional_vars=None):
        self.name = name
        self.template = template
        self.required_vars = required_vars or []
        self.optional_vars = optional_vars or []
    
    def render(self, **kwargs):
        """Renderiza o template com as vari√°veis fornecidas."""
        # Verificar vari√°veis obrigat√≥rias
        missing_vars = [var for var in self.required_vars if var not in kwargs]
        if missing_vars:
            raise ValueError(f"Vari√°veis obrigat√≥rias ausentes: {missing_vars}")
        
        # Definir padr√µes para vari√°veis opcionais
        for var in self.optional_vars:
            if var not in kwargs:
                kwargs[var] = ""
        
        return self.template.format(**kwargs)
    
    def validate_output(self, output, validation_rules=None):
        """Valida a sa√≠da baseada em regras definidas."""
        if not validation_rules:
            return True
        
        for rule in validation_rules:
            if not rule(output):
                return False
        return True

# Templates pr√©-definidos
TEMPLATES = {
    "code_review": PromptTemplate(
        name="code_review",
        template="""
Voc√™ √© um engenheiro s√™nior revisando c√≥digo. Analise o seguinte c√≥digo:

```{language}
{code}
```

Forne√ßa uma revis√£o abrangente incluindo:

1. **Qualidade do C√≥digo (1-10)**: `{quality_focus}`
2. **Problemas Identificados**: 
   - Bugs potenciais
   - Problemas de performance
   - Quest√µes de seguran√ßa
   - Viola√ß√µes de melhores pr√°ticas

3. **Sugest√µes de Melhoria**:
   - Refatora√ß√µes espec√≠ficas
   - Otimiza√ß√µes de performance  
   - Melhorias de legibilidade

4. **C√≥digo Melhorado**: Forne√ßa uma vers√£o melhorada quando aplic√°vel

`{additional_instructions}`

Revis√£o:
""",
        required_vars=["language", "code"],
        optional_vars=["quality_focus", "additional_instructions"]
    ),
    
    "documentation_generator": PromptTemplate(
        name="documentation_generator", 
        template="""
Gere documenta√ß√£o t√©cnica para o seguinte `{doc_type}`:

`{content}`

A documenta√ß√£o deve incluir:

1. **Vis√£o Geral**: Prop√≥sito e funcionalidade principal
2. **Instala√ß√£o/Setup**: `{setup_requirements}`
3. **Uso B√°sico**: Exemplos pr√°ticos
4. **Par√¢metros/Configura√ß√£o**: Detalhes t√©cnicos
5. **Exemplos Avan√ßados**: Casos de uso complexos
6. **Troubleshooting**: Problemas comuns e solu√ß√µes
7. **Refer√™ncia API**: `{api_details}`

Formato: `{output_format}`

Documenta√ß√£o:
""",
        required_vars=["doc_type", "content"],
        optional_vars=["setup_requirements", "api_details", "output_format"]
    )
}

# Uso dos templates
def review_code(code, language="python"):
    template = TEMPLATES["code_review"]
    return template.render(
        code=code,
        language=language,
        quality_focus="Focar em manutenibilidade e performance",
        additional_instructions="Priorize sugest√µes que reduzam complexidade."
    )
```

### 5. Prompting Adaptativo com Feedback Loop

```python
class AdaptivePrompting:
    """
    Sistema que melhora prompts baseado em feedback e resultados.
    """
    
    def __init__(self):
        self.prompt_history = []
        self.performance_scores = []
        self.best_patterns = {}
    
    def execute_with_feedback(self, base_prompt, llm_client, feedback_fn):
        """
        Executa prompt e coleta feedback para melhorias futuras.
        """
        variations = self.generate_prompt_variations(base_prompt)
        best_result = None
        best_score = 0
        
        for variation in variations:
            result = llm_client.generate(variation)
            score = feedback_fn(result)
            
            if score > best_score:
                best_score = score
                best_result = result
                
            # Armazenar para an√°lise
            self.prompt_history.append({
                "prompt": variation,
                "result": result,
                "score": score
            })
        
        self.analyze_patterns()
        return best_result
    
    def generate_prompt_variations(self, base_prompt):
        """Gera varia√ß√µes do prompt base."""
        variations = [base_prompt]
        
        # Varia√ß√£o com mais contexto
        variations.append(f"Contexto adicional relevante:\n\n{base_prompt}")
        
        # Varia√ß√£o com exemplos
        variations.append(f"{base_prompt}\n\nExemplo de resposta de alta qualidade:\n[exemplo aqui]")
        
        # Varia√ß√£o com restri√ß√µes espec√≠ficas
        variations.append(f"{base_prompt}\n\nRestri√ß√µes importantes:\n- Seja espec√≠fico e acion√°vel\n- Use exemplos concretos\n- Mantenha resposta focada")
        
        return variations
    
    def analyze_patterns(self):
        """Analisa padr√µes em prompts de alta performance."""
        high_performance = [h for h in self.prompt_history if h["score"] > 0.8]
        
        if len(high_performance) > 5:
            # Identificar padr√µes comuns
            common_phrases = self.extract_common_patterns(high_performance)
            self.best_patterns.update(common_phrases)
    
    def extract_common_patterns(self, high_performance_prompts):
        """Extrai padr√µes comuns de prompts de alta performance."""
        # Implementa√ß√£o simplificada - na pr√°tica, usaria NLP mais avan√ßado
        patterns = {}
        
        for prompt_data in high_performance_prompts:
            prompt = prompt_data["prompt"]
            # Analisar estrutura, palavras-chave, etc.
            if "passo-a-passo" in prompt.lower():
                patterns["step_by_step"] = patterns.get("step_by_step", 0) + 1
            if "exemplo" in prompt.lower():
                patterns["examples"] = patterns.get("examples", 0) + 1
                
        return patterns
```

## üéØ Estrat√©gias Avan√ßadas para Produ√ß√£o

### 1. Sistema de Valida√ß√£o de Prompts

```python
class PromptValidator:
    """
    Valida qualidade e efic√°cia de prompts antes da produ√ß√£o.
    """
    
    def __init__(self):
        self.validation_rules = [
            self.check_clarity,
            self.check_specificity, 
            self.check_completeness,
            self.check_consistency
        ]
    
    def validate_prompt(self, prompt):
        """Executa todas as valida√ß√µes em um prompt."""
        results = {}
        overall_score = 0
        
        for rule in self.validation_rules:
            score, feedback = rule(prompt)
            results[rule.__name__] = {
                "score": score,
                "feedback": feedback
            }
            overall_score += score
        
        overall_score /= len(self.validation_rules)
        
        return {
            "overall_score": overall_score,
            "detailed_results": results,
            "recommendation": self.get_recommendation(overall_score)
        }
    
    def check_clarity(self, prompt):
        """Verifica clareza das instru√ß√µes."""
        clarity_keywords = ["claramente", "especificamente", "exatamente"]
        ambiguous_words = ["talvez", "possivelmente", "aproximadamente"]
        
        clarity_score = sum(1 for word in clarity_keywords if word in prompt.lower())
        ambiguity_penalty = sum(1 for word in ambiguous_words if word in prompt.lower())
        
        score = max(0, min(1, (clarity_score - ambiguity_penalty) / 3))
        
        feedback = []
        if score < 0.5:
            feedback.append("Considere usar linguagem mais espec√≠fica")
        if ambiguity_penalty > 0:
            feedback.append("Evite palavras amb√≠guas")
            
        return score, feedback
    
    def check_specificity(self, prompt):
        """Verifica se o prompt √© suficientemente espec√≠fico."""
        specific_indicators = ["formato:", "exemplo:", "passos:", "incluir:"]
        specificity_score = sum(1 for indicator in specific_indicators if indicator in prompt.lower())
        
        score = min(1, specificity_score / 2)
        
        feedback = []
        if score < 0.5:
            feedback.append("Adicione mais detalhes sobre formato esperado")
            feedback.append("Considere incluir exemplos")
            
        return score, feedback
    
    def check_completeness(self, prompt):
        """Verifica se o prompt cobre todos os aspectos necess√°rios."""
        essential_elements = ["contexto", "tarefa", "formato", "restri√ß√µes"]
        present_elements = sum(1 for element in essential_elements if element in prompt.lower())
        
        score = present_elements / len(essential_elements)
        
        feedback = []
        if score < 0.75:
            missing = [elem for elem in essential_elements if elem not in prompt.lower()]
            feedback.append(f"Considere adicionar: {', '.join(missing)}")
            
        return score, feedback
    
    def check_consistency(self, prompt):
        """Verifica consist√™ncia interna do prompt."""
        # Verifica√ß√£o b√°sica de consist√™ncia
        has_contradictions = False
        
        # Exemplo: verificar se pede formato JSON mas tamb√©m formato texto
        if "formato json" in prompt.lower() and "formato texto" in prompt.lower():
            has_contradictions = True
        
        score = 0 if has_contradictions else 1
        
        feedback = []
        if has_contradictions:
            feedback.append("Detectadas poss√≠veis contradi√ß√µes nas instru√ß√µes")
            
        return score, feedback
    
    def get_recommendation(self, score):
        """Fornece recomenda√ß√£o baseada na pontua√ß√£o."""
        if score >= 0.8:
            return "Prompt est√° pronto para produ√ß√£o"
        elif score >= 0.6:
            return "Prompt precisa de pequenos ajustes"
        elif score >= 0.4:
            return "Prompt requer melhorias significativas"
        else:
            return "Prompt precisa ser reescrito"
```

### 2. A/B Testing para Prompts

```python
import random
from typing import List, Dict, Any

class PromptABTesting:
    """
    Framework para testar diferentes vers√µes de prompts em produ√ß√£o.
    """
    
    def __init__(self):
        self.experiments = {}
        self.results = {}
    
    def create_experiment(self, experiment_name: str, prompts: List[str], 
                         traffic_split: List[float] = None):
        """Cria um novo experimento A/B para prompts."""
        if traffic_split is None:
            traffic_split = [1.0 / len(prompts)] * len(prompts)
        
        if len(traffic_split) != len(prompts):
            raise ValueError("Traffic split deve ter mesmo tamanho que lista de prompts")
        
        if abs(sum(traffic_split) - 1.0) > 0.001:
            raise ValueError("Traffic split deve somar 1.0")
        
        self.experiments[experiment_name] = {
            "prompts": prompts,
            "traffic_split": traffic_split,
            "active": True
        }
        
        self.results[experiment_name] = {
            f"variant_{i}": {"requests": 0, "success_rate": 0, "avg_score": 0, "scores": []}
            for i in range(len(prompts))
        }
    
    def get_prompt_variant(self, experiment_name: str) -> tuple:
        """Retorna variante do prompt baseada no traffic split."""
        if experiment_name not in self.experiments:
            raise ValueError(f"Experimento {experiment_name} n√£o encontrado")
        
        experiment = self.experiments[experiment_name]
        if not experiment["active"]:
            raise ValueError(f"Experimento {experiment_name} n√£o est√° ativo")
        
        # Sele√ß√£o aleat√≥ria baseada no traffic split
        rand = random.random()
        cumulative = 0
        
        for i, split in enumerate(experiment["traffic_split"]):
            cumulative += split
            if rand <= cumulative:
                return i, experiment["prompts"][i]
        
        # Fallback para √∫ltima variante
        return len(experiment["prompts"]) - 1, experiment["prompts"][-1]
    
    def record_result(self, experiment_name: str, variant_index: int, 
                     success: bool, quality_score: float = None):
        """Registra resultado de uma execu√ß√£o do experimento."""
        if experiment_name not in self.results:
            return
        
        variant_key = f"variant_{variant_index}"
        variant_results = self.results[experiment_name][variant_key]
        
        variant_results["requests"] += 1
        
        if success:
            variant_results["success_rate"] = (
                (variant_results["success_rate"] * (variant_results["requests"] - 1) + 1) 
                / variant_results["requests"]
            )
        
        if quality_score is not None:
            variant_results["scores"].append(quality_score)
            variant_results["avg_score"] = sum(variant_results["scores"]) / len(variant_results["scores"])
    
    def get_experiment_results(self, experiment_name: str) -> Dict[str, Any]:
        """Retorna resultados detalhados do experimento."""
        if experiment_name not in self.results:
            return {}
        
        results = self.results[experiment_name]
        
        # Calcular signific√¢ncia estat√≠stica (implementa√ß√£o simplificada)
        analysis = {
            "experiment": experiment_name,
            "variants": [],
            "recommendation": None
        }
        
        best_variant = None
        best_score = 0
        
        for variant_key, data in results.items():
            variant_analysis = {
                "variant": variant_key,
                "requests": data["requests"],
                "success_rate": data["success_rate"],
                "avg_quality_score": data["avg_score"],
                "confidence": "high" if data["requests"] > 100 else "low"
            }
            
            # Score combinado (success rate + quality)
            combined_score = (data["success_rate"] * 0.6 + data["avg_score"] * 0.4)
            variant_analysis["combined_score"] = combined_score
            
            if combined_score > best_score and data["requests"] > 50:
                best_score = combined_score
                best_variant = variant_key
            
            analysis["variants"].append(variant_analysis)
        
        if best_variant:
            analysis["recommendation"] = f"Usar {best_variant} como padr√£o"
        else:
            analysis["recommendation"] = "Coletar mais dados antes de decidir"
        
        return analysis
    
    def stop_experiment(self, experiment_name: str):
        """Para um experimento e retorna resultados finais."""
        if experiment_name in self.experiments:
            self.experiments[experiment_name]["active"] = False
        
        return self.get_experiment_results(experiment_name)

# Exemplo de uso
def demonstrate_ab_testing():
    """Demonstra como usar o sistema de A/B testing."""
    
    # Inicializar sistema de testes
    ab_tester = PromptABTesting()
    
    # Criar experimento com duas vers√µes de prompt
    prompts = [
        "Analise o seguinte c√≥digo e forne√ßa feedback:",  # Vers√£o A
        "Como engenheiro s√™nior, revise este c√≥digo e identifique melhorias espec√≠ficas:"  # Vers√£o B
    ]
    
    ab_tester.create_experiment("code_review_prompt", prompts, [0.5, 0.5])
    
    # Simular uso em produ√ß√£o
    for _ in range(200):
        variant_index, prompt = ab_tester.get_prompt_variant("code_review_prompt")
        
        # Simular execu√ß√£o e avalia√ß√£o
        success = random.random() > 0.1  # 90% success rate base
        quality_score = random.uniform(0.6, 1.0)
        
        ab_tester.record_result("code_review_prompt", variant_index, success, quality_score)
    
    # Analisar resultados
    results = ab_tester.get_experiment_results("code_review_prompt")
    print("Resultados do A/B Test:")
    print(json.dumps(results, indent=2))
    
    # Parar experimento
    final_results = ab_tester.stop_experiment("code_review_prompt")
    print(f"Recomenda√ß√£o final: {final_results['recommendation']}")
```

## üé® T√©cnicas Especializadas

### 1. Prompting para Dom√≠nios Espec√≠ficos

```python
class DomainSpecificPrompts:
    """
    Templates especializados para diferentes dom√≠nios t√©cnicos.
    """
    
    @staticmethod
    def cybersecurity_analysis():
        return """
Voc√™ √© um especialista em ciberseguran√ßa com 15 anos de experi√™ncia.
Analise o seguinte c√≥digo/sistema para vulnerabilidades de seguran√ßa:

{input_code}

Forne√ßa uma an√°lise estruturada incluindo:

üîí **CLASSIFICA√á√ÉO DE RISCO**
- Cr√≠tico (9-10): Vulnerabilidades que permitem comprometimento total
- Alto (7-8): Exposi√ß√£o significativa de dados ou sistemas
- M√©dio (4-6): Vulnerabilidades que requerem condi√ß√µes espec√≠ficas
- Baixo (1-3): Problemas menores de seguran√ßa

üéØ **VULNERABILIDADES IDENTIFICADAS**
Para cada vulnerabilidade encontrada:
- Tipo (ex: SQL Injection, XSS, Buffer Overflow)
- Localiza√ß√£o exata no c√≥digo
- Impacto potencial
- Exploitabilidade
- CVSS Score estimado

üõ°Ô∏è **RECOMENDA√á√ïES DE MITIGA√á√ÉO**
- Corre√ß√µes imediatas (quick wins)
- Melhorias de m√©dio prazo
- Arquitetura de seguran√ßa recomendada
- Ferramentas de seguran√ßa sugeridas

üîç **C√ìDIGO SEGURO SUGERIDO**
Forne√ßa vers√µes corrigidas dos trechos vulner√°veis.

An√°lise:
"""
    
    @staticmethod
    def performance_optimization():
        return """
Voc√™ √© um engenheiro de performance especialista em otimiza√ß√£o de sistemas.
Analise o seguinte c√≥digo/configura√ß√£o para gargalos de performance:

{input_code}

Estruture sua an√°lise seguindo esta metodologia:

‚ö° **M√âTRICAS DE BASELINE**
- Complexidade computacional atual (Big O)
- Uso estimado de mem√≥ria
- I/O operations identificadas
- Depend√™ncias externas

üîç **GARGALOS IDENTIFICADOS**
Para cada gargalo:
- Localiza√ß√£o espec√≠fica
- Tipo (CPU, Memory, I/O, Network)
- Impacto estimado na performance
- Frequ√™ncia de execu√ß√£o

üöÄ **OTIMIZA√á√ïES RECOMENDADAS**
Priorize por impacto vs esfor√ßo:

**Alto Impacto, Baixo Esfor√ßo (Quick Wins):**
- [Listar otimiza√ß√µes simples]

**Alto Impacto, Alto Esfor√ßo (Projetos):**
- [Listar refatora√ß√µes significativas]

**Otimiza√ß√µes Espec√≠ficas:**
- Algoritmos mais eficientes
- Estruturas de dados otimizadas
- Padr√µes de cache
- Paraleliza√ß√£o/concorr√™ncia

üìä **ESTIMATIVA DE MELHORIA**
- Performance gain esperado (%)
- Redu√ß√£o de recursos estimada
- Tempo de implementa√ß√£o

üí° **C√ìDIGO OTIMIZADO**
Forne√ßa implementa√ß√µes otimizadas dos trechos cr√≠ticos.

An√°lise:
"""
    
    @staticmethod
    def architecture_review():
        return """
Voc√™ √© um arquiteto de software s√™nior com expertise em sistemas distribu√≠dos.
Revise a seguinte arquitetura/design:

{architecture_description}

Conduza uma revis√£o abrangente seguindo estes pilares:

üèóÔ∏è **PRINC√çPIOS ARQUITETURAIS**
Avalie ader√™ncia aos princ√≠pios:
- Single Responsibility
- Open/Closed Principle  
- Dependency Inversion
- DRY (Don't Repeat Yourself)
- KISS (Keep It Simple, Stupid)

üîß **QUALIDADE ARQUITETURAL**
- **Escalabilidade**: Capacidade de growth horizontal/vertical
- **Maintainability**: Facilidade de mudan√ßas e debugging
- **Reliability**: Toler√¢ncia a falhas e recovery
- **Performance**: Efici√™ncia e responsividade
- **Security**: Prote√ß√£o e controle de acesso

üåê **PADR√ïES ARQUITETURAIS**
- Padr√µes utilizados (identificar)
- Padr√µes recomendados (sugerir)
- Anti-patterns detectados (alertar)

‚ö†Ô∏è **RISCOS E TRADE-OFFS**
- Pontos de falha √∫nicos
- Gargalos de escalabilidade
- D√©bito t√©cnico acumulado
- Complexidade desnecess√°ria

üéØ **RECOMENDA√á√ïES ESTRAT√âGICAS**
**Curto Prazo (1-3 meses):**
- [Melhorias imediatas]

**M√©dio Prazo (3-12 meses):**
- [Refatora√ß√µes significativas]

**Longo Prazo (1+ anos):**
- [Evolu√ß√£o arquitetural]

üìã **IMPLEMENTA√á√ÉO**
- Prioriza√ß√£o por impacto
- Estimativas de esfor√ßo
- Plano de migra√ß√£o quando aplic√°vel

Revis√£o Arquitetural:
"""
```

### 2. Prompting Contextual Din√¢mico

```python
class ContextualPromptEngine:
    """
    Engine que adapta prompts baseado em contexto din√¢mico.
    """
    
    def __init__(self):
        self.context_history = []
        self.user_preferences = {}
        self.domain_context = {}
    
    def build_dynamic_prompt(self, base_task, user_id=None, session_context=None):
        """Constr√≥i prompt adaptado ao contexto atual."""
        
        # Carregar prefer√™ncias do usu√°rio
        user_prefs = self.user_preferences.get(user_id, {})
        
        # Analisar contexto da sess√£o
        context = session_context or {}
        
        # Determinar n√≠vel de expertise
        expertise_level = self.determine_expertise_level(user_id, context)
        
        # Selecionar estilo de comunica√ß√£o
        communication_style = user_prefs.get("style", "professional")
        
        # Construir prompt adaptado
        prompt_components = []
        
        # 1. Definir persona baseada no expertise
        persona = self.get_persona_for_expertise(expertise_level)
        prompt_components.append(f"Voc√™ √© {persona}.")
        
        # 2. Adicionar contexto relevante
        if context.get("previous_interactions"):
            prompt_components.append(f"Contexto da conversa anterior: {context['previous_interactions'][-1]}")
        
        # 3. Adaptar complexidade
        complexity_instruction = self.get_complexity_instruction(expertise_level)
        prompt_components.append(complexity_instruction)
        
        # 4. Incluir prefer√™ncias de formato
        format_prefs = user_prefs.get("output_format", "structured")
        format_instruction = self.get_format_instruction(format_prefs)
        prompt_components.append(format_instruction)
        
        # 5. Adicionar tarefa principal
        prompt_components.append(f"Tarefa: {base_task}")
        
        # 6. Instru√ß√µes de estilo
        style_instruction = self.get_style_instruction(communication_style)
        prompt_components.append(style_instruction)
        
        return "\n\n".join(prompt_components)
    
    def determine_expertise_level(self, user_id, context):
        """Determina n√≠vel de expertise baseado em hist√≥rico."""
        if not user_id:
            return "intermediate"
        
        # An√°lise simplificada - na pr√°tica, seria mais sofisticada
        user_history = [ctx for ctx in self.context_history if ctx.get("user_id") == user_id]
        
        if len(user_history) < 5:
            return "beginner"
        elif len(user_history) < 20:
            return "intermediate"
        else:
            return "expert"
    
    def get_persona_for_expertise(self, level):
        """Retorna persona apropriada para o n√≠vel de expertise."""
        personas = {
            "beginner": "um mentor paciente que explica conceitos fundamentais claramente",
            "intermediate": "um colega experiente que fornece insights pr√°ticos",
            "expert": "um especialista t√©cnico que discute detalhes avan√ßados e nuances"
        }
        return personas.get(level, personas["intermediate"])
    
    def get_complexity_instruction(self, level):
        """Retorna instru√ß√£o de complexidade baseada no n√≠vel."""
        instructions = {
            "beginner": "Explique conceitos b√°sicos, evite jarg√£o t√©cnico excessivo, forne√ßa exemplos simples.",
            "intermediate": "Use terminologia t√©cnica apropriada, forne√ßa contexto pr√°tico, inclua exemplos relevantes.",
            "expert": "Discuta detalhes t√©cnicos profundos, considere edge cases, referencie melhores pr√°ticas avan√ßadas."
        }
        return instructions.get(level, instructions["intermediate"])
    
    def get_format_instruction(self, format_pref):
        """Retorna instru√ß√£o de formato baseada na prefer√™ncia."""
        formats = {
            "structured": "Use formata√ß√£o estruturada com cabe√ßalhos, listas e se√ß√µes claras.",
            "conversational": "Use tom conversacional e fluxo natural de ideias.",
            "technical": "Forne√ßa documenta√ß√£o t√©cnica detalhada com especifica√ß√µes precisas.",
            "executive": "Forne√ßa resumo executivo focado em impacto e decis√µes."
        }
        return formats.get(format_pref, formats["structured"])
    
    def get_style_instruction(self, style):
        """Retorna instru√ß√£o de estilo de comunica√ß√£o."""
        styles = {
            "professional": "Mantenha tom profissional e objetivo.",
            "friendly": "Use tom amig√°vel e acess√≠vel.",
            "academic": "Use estilo acad√™mico com refer√™ncias e rigor cient√≠fico.",
            "practical": "Foque em aplica√ß√µes pr√°ticas e solu√ß√µes acion√°veis."
        }
        return styles.get(style, styles["professional"])
    
    def update_context(self, user_id, interaction_data):
        """Atualiza contexto baseado em nova intera√ß√£o."""
        context_entry = {
            "user_id": user_id,
            "timestamp": "2024-03-20T10:00:00Z",
            "interaction": interaction_data
        }
        self.context_history.append(context_entry)
        
        # Manter apenas √∫ltimas 100 intera√ß√µes por usu√°rio
        user_contexts = [ctx for ctx in self.context_history if ctx.get("user_id") == user_id]
        if len(user_contexts) > 100:
            # Remove contextos mais antigos
            self.context_history = [
                ctx for ctx in self.context_history 
                if ctx.get("user_id") != user_id or ctx in user_contexts[-100:]
            ]
```

## üî¨ An√°lise e Otimiza√ß√£o Cont√≠nua

### Sistema de M√©tricas para Prompts

```python
class PromptMetricsSystem:
    """
    Sistema abrangente de m√©tricas para monitorar performance de prompts.
    """
    
    def __init__(self):
        self.metrics_store = defaultdict(list)
        self.benchmarks = {}
    
    def track_prompt_performance(self, prompt_id, metrics_data):
        """Rastreia performance de um prompt espec√≠fico."""
        timestamp = datetime.utcnow().isoformat()
        
        metrics_entry = {
            "timestamp": timestamp,
            "prompt_id": prompt_id,
            **metrics_data
        }
        
        self.metrics_store[prompt_id].append(metrics_entry)
    
    def calculate_aggregate_metrics(self, prompt_id, time_window_hours=24):
        """Calcula m√©tricas agregadas para um prompt."""
        cutoff_time = datetime.utcnow() - timedelta(hours=time_window_hours)
        
        recent_metrics = [
            m for m in self.metrics_store[prompt_id]
            if datetime.fromisoformat(m["timestamp"]) > cutoff_time
        ]
        
        if not recent_metrics:
            return None
        
        # Calcular m√©tricas agregadas
        success_rates = [m.get("success", 0) for m in recent_metrics]
        response_times = [m.get("response_time", 0) for m in recent_metrics]
        quality_scores = [m.get("quality_score", 0) for m in recent_metrics if m.get("quality_score")]
        
        return {
            "prompt_id": prompt_id,
            "time_window_hours": time_window_hours,
            "total_requests": len(recent_metrics),
            "success_rate": sum(success_rates) / len(success_rates) if success_rates else 0,
            "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
            "avg_quality_score": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            "error_rate": (len(recent_metrics) - sum(success_rates)) / len(recent_metrics) if recent_metrics else 0
        }
    
    def detect_performance_anomalies(self, prompt_id):
        """Detecta anomalias na performance de prompts."""
        recent_metrics = self.calculate_aggregate_metrics(prompt_id, 1)  # √öltima hora
        baseline_metrics = self.calculate_aggregate_metrics(prompt_id, 168)  # √öltima semana
        
        if not recent_metrics or not baseline_metrics:
            return []
        
        anomalies = []
        
        # Verificar degrada√ß√£o significativa na taxa de sucesso
        success_rate_drop = baseline_metrics["success_rate"] - recent_metrics["success_rate"]
        if success_rate_drop > 0.1:  # 10% drop
            anomalies.append({
                "type": "success_rate_degradation",
                "severity": "high" if success_rate_drop > 0.2 else "medium",
                "current": recent_metrics["success_rate"],
                "baseline": baseline_metrics["success_rate"],
                "message": f"Taxa de sucesso caiu {success_rate_drop:.2%} comparado √† baseline"
            })
        
        # Verificar aumento no tempo de resposta
        response_time_increase = recent_metrics["avg_response_time"] / baseline_metrics["avg_response_time"] - 1
        if response_time_increase > 0.3:  # 30% increase
            anomalies.append({
                "type": "response_time_increase", 
                "severity": "medium" if response_time_increase < 0.5 else "high",
                "current": recent_metrics["avg_response_time"],
                "baseline": baseline_metrics["avg_response_time"],
                "message": f"Tempo de resposta aumentou {response_time_increase:.1%}"
            })
        
        return anomalies
    
    def generate_optimization_recommendations(self, prompt_id):
        """Gera recomenda√ß√µes de otimiza√ß√£o baseadas em m√©tricas."""
        metrics = self.calculate_aggregate_metrics(prompt_id, 168)  # √öltima semana
        anomalies = self.detect_performance_anomalies(prompt_id)
        
        recommendations = []
        
        if metrics:
            # Recomenda√ß√µes baseadas em m√©tricas
            if metrics["success_rate"] < 0.85:
                recommendations.append({
                    "priority": "high",
                    "category": "reliability",
                    "title": "Melhorar Taxa de Sucesso",
                    "description": "Taxa de sucesso abaixo de 85%. Considere revisar clareza das instru√ß√µes.",
                    "suggested_actions": [
                        "Adicionar mais exemplos espec√≠ficos",
                        "Clarificar instru√ß√µes amb√≠guas",
                        "Implementar valida√ß√£o de entrada"
                    ]
                })
            
            if metrics["avg_quality_score"] < 0.7:
                recommendations.append({
                    "priority": "medium",
                    "category": "quality",
                    "title": "Melhorar Qualidade da Sa√≠da",
                    "description": "Score de qualidade m√©dio abaixo de 70%.",
                    "suggested_actions": [
                        "Adicionar crit√©rios de qualidade espec√≠ficos ao prompt",
                        "Implementar few-shot learning com exemplos de alta qualidade",
                        "Usar chain-of-thought para racioc√≠nio mais estruturado"
                    ]
                })
            
            if metrics["avg_response_time"] > 10:  # 10 segundos
                recommendations.append({
                    "priority": "medium",
                    "category": "performance",
                    "title": "Otimizar Tempo de Resposta",
                    "description": "Tempo de resposta m√©dio acima de 10 segundos.",
                    "suggested_actions": [
                        "Reduzir tamanho do prompt se poss√≠vel",
                        "Usar modelos mais r√°pidos para tarefas simples",
                        "Implementar cache para respostas comuns"
                    ]
                })
        
        # Recomenda√ß√µes baseadas em anomalias
        for anomaly in anomalies:
            if anomaly["type"] == "success_rate_degradation":
                recommendations.append({
                    "priority": "urgent",
                    "category": "incident",
                    "title": "Degrada√ß√£o Cr√≠tica Detectada",
                    "description": anomaly["message"],
                    "suggested_actions": [
                        "Investigar mudan√ßas recentes no prompt ou modelo",
                        "Revisar logs de erro para padr√µes",
                        "Considerar rollback para vers√£o est√°vel"
                    ]
                })
        
        return recommendations
```

## üéØ Conclus√£o e Melhores Pr√°ticas

Dominar engenharia de prompts √© uma jornada cont√≠nua. Aqui est√£o as li√ß√µes mais importantes que aprendi:

### üèÜ Princ√≠pios Fundamentais

1. **Clareza Sobre Cleverness**: Prompts claros superam prompts "inteligentes"
2. **Itera√ß√£o Baseada em Dados**: Use m√©tricas reais, n√£o intui√ß√£o
3. **Contexto √© Rei**: Forne√ßa contexto suficiente sem overload
4. **Valida√ß√£o Cont√≠nua**: Teste prompts em cen√°rios reais
5. **Especializa√ß√£o**: Prompts espec√≠ficos superam prompts gen√©ricos

### üé® T√©cnicas Avan√ßadas Resumidas

- **Chain-of-Thought**: Para racioc√≠nio complexo
- **Few-Shot Learning**: Para tarefas com padr√µes espec√≠ficos  
- **Prompting Hier√°rquico**: Para problemas multi-etapa
- **Templates Reutiliz√°veis**: Para consist√™ncia e efici√™ncia
- **A/B Testing**: Para otimiza√ß√£o baseada em evid√™ncias

### üöÄ Implementa√ß√£o em Produ√ß√£o

- **Monitoramento**: Implemente m√©tricas abrangentes
- **Versionamento**: Trate prompts como c√≥digo
- **Fallbacks**: Sempre tenha planos de conting√™ncia
- **Seguran√ßa**: Valide entradas e sa√≠das
- **Custos**: Monitore e otimize usage de tokens

### üìà Pr√≥ximos Passos

1. **Experimente**: Implemente as t√©cnicas em seus projetos
2. **Me√ßa**: Estabele√ßa m√©tricas de baseline
3. **Iterate**: Melhore baseado em dados reais
4. **Compartilhe**: Documente aprendizados para o time
5. **Evolua**: Mantenha-se atualizado com novas t√©cnicas

A engenharia de prompts eficaz combina arte e ci√™ncia. Com as t√©cnicas e frameworks apresentados aqui, voc√™ est√° equipado para criar prompts que n√£o apenas funcionam, mas excedem expectativas em sistemas de produ√ß√£o.

Lembre-se: o melhor prompt √© aquele que resolve consistentemente o problema do usu√°rio da forma mais eficiente poss√≠vel. Tudo mais √© secund√°rio.