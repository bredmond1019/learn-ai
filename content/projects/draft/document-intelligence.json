{
  "slug": "document-intelligence",
  "title": "Document Intelligence Platform",
  "description": "AI-powered document processing platform using OCR, NLP, and machine learning to extract, classify, and analyze structured data from unstructured documents with 95% accuracy across 20+ document types.",
  "longDescription": "Architected and developed a comprehensive document intelligence platform that transforms unstructured documents into actionable data through advanced AI and machine learning techniques. This project combines Brandon's systems engineering expertise with cutting-edge AI technologies to solve complex document processing challenges faced by enterprises.\n\nThe platform handles diverse document types including invoices, contracts, forms, reports, and medical records, using a combination of computer vision, natural language processing, and custom machine learning models. It emphasizes accuracy and reliability through multi-stage validation, human-in-the-loop workflows, and continuous learning capabilities.\n\nDeployed across multiple industries including healthcare, finance, and legal services, the platform has processed millions of documents while maintaining high accuracy rates and significantly reducing manual processing time.",
  "tags": ["Machine Learning", "OCR", "NLP", "Computer Vision", "Python", "TensorFlow", "Document Processing"],
  "featured": true,
  "githubUrl": "https://github.com/brandonjredmond/document-intelligence",
  "demoUrl": "https://document-ai-demo.brandonredmond.dev",
  "techStack": [
    {
      "category": "AI/ML Framework",
      "items": ["TensorFlow", "PyTorch", "Hugging Face Transformers", "OpenCV", "scikit-learn", "spaCy"]
    },
    {
      "category": "OCR & Vision",
      "items": ["Tesseract", "Google Cloud Vision", "AWS Textract", "Azure Cognitive Services", "PaddleOCR"]
    },
    {
      "category": "Backend Platform",
      "items": ["FastAPI", "Celery", "Redis", "PostgreSQL", "MinIO", "Docker", "Kubernetes"]
    },
    {
      "category": "Monitoring & MLOps",
      "items": ["MLflow", "Weights & Biases", "Prometheus", "Grafana", "Apache Airflow"]
    }
  ],
  "features": [
    "Multi-modal document analysis (text, images, tables, forms)",
    "Intelligent document classification with 95%+ accuracy",
    "Custom field extraction for domain-specific documents",
    "Real-time processing with batch processing capabilities",
    "Human-in-the-loop validation workflows",
    "Continuous learning and model improvement pipeline",
    "Multi-language support for 15+ languages",
    "Compliance features for GDPR, HIPAA, and SOX requirements"
  ],
  "challenges": [
    "Handling poor quality document scans and images",
    "Developing robust table extraction from complex layouts",
    "Creating domain-specific models for specialized document types",
    "Balancing processing speed with accuracy requirements",
    "Implementing effective human-in-the-loop feedback mechanisms"
  ],
  "outcomes": [
    { "metric": "Processing Accuracy", "value": "95%+" },
    { "metric": "Document Types Supported", "value": "20+" },
    { "metric": "Processing Speed", "value": "10x faster than manual" },
    { "metric": "Error Reduction", "value": "90%" },
    { "metric": "Customer Satisfaction", "value": "98%" }
  ],
  "educational": [
    "Computer vision techniques for document layout analysis",
    "Natural language processing for information extraction",
    "Machine learning model training and optimization for OCR",
    "MLOps practices for production AI systems",
    "Data preprocessing and augmentation for document AI",
    "Human-in-the-loop system design and implementation"
  ],
  "codeSnippets": [
    {
      "title": "Document Processing Pipeline",
      "language": "python",
      "code": "import cv2\nimport numpy as np\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport pytesseract\nfrom PIL import Image\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport spacy\nfrom sklearn.preprocessing import LabelEncoder\nimport json\nimport logging\n\nclass DocumentType(Enum):\n    INVOICE = \"invoice\"\n    CONTRACT = \"contract\"\n    FORM = \"form\"\n    RECEIPT = \"receipt\"\n    REPORT = \"report\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass BoundingBox:\n    x: int\n    y: int\n    width: int\n    height: int\n    confidence: float = 0.0\n\n@dataclass\nclass ExtractedField:\n    field_name: str\n    value: str\n    confidence: float\n    bounding_box: Optional[BoundingBox] = None\n    validation_status: str = \"pending\"\n\n@dataclass\nclass DocumentAnalysisResult:\n    document_type: DocumentType\n    confidence: float\n    extracted_fields: List[ExtractedField]\n    raw_text: str\n    processing_time: float\n    quality_score: float\n\nclass DocumentPreprocessor:\n    \"\"\"Handles document image preprocessing for optimal OCR\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Apply preprocessing steps to improve OCR accuracy\"\"\"\n        try:\n            # Convert to grayscale if needed\n            if len(image.shape) == 3:\n                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            else:\n                gray = image.copy()\n            \n            # Noise reduction\n            denoised = cv2.fastNlMeansDenoising(gray)\n            \n            # Deskewing\n            deskewed = self._deskew_image(denoised)\n            \n            # Enhance contrast\n            enhanced = self._enhance_contrast(deskewed)\n            \n            # Binarization with adaptive threshold\n            binary = cv2.adaptiveThreshold(\n                enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n                cv2.THRESH_BINARY, 11, 2\n            )\n            \n            return binary\n            \n        except Exception as e:\n            self.logger.error(f\"Preprocessing failed: {e}\")\n            return image\n    \n    def _deskew_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Correct image skew using Hough line detection\"\"\"\n        edges = cv2.Canny(image, 50, 150, apertureSize=3)\n        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n        \n        if lines is not None:\n            angles = []\n            for rho, theta in lines[:10]:  # Use first 10 lines\n                angle = theta * 180 / np.pi - 90\n                angles.append(angle)\n            \n            # Get median angle\n            median_angle = np.median(angles)\n            \n            # Rotate image if skew is significant\n            if abs(median_angle) > 0.5:\n                (h, w) = image.shape[:2]\n                center = (w // 2, h // 2)\n                rotation_matrix = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n                return cv2.warpAffine(image, rotation_matrix, (w, h), \n                                    flags=cv2.INTER_CUBIC, \n                                    borderMode=cv2.BORDER_REPLICATE)\n        \n        return image\n    \n    def _enhance_contrast(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Enhance image contrast using CLAHE\"\"\"\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        return clahe.apply(image)\n    \n    def calculate_quality_score(self, image: np.ndarray) -> float:\n        \"\"\"Calculate image quality score for processing confidence\"\"\"\n        # Sharpness (Laplacian variance)\n        laplacian_var = cv2.Laplacian(image, cv2.CV_64F).var()\n        sharpness_score = min(laplacian_var / 1000, 1.0)\n        \n        # Contrast (standard deviation)\n        contrast_score = min(image.std() / 100, 1.0)\n        \n        # Resolution adequacy\n        height, width = image.shape[:2]\n        resolution_score = min((height * width) / (1000 * 1000), 1.0)\n        \n        # Weighted average\n        quality_score = (\n            0.4 * sharpness_score + \n            0.3 * contrast_score + \n            0.3 * resolution_score\n        )\n        \n        return quality_score\n\nclass DocumentClassifier:\n    \"\"\"Classifies documents using deep learning\"\"\"\n    \n    def __init__(self, model_path: str):\n        self.model = tf.keras.models.load_model(model_path)\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.classes_ = np.array([dt.value for dt in DocumentType])\n        \n    def extract_features(self, image: np.ndarray, text: str) -> np.ndarray:\n        \"\"\"Extract features for document classification\"\"\"\n        # Image features\n        img_resized = cv2.resize(image, (224, 224))\n        img_features = img_resized.flatten() / 255.0\n        \n        # Text features (simple bag of words for demo)\n        common_words = [\n            'invoice', 'contract', 'agreement', 'receipt', 'total', \n            'amount', 'date', 'signature', 'terms', 'conditions'\n        ]\n        \n        text_lower = text.lower()\n        text_features = [1 if word in text_lower else 0 for word in common_words]\n        \n        # Layout features\n        contours, _ = cv2.findContours(\n            image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n        layout_features = [\n            len(contours),  # Number of components\n            np.mean([cv2.contourArea(c) for c in contours]) if contours else 0,\n            image.shape[0] / image.shape[1]  # Aspect ratio\n        ]\n        \n        # Combine all features\n        all_features = np.concatenate([\n            img_features[:1000],  # Truncate image features\n            text_features,\n            layout_features\n        ])\n        \n        return all_features.reshape(1, -1)\n    \n    def classify(self, image: np.ndarray, text: str) -> Tuple[DocumentType, float]:\n        \"\"\"Classify document type\"\"\"\n        features = self.extract_features(image, text)\n        \n        # Pad or truncate features to match model input\n        expected_features = 1013  # Adjust based on your model\n        if features.shape[1] < expected_features:\n            padding = np.zeros((1, expected_features - features.shape[1]))\n            features = np.concatenate([features, padding], axis=1)\n        elif features.shape[1] > expected_features:\n            features = features[:, :expected_features]\n        \n        predictions = self.model.predict(features)\n        predicted_class_idx = np.argmax(predictions[0])\n        confidence = np.max(predictions[0])\n        \n        document_type = DocumentType(self.label_encoder.classes_[predicted_class_idx])\n        \n        return document_type, confidence\n\nclass FieldExtractor:\n    \"\"\"Extracts specific fields from documents using NER and patterns\"\"\"\n    \n    def __init__(self):\n        # Load pre-trained NER model\n        self.tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n        self.model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n        \n        # Load spaCy model for additional processing\n        self.nlp = spacy.load(\"en_core_web_sm\")\n        \n        # Field extraction patterns for different document types\n        self.field_patterns = {\n            DocumentType.INVOICE: {\n                'invoice_number': [r'invoice[\\s#:]*([A-Z0-9-]+)', r'inv[\\s#:]*([A-Z0-9-]+)'],\n                'date': [r'date[\\s:]*([0-9]{1,2}[/-][0-9]{1,2}[/-][0-9]{2,4})', \n                        r'([0-9]{1,2}[/-][0-9]{1,2}[/-][0-9]{2,4})'],\n                'total': [r'total[\\s:$]*([0-9,]+\\.?[0-9]*)', r'amount[\\s:$]*([0-9,]+\\.?[0-9]*)'],\n                'vendor': [r'from[\\s:]*([A-Za-z\\s]+)', r'vendor[\\s:]*([A-Za-z\\s]+)']\n            },\n            DocumentType.CONTRACT: {\n                'contract_number': [r'contract[\\s#:]*([A-Z0-9-]+)', r'agreement[\\s#:]*([A-Z0-9-]+)'],\n                'effective_date': [r'effective[\\s:]*([0-9]{1,2}[/-][0-9]{1,2}[/-][0-9]{2,4})'],\n                'parties': [r'between[\\s:]*([A-Za-z\\s,]+)and([A-Za-z\\s,]+)'],\n                'term': [r'term[\\s:]*([0-9]+[\\s]*(year|month|day)s?)'],\n            }\n        }\n    \n    def extract_fields(self, text: str, document_type: DocumentType, \n                      image: Optional[np.ndarray] = None) -> List[ExtractedField]:\n        \"\"\"Extract fields based on document type\"\"\"\n        extracted_fields = []\n        \n        # Get patterns for document type\n        patterns = self.field_patterns.get(document_type, {})\n        \n        # Extract using regex patterns\n        for field_name, field_patterns in patterns.items():\n            for pattern in field_patterns:\n                matches = re.finditer(pattern, text, re.IGNORECASE)\n                for match in matches:\n                    value = match.group(1).strip()\n                    confidence = 0.8  # Base confidence for regex matches\n                    \n                    extracted_fields.append(ExtractedField(\n                        field_name=field_name,\n                        value=value,\n                        confidence=confidence\n                    ))\n                    break  # Use first match\n        \n        # Extract using NER for additional entities\n        ner_fields = self._extract_ner_fields(text)\n        extracted_fields.extend(ner_fields)\n        \n        # Extract using spaCy for structured information\n        spacy_fields = self._extract_spacy_fields(text)\n        extracted_fields.extend(spacy_fields)\n        \n        return self._deduplicate_fields(extracted_fields)\n    \n    def _extract_ner_fields(self, text: str) -> List[ExtractedField]:\n        \"\"\"Extract fields using Named Entity Recognition\"\"\"\n        # Tokenize text\n        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, \n                               padding=True, max_length=512)\n        \n        # Get predictions\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n            predicted_token_class = predictions.argmax(-1)\n        \n        # Convert predictions to entities\n        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n        \n        extracted_fields = []\n        current_entity = None\n        current_tokens = []\n        \n        for token, pred in zip(tokens, predicted_token_class[0]):\n            label = self.model.config.id2label[pred.item()]\n            \n            if label.startswith('B-'):  # Beginning of entity\n                if current_entity:\n                    # Save previous entity\n                    value = self.tokenizer.convert_tokens_to_string(current_tokens)\n                    extracted_fields.append(ExtractedField(\n                        field_name=current_entity.lower(),\n                        value=value.strip(),\n                        confidence=0.7\n                    ))\n                \n                current_entity = label[2:]  # Remove B- prefix\n                current_tokens = [token]\n                \n            elif label.startswith('I-') and current_entity:  # Inside entity\n                current_tokens.append(token)\n                \n            else:  # Outside entity\n                if current_entity:\n                    value = self.tokenizer.convert_tokens_to_string(current_tokens)\n                    extracted_fields.append(ExtractedField(\n                        field_name=current_entity.lower(),\n                        value=value.strip(),\n                        confidence=0.7\n                    ))\n                    current_entity = None\n                    current_tokens = []\n        \n        return extracted_fields\n    \n    def _extract_spacy_fields(self, text: str) -> List[ExtractedField]:\n        \"\"\"Extract fields using spaCy NLP pipeline\"\"\"\n        doc = self.nlp(text)\n        extracted_fields = []\n        \n        # Extract named entities\n        for ent in doc.ents:\n            if ent.label_ in ['MONEY', 'DATE', 'ORG', 'PERSON']:\n                field_name = ent.label_.lower()\n                extracted_fields.append(ExtractedField(\n                    field_name=field_name,\n                    value=ent.text,\n                    confidence=0.6\n                ))\n        \n        return extracted_fields\n    \n    def _deduplicate_fields(self, fields: List[ExtractedField]) -> List[ExtractedField]:\n        \"\"\"Remove duplicate fields, keeping highest confidence\"\"\"\n        field_dict = {}\n        \n        for field in fields:\n            key = (field.field_name, field.value.lower())\n            if key not in field_dict or field.confidence > field_dict[key].confidence:\n                field_dict[key] = field\n        \n        return list(field_dict.values())\n\nclass DocumentIntelligencePipeline:\n    \"\"\"Main pipeline orchestrating document processing\"\"\"\n    \n    def __init__(self, classifier_model_path: str):\n        self.preprocessor = DocumentPreprocessor()\n        self.classifier = DocumentClassifier(classifier_model_path)\n        self.field_extractor = FieldExtractor()\n        self.logger = logging.getLogger(__name__)\n    \n    async def process_document(self, image_path: str) -> DocumentAnalysisResult:\n        \"\"\"Process a document end-to-end\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Load image\n            image = cv2.imread(image_path)\n            if image is None:\n                raise ValueError(f\"Could not load image: {image_path}\")\n            \n            # Preprocess image\n            processed_image = self.preprocessor.preprocess_image(image)\n            quality_score = self.preprocessor.calculate_quality_score(processed_image)\n            \n            # OCR extraction\n            raw_text = pytesseract.image_to_string(\n                Image.fromarray(processed_image),\n                config='--oem 3 --psm 6'\n            )\n            \n            # Document classification\n            doc_type, classification_confidence = self.classifier.classify(\n                processed_image, raw_text\n            )\n            \n            # Field extraction\n            extracted_fields = self.field_extractor.extract_fields(\n                raw_text, doc_type, processed_image\n            )\n            \n            processing_time = time.time() - start_time\n            \n            result = DocumentAnalysisResult(\n                document_type=doc_type,\n                confidence=classification_confidence,\n                extracted_fields=extracted_fields,\n                raw_text=raw_text,\n                processing_time=processing_time,\n                quality_score=quality_score\n            )\n            \n            self.logger.info(\n                f\"Processed document: {doc_type.value} \"\n                f\"(confidence: {classification_confidence:.2f}, \"\n                f\"fields: {len(extracted_fields)}, \"\n                f\"time: {processing_time:.2f}s)\"\n            )\n            \n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Document processing failed: {e}\")\n            raise\n\n# Usage example\nasync def process_documents():\n    pipeline = DocumentIntelligencePipeline(\"path/to/classifier/model\")\n    \n    document_paths = [\n        \"invoice1.pdf\",\n        \"contract2.pdf\",\n        \"receipt3.jpg\"\n    ]\n    \n    results = []\n    for doc_path in document_paths:\n        try:\n            result = await pipeline.process_document(doc_path)\n            results.append(result)\n            \n            print(f\"\\nDocument: {doc_path}\")\n            print(f\"Type: {result.document_type.value}\")\n            print(f\"Confidence: {result.confidence:.2f}\")\n            print(f\"Quality Score: {result.quality_score:.2f}\")\n            print(f\"Processing Time: {result.processing_time:.2f}s\")\n            print(\"\\nExtracted Fields:\")\n            \n            for field in result.extracted_fields:\n                print(f\"  {field.field_name}: {field.value} (conf: {field.confidence:.2f})\")\n                \n        except Exception as e:\n            print(f\"Failed to process {doc_path}: {e}\")\n    \n    return results\n\nif __name__ == '__main__':\n    import asyncio\n    asyncio.run(process_documents())"
    },
    {
      "title": "Human-in-the-Loop Validation System",
      "language": "python",
      "code": "from typing import Dict, List, Any, Optional, Callable\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nimport json\nimport asyncio\nfrom datetime import datetime, timedelta\nimport uuid\nimport logging\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\nfrom pydantic import BaseModel\nimport redis\nfrom sqlalchemy import create_engine, Column, String, DateTime, Float, Text, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\n\nclass ValidationStatus(Enum):\n    PENDING = \"pending\"\n    IN_REVIEW = \"in_review\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n    NEEDS_CORRECTION = \"needs_correction\"\n\nclass ValidationPriority(Enum):\n    LOW = 1\n    MEDIUM = 2\n    HIGH = 3\n    URGENT = 4\n\n@dataclass\nclass ValidationTask:\n    task_id: str\n    document_id: str\n    document_type: str\n    extracted_fields: List[ExtractedField]\n    confidence_score: float\n    priority: ValidationPriority\n    assigned_to: Optional[str] = None\n    status: ValidationStatus = ValidationStatus.PENDING\n    created_at: datetime = None\n    updated_at: datetime = None\n    reviewer_notes: str = \"\"\n    \n    def __post_init__(self):\n        if self.created_at is None:\n            self.created_at = datetime.now()\n        if self.updated_at is None:\n            self.updated_at = datetime.now()\n\nclass ValidationRule:\n    \"\"\"Defines rules for when human validation is required\"\"\"\n    \n    def __init__(self):\n        self.rules = {\n            'low_confidence': lambda task: task.confidence_score < 0.7,\n            'high_value_fields': lambda task: any(\n                field.field_name in ['total', 'amount'] and \n                self._extract_number(field.value) > 10000\n                for field in task.extracted_fields\n            ),\n            'conflicting_extractions': lambda task: self._has_conflicts(task),\n            'sensitive_document': lambda task: task.document_type in ['contract', 'legal'],\n            'multiple_candidates': lambda task: any(\n                field.confidence < 0.8 for field in task.extracted_fields\n            )\n        }\n    \n    def requires_validation(self, task: ValidationTask) -> tuple[bool, List[str]]:\n        \"\"\"Check if task requires human validation\"\"\"\n        triggered_rules = []\n        \n        for rule_name, rule_func in self.rules.items():\n            try:\n                if rule_func(task):\n                    triggered_rules.append(rule_name)\n            except Exception as e:\n                logging.warning(f\"Validation rule {rule_name} failed: {e}\")\n        \n        return len(triggered_rules) > 0, triggered_rules\n    \n    def _extract_number(self, value: str) -> float:\n        \"\"\"Extract numeric value from string\"\"\"\n        import re\n        numbers = re.findall(r'[0-9,]+\\.?[0-9]*', value.replace(',', ''))\n        return float(numbers[0]) if numbers else 0.0\n    \n    def _has_conflicts(self, task: ValidationTask) -> bool:\n        \"\"\"Check for conflicting field extractions\"\"\"\n        field_groups = {}\n        for field in task.extracted_fields:\n            if field.field_name not in field_groups:\n                field_groups[field.field_name] = []\n            field_groups[field.field_name].append(field)\n        \n        # Check for fields with multiple different values\n        for field_name, fields in field_groups.items():\n            if len(fields) > 1:\n                unique_values = set(field.value.lower() for field in fields)\n                if len(unique_values) > 1:\n                    return True\n        \n        return False\n\nclass ValidationQueue:\n    \"\"\"Manages validation task queue with priority and assignment\"\"\"\n    \n    def __init__(self, redis_client):\n        self.redis = redis_client\n        self.queue_key = \"validation_queue\"\n        self.active_tasks_key = \"active_validation_tasks\"\n        self.validator_assignments_key = \"validator_assignments\"\n    \n    async def add_task(self, task: ValidationTask) -> None:\n        \"\"\"Add task to validation queue\"\"\"\n        task_data = {\n            'task_id': task.task_id,\n            'priority': task.priority.value,\n            'created_at': task.created_at.isoformat(),\n            'data': json.dumps(asdict(task))\n        }\n        \n        # Add to priority queue (Redis sorted set)\n        await self.redis.zadd(\n            self.queue_key, \n            {task.task_id: task.priority.value * 1000 + int(task.created_at.timestamp())}\n        )\n        \n        # Store task data\n        await self.redis.hset(f\"task:{task.task_id}\", mapping=task_data)\n    \n    async def get_next_task(self, validator_id: str) -> Optional[ValidationTask]:\n        \"\"\"Get next task for validator\"\"\"\n        # Get highest priority task\n        tasks = await self.redis.zrevrange(self.queue_key, 0, 0, withscores=True)\n        \n        if not tasks:\n            return None\n        \n        task_id = tasks[0][0].decode('utf-8')\n        \n        # Remove from queue and add to active tasks\n        pipe = self.redis.pipeline()\n        pipe.zrem(self.queue_key, task_id)\n        pipe.hset(self.active_tasks_key, task_id, validator_id)\n        pipe.hset(self.validator_assignments_key, validator_id, task_id)\n        await pipe.execute()\n        \n        # Get task data\n        task_data = await self.redis.hget(f\"task:{task_id}\", \"data\")\n        if task_data:\n            task_dict = json.loads(task_data.decode('utf-8'))\n            task = ValidationTask(**task_dict)\n            task.assigned_to = validator_id\n            task.status = ValidationStatus.IN_REVIEW\n            return task\n        \n        return None\n    \n    async def complete_task(self, task_id: str, validator_id: str, \n                           result: Dict[str, Any]) -> None:\n        \"\"\"Mark task as completed\"\"\"\n        pipe = self.redis.pipeline()\n        pipe.hdel(self.active_tasks_key, task_id)\n        pipe.hdel(self.validator_assignments_key, validator_id)\n        pipe.hset(f\"task:{task_id}:result\", mapping=result)\n        await pipe.execute()\n    \n    async def get_queue_stats(self) -> Dict[str, int]:\n        \"\"\"Get queue statistics\"\"\"\n        pending_count = await self.redis.zcard(self.queue_key)\n        active_count = await self.redis.hlen(self.active_tasks_key)\n        \n        return {\n            'pending': pending_count,\n            'active': active_count,\n            'total': pending_count + active_count\n        }\n\nclass ValidationWebSocketManager:\n    \"\"\"Manages WebSocket connections for real-time validation interface\"\"\"\n    \n    def __init__(self):\n        self.active_connections: Dict[str, WebSocket] = {}\n        self.validator_tasks: Dict[str, str] = {}  # validator_id -> task_id\n    \n    async def connect(self, websocket: WebSocket, validator_id: str):\n        await websocket.accept()\n        self.active_connections[validator_id] = websocket\n    \n    def disconnect(self, validator_id: str):\n        if validator_id in self.active_connections:\n            del self.active_connections[validator_id]\n        if validator_id in self.validator_tasks:\n            del self.validator_tasks[validator_id]\n    \n    async def send_task(self, validator_id: str, task: ValidationTask):\n        if validator_id in self.active_connections:\n            websocket = self.active_connections[validator_id]\n            self.validator_tasks[validator_id] = task.task_id\n            \n            await websocket.send_json({\n                'type': 'new_task',\n                'task': asdict(task)\n            })\n    \n    async def send_message(self, validator_id: str, message: Dict[str, Any]):\n        if validator_id in self.active_connections:\n            websocket = self.active_connections[validator_id]\n            await websocket.send_json(message)\n    \n    async def broadcast_stats(self, stats: Dict[str, Any]):\n        message = {\n            'type': 'queue_stats',\n            'stats': stats\n        }\n        \n        disconnected = []\n        for validator_id, websocket in self.active_connections.items():\n            try:\n                await websocket.send_json(message)\n            except:\n                disconnected.append(validator_id)\n        \n        # Clean up disconnected clients\n        for validator_id in disconnected:\n            self.disconnect(validator_id)\n\nclass HumanInTheLoopValidationSystem:\n    \"\"\"Main system coordinating human validation workflow\"\"\"\n    \n    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n        self.redis = redis.from_url(redis_url, decode_responses=True)\n        self.validation_rules = ValidationRule()\n        self.validation_queue = ValidationQueue(self.redis)\n        self.websocket_manager = ValidationWebSocketManager()\n        self.app = FastAPI()\n        self.setup_routes()\n        self.logger = logging.getLogger(__name__)\n    \n    def setup_routes(self):\n        \"\"\"Setup FastAPI routes for validation interface\"\"\"\n        \n        @self.app.websocket(\"/ws/{validator_id}\")\n        async def websocket_endpoint(websocket: WebSocket, validator_id: str):\n            await self.websocket_manager.connect(websocket, validator_id)\n            try:\n                while True:\n                    # Send next task if available\n                    task = await self.validation_queue.get_next_task(validator_id)\n                    if task:\n                        await self.websocket_manager.send_task(validator_id, task)\n                    \n                    # Wait for response or timeout\n                    try:\n                        data = await asyncio.wait_for(websocket.receive_json(), timeout=30.0)\n                        await self.handle_validation_response(validator_id, data)\n                    except asyncio.TimeoutError:\n                        # Send ping to keep connection alive\n                        await websocket.send_json({'type': 'ping'})\n                        \n            except WebSocketDisconnect:\n                self.websocket_manager.disconnect(validator_id)\n        \n        @self.app.get(\"/validation_interface\")\n        async def validation_interface():\n            return HTMLResponse(self.get_validation_html())\n        \n        @self.app.get(\"/stats\")\n        async def get_stats():\n            return await self.validation_queue.get_queue_stats()\n    \n    async def process_document_result(self, document_result: DocumentAnalysisResult, \n                                    document_id: str) -> Optional[ValidationTask]:\n        \"\"\"Process document analysis result and create validation task if needed\"\"\"\n        \n        # Create validation task\n        task = ValidationTask(\n            task_id=str(uuid.uuid4()),\n            document_id=document_id,\n            document_type=document_result.document_type.value,\n            extracted_fields=document_result.extracted_fields,\n            confidence_score=document_result.confidence,\n            priority=self._determine_priority(document_result)\n        )\n        \n        # Check if validation is required\n        needs_validation, triggered_rules = self.validation_rules.requires_validation(task)\n        \n        if needs_validation:\n            self.logger.info(\n                f\"Document {document_id} requires validation. \"\n                f\"Triggered rules: {triggered_rules}\"\n            )\n            \n            await self.validation_queue.add_task(task)\n            \n            # Broadcast updated stats\n            stats = await self.validation_queue.get_queue_stats()\n            await self.websocket_manager.broadcast_stats(stats)\n            \n            return task\n        else:\n            self.logger.info(f\"Document {document_id} auto-approved\")\n            return None\n    \n    async def handle_validation_response(self, validator_id: str, response: Dict[str, Any]):\n        \"\"\"Handle validation response from human validator\"\"\"\n        response_type = response.get('type')\n        \n        if response_type == 'task_completed':\n            task_id = response.get('task_id')\n            status = response.get('status')\n            corrected_fields = response.get('corrected_fields', [])\n            notes = response.get('notes', '')\n            \n            # Process the validation result\n            result = {\n                'validator_id': validator_id,\n                'status': status,\n                'corrected_fields': json.dumps(corrected_fields),\n                'notes': notes,\n                'completed_at': datetime.now().isoformat()\n            }\n            \n            await self.validation_queue.complete_task(task_id, validator_id, result)\n            \n            # Send confirmation\n            await self.websocket_manager.send_message(validator_id, {\n                'type': 'task_confirmed',\n                'task_id': task_id\n            })\n            \n            # Update learning system with validation feedback\n            await self._update_learning_system(task_id, result)\n            \n        elif response_type == 'request_next_task':\n            # Validator is ready for next task\n            task = await self.validation_queue.get_next_task(validator_id)\n            if task:\n                await self.websocket_manager.send_task(validator_id, task)\n            else:\n                await self.websocket_manager.send_message(validator_id, {\n                    'type': 'no_tasks_available'\n                })\n    \n    def _determine_priority(self, result: DocumentAnalysisResult) -> ValidationPriority:\n        \"\"\"Determine validation priority based on document characteristics\"\"\"\n        if result.confidence < 0.5:\n            return ValidationPriority.URGENT\n        elif result.document_type in [DocumentType.CONTRACT]:\n            return ValidationPriority.HIGH\n        elif result.confidence < 0.7:\n            return ValidationPriority.MEDIUM\n        else:\n            return ValidationPriority.LOW\n    \n    async def _update_learning_system(self, task_id: str, validation_result: Dict[str, Any]):\n        \"\"\"Update ML models based on human validation feedback\"\"\"\n        # This would integrate with your ML pipeline to retrain models\n        # based on human corrections\n        self.logger.info(f\"Updating learning system with validation for task {task_id}\")\n        \n        # Store validation feedback for model retraining\n        feedback_data = {\n            'task_id': task_id,\n            'validation_result': validation_result,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        await self.redis.lpush('model_feedback_queue', json.dumps(feedback_data))\n    \n    def get_validation_html(self) -> str:\n        \"\"\"Return HTML for validation interface\"\"\"\n        return \"\"\"\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <title>Document Validation Interface</title>\n            <style>\n                body { font-family: Arial, sans-serif; margin: 20px; }\n                .task-container { border: 1px solid #ccc; padding: 20px; margin: 10px 0; }\n                .field-item { margin: 10px 0; padding: 10px; background: #f9f9f9; }\n                .field-input { width: 100%; padding: 5px; }\n                .buttons { margin: 20px 0; }\n                button { margin: 5px; padding: 10px 20px; }\n                .approve { background: #4CAF50; color: white; }\n                .reject { background: #f44336; color: white; }\n                .correct { background: #2196F3; color: white; }\n            </style>\n        </head>\n        <body>\n            <h1>Document Validation Interface</h1>\n            <div id=\"status\">Connecting...</div>\n            <div id=\"task-container\"></div>\n            \n            <script>\n                const validatorId = 'validator_' + Math.random().toString(36).substr(2, 9);\n                const ws = new WebSocket(`ws://localhost:8000/ws/${validatorId}`);\n                let currentTask = null;\n                \n                ws.onmessage = function(event) {\n                    const data = JSON.parse(event.data);\n                    \n                    if (data.type === 'new_task') {\n                        displayTask(data.task);\n                        currentTask = data.task;\n                    } else if (data.type === 'no_tasks_available') {\n                        document.getElementById('status').textContent = 'No tasks available';\n                    } else if (data.type === 'task_confirmed') {\n                        document.getElementById('status').textContent = 'Task completed successfully';\n                        requestNextTask();\n                    }\n                };\n                \n                function displayTask(task) {\n                    const container = document.getElementById('task-container');\n                    const html = `\n                        <div class=\"task-container\">\n                            <h3>Document Type: ${task.document_type}</h3>\n                            <p>Confidence: ${task.confidence_score}</p>\n                            <div id=\"fields\">\n                                ${task.extracted_fields.map((field, index) => `\n                                    <div class=\"field-item\">\n                                        <label>${field.field_name} (conf: ${field.confidence}):</label>\n                                        <input type=\"text\" class=\"field-input\" id=\"field_${index}\" value=\"${field.value}\">\n                                    </div>\n                                `).join('')}\n                            </div>\n                            <div class=\"buttons\">\n                                <button class=\"approve\" onclick=\"approveTask()\">Approve</button>\n                                <button class=\"correct\" onclick=\"correctTask()\">Correct & Approve</button>\n                                <button class=\"reject\" onclick=\"rejectTask()\">Reject</button>\n                            </div>\n                            <textarea id=\"notes\" placeholder=\"Notes (optional)\" rows=\"3\" cols=\"50\"></textarea>\n                        </div>\n                    `;\n                    container.innerHTML = html;\n                    document.getElementById('status').textContent = 'Review the extracted fields';\n                }\n                \n                function approveTask() {\n                    submitTask('approved');\n                }\n                \n                function correctTask() {\n                    const correctedFields = [];\n                    currentTask.extracted_fields.forEach((field, index) => {\n                        const input = document.getElementById(`field_${index}`);\n                        correctedFields.push({\n                            field_name: field.field_name,\n                            value: input.value,\n                            confidence: 1.0,\n                            corrected: input.value !== field.value\n                        });\n                    });\n                    \n                    submitTask('approved', correctedFields);\n                }\n                \n                function rejectTask() {\n                    submitTask('rejected');\n                }\n                \n                function submitTask(status, correctedFields = null) {\n                    const notes = document.getElementById('notes').value;\n                    \n                    ws.send(JSON.stringify({\n                        type: 'task_completed',\n                        task_id: currentTask.task_id,\n                        status: status,\n                        corrected_fields: correctedFields,\n                        notes: notes\n                    }));\n                    \n                    document.getElementById('status').textContent = 'Submitting...';\n                }\n                \n                function requestNextTask() {\n                    ws.send(JSON.stringify({\n                        type: 'request_next_task'\n                    }));\n                }\n                \n                // Request first task when connected\n                ws.onopen = function() {\n                    document.getElementById('status').textContent = 'Connected';\n                    requestNextTask();\n                };\n            </script>\n        </body>\n        </html>\n        \"\"\"\n\n# Usage example\nif __name__ == '__main__':\n    import uvicorn\n    \n    # Initialize the validation system\n    validation_system = HumanInTheLoopValidationSystem()\n    \n    # Run the web server\n    uvicorn.run(validation_system.app, host=\"0.0.0.0\", port=8000)"
    }
  ]
}