{
  "slug": "real-time-data-pipeline",
  "title": "Real-Time Data Pipeline Platform",
  "description": "High-performance real-time data processing platform built with Apache Kafka, Apache Flink, and Kubernetes, capable of processing 100K+ events per second with sub-second latency for streaming analytics and ML workflows.",
  "longDescription": "Engineered a comprehensive real-time data processing platform that enables organizations to process, transform, and analyze streaming data at scale. This project addresses the critical need for low-latency data processing in modern applications, combining Brandon's distributed systems expertise with cutting-edge stream processing technologies.\n\nThe platform provides a unified framework for building real-time analytics, fraud detection, personalization engines, and ML feature pipelines. It emphasizes operational simplicity through automated scaling, self-healing capabilities, and comprehensive monitoring, while maintaining high performance through optimized data serialization, intelligent partitioning strategies, and efficient resource utilization.\n\nDeployed across multiple production environments, the platform consistently delivers sub-second processing latency while handling massive data volumes, making it ideal for use cases requiring immediate insights from streaming data.",
  "tags": ["Apache Kafka", "Apache Flink", "Kubernetes", "Streaming Analytics", "Python", "Scala", "Machine Learning"],
  "featured": true,
  "githubUrl": "https://github.com/brandonjredmond/real-time-data-pipeline",
  "demoUrl": "https://streaming-analytics-demo.brandonredmond.dev",
  "techStack": [
    {
      "category": "Stream Processing",
      "items": ["Apache Kafka", "Apache Flink", "Kafka Streams", "Apache Pulsar", "Redis Streams"]
    },
    {
      "category": "Data Storage",
      "items": ["Apache Cassandra", "ClickHouse", "PostgreSQL", "Amazon S3", "Apache Parquet"]
    },
    {
      "category": "Orchestration & Deployment",
      "items": ["Kubernetes", "Apache Airflow", "Helm", "Terraform", "Docker"]
    },
    {
      "category": "Monitoring & Observability",
      "items": ["Prometheus", "Grafana", "Jaeger", "Apache Kafka Manager", "Confluent Control Center"]
    }
  ],
  "features": [
    "Real-time stream processing with exactly-once semantics",
    "Auto-scaling based on throughput and latency metrics",
    "Schema evolution support with backward compatibility",
    "Built-in data quality validation and anomaly detection",
    "Flexible windowing operations for time-based aggregations",
    "Integration with ML pipelines for real-time feature engineering",
    "Multi-tenant architecture with resource isolation",
    "Comprehensive backpressure handling and circuit breaker patterns"
  ],
  "challenges": [
    "Achieving exactly-once processing semantics in distributed environment",
    "Optimizing memory usage for large state operations in Flink",
    "Designing efficient partitioning strategies for high-cardinality data",
    "Implementing flexible schema evolution without downtime",
    "Balancing throughput vs latency optimization across different workloads"
  ],
  "outcomes": [
    { "metric": "Processing Throughput", "value": "100K+ events/sec" },
    { "metric": "End-to-End Latency", "value": "< 500ms P99" },
    { "metric": "System Uptime", "value": "99.95%" },
    { "metric": "Data Quality Score", "value": "99.8%" },
    { "metric": "Cost Reduction", "value": "40% vs batch processing" }
  ],
  "educational": [
    "Stream processing fundamentals and windowing strategies",
    "Apache Kafka optimization techniques for high throughput",
    "Flink state management patterns and checkpointing strategies",
    "Real-time feature engineering for machine learning pipelines",
    "Data quality monitoring and alerting in streaming systems",
    "Performance tuning techniques for distributed stream processing"
  ],
  "codeSnippets": [
    {
      "title": "Real-Time Feature Engineering Pipeline",
      "language": "scala",
      "code": "import org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.time.Time\nimport org.apache.flink.streaming.api.windowing.windows.TimeWindow\nimport org.apache.flink.streaming.api.functions.windowing.WindowFunction\nimport org.apache.flink.util.Collector\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer\n\n// Case classes for data models\ncase class UserEvent(\n  userId: String,\n  eventType: String,\n  timestamp: Long,\n  properties: Map[String, String]\n)\n\ncase class UserFeatures(\n  userId: String,\n  windowStart: Long,\n  windowEnd: Long,\n  eventCount: Long,\n  uniqueEventTypes: Long,\n  avgTimeBetweenEvents: Double,\n  topEventType: String,\n  sessionDuration: Long\n)\n\n// Real-time feature engineering job\nobject UserFeatureEngineeringJob {\n  \n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    env.enableCheckpointing(5000) // Checkpoint every 5 seconds\n    \n    // Configure Kafka consumer\n    val kafkaProps = new Properties()\n    kafkaProps.setProperty(\"bootstrap.servers\", \"localhost:9092\")\n    kafkaProps.setProperty(\"group.id\", \"feature-engineering\")\n    \n    val userEventStream = env\n      .addSource(new FlinkKafkaConsumer[String](\n        \"user-events\",\n        new SimpleStringSchema(),\n        kafkaProps\n      ))\n      .map(parseUserEvent)\n      .filter(_.nonEmpty)\n      .map(_.get)\n      .assignTimestampsAndWatermarks(\n        WatermarkStrategy\n          .forBoundedOutOfOrderness(Duration.ofSeconds(10))\n          .withTimestampAssigner((event, _) => event.timestamp)\n      )\n    \n    // Create features using sliding windows\n    val userFeatures = userEventStream\n      .keyBy(_.userId)\n      .window(SlidingEventTimeWindows.of(Time.minutes(10), Time.minutes(1)))\n      .apply(new UserFeatureFunction())\n    \n    // Enrich features with user profile data\n    val enrichedFeatures = userFeatures\n      .connect(getUserProfileStream(env))\n      .keyBy(_._1, _._1) // Key both streams by userId\n      .process(new FeatureEnrichmentFunction())\n    \n    // Write features to feature store\n    enrichedFeatures\n      .addSink(new FlinkKafkaProducer[String](\n        \"user-features\",\n        (features: EnrichedUserFeatures) => new ProducerRecord[String, String](\n          \"user-features\",\n          features.userId,\n          features.toJson\n        ),\n        kafkaProps,\n        FlinkKafkaProducer.Semantic.EXACTLY_ONCE\n      ))\n    \n    // Execute the job\n    env.execute(\"User Feature Engineering Pipeline\")\n  }\n  \n  // Parse JSON events with error handling\n  def parseUserEvent(json: String): Option[UserEvent] = {\n    try {\n      // Simplified JSON parsing - use proper JSON library in production\n      val parsed = ujson.read(json)\n      Some(UserEvent(\n        userId = parsed(\"userId\").str,\n        eventType = parsed(\"eventType\").str,\n        timestamp = parsed(\"timestamp\").num.toLong,\n        properties = parsed(\"properties\").obj.toMap.mapValues(_.str)\n      ))\n    } catch {\n      case _: Exception => None\n    }\n  }\n}\n\n// Window function to compute features\nclass UserFeatureFunction extends WindowFunction[UserEvent, UserFeatures, String, TimeWindow] {\n  \n  override def apply(\n    userId: String,\n    window: TimeWindow,\n    events: Iterable[UserEvent],\n    out: Collector[UserFeatures]\n  ): Unit = {\n    \n    val eventList = events.toList.sortBy(_.timestamp)\n    \n    if (eventList.nonEmpty) {\n      val eventCount = eventList.length\n      val uniqueEventTypes = eventList.map(_.eventType).toSet.size\n      \n      // Calculate average time between events\n      val avgTimeBetweenEvents = if (eventList.length > 1) {\n        val timeDiffs = eventList.zip(eventList.tail).map {\n          case (prev, curr) => curr.timestamp - prev.timestamp\n        }\n        timeDiffs.sum.toDouble / timeDiffs.length\n      } else 0.0\n      \n      // Find most frequent event type\n      val topEventType = eventList\n        .groupBy(_.eventType)\n        .mapValues(_.length)\n        .maxBy(_._2)\n        ._1\n      \n      // Calculate session duration\n      val sessionDuration = eventList.last.timestamp - eventList.head.timestamp\n      \n      val features = UserFeatures(\n        userId = userId,\n        windowStart = window.getStart,\n        windowEnd = window.getEnd,\n        eventCount = eventCount,\n        uniqueEventTypes = uniqueEventTypes,\n        avgTimeBetweenEvents = avgTimeBetweenEvents,\n        topEventType = topEventType,\n        sessionDuration = sessionDuration\n      )\n      \n      out.collect(features)\n    }\n  }\n}"
    },
    {
      "title": "Advanced Stream Processing with State Management",
      "language": "python",
      "code": "from pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer\nfrom pyflink.datastream.formats.json import JsonRowSerializationSchema\nfrom pyflink.datastream.state import ValueStateDescriptor, ListStateDescriptor\nfrom pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext\nfrom pyflink.common.typeinfo import Types\nfrom pyflink.common.time import Time\nimport json\nfrom typing import Dict, List, Optional\nfrom datetime import datetime, timedelta\n\nclass FraudDetectionFunction(KeyedProcessFunction):\n    \"\"\"\n    Advanced fraud detection using stateful stream processing\n    Detects suspicious patterns across multiple time windows\n    \"\"\"\n    \n    def __init__(self):\n        # State descriptors\n        self.transaction_count_state = None\n        self.transaction_history_state = None\n        self.velocity_state = None\n        self.fraud_score_state = None\n        \n    def open(self, runtime_context: RuntimeContext):\n        # Initialize state descriptors\n        self.transaction_count_state = runtime_context.get_state(\n            ValueStateDescriptor(\"transaction_count\", Types.INT())\n        )\n        \n        self.transaction_history_state = runtime_context.get_list_state(\n            ListStateDescriptor(\"transaction_history\", Types.PICKLED_BYTE_ARRAY())\n        )\n        \n        self.velocity_state = runtime_context.get_state(\n            ValueStateDescriptor(\"velocity\", Types.DOUBLE())\n        )\n        \n        self.fraud_score_state = runtime_context.get_state(\n            ValueStateDescriptor(\"fraud_score\", Types.DOUBLE())\n        )\n    \n    def process_element(self, transaction, ctx):\n        # Parse transaction data\n        user_id = transaction['user_id']\n        amount = float(transaction['amount'])\n        timestamp = transaction['timestamp']\n        merchant = transaction['merchant']\n        location = transaction['location']\n        \n        # Get current state\n        current_count = self.transaction_count_state.value() or 0\n        current_history = list(self.transaction_history_state.get())\n        current_velocity = self.velocity_state.value() or 0.0\n        current_fraud_score = self.fraud_score_state.value() or 0.0\n        \n        # Clean old transactions (keep only last 24 hours)\n        cutoff_time = timestamp - (24 * 3600 * 1000)  # 24 hours in milliseconds\n        recent_history = [\n            tx for tx in current_history \n            if tx['timestamp'] > cutoff_time\n        ]\n        \n        # Add current transaction to history\n        recent_history.append({\n            'amount': amount,\n            'timestamp': timestamp,\n            'merchant': merchant,\n            'location': location\n        })\n        \n        # Calculate fraud indicators\n        fraud_indicators = self.calculate_fraud_indicators(\n            recent_history, amount, merchant, location\n        )\n        \n        # Update fraud score using weighted average\n        new_fraud_score = self.update_fraud_score(\n            current_fraud_score, fraud_indicators\n        )\n        \n        # Update velocity (transactions per hour)\n        new_velocity = self.calculate_velocity(recent_history)\n        \n        # Update state\n        self.transaction_count_state.update(len(recent_history))\n        self.transaction_history_state.clear()\n        for tx in recent_history:\n            self.transaction_history_state.add(tx)\n        self.velocity_state.update(new_velocity)\n        self.fraud_score_state.update(new_fraud_score)\n        \n        # Generate alert if fraud score exceeds threshold\n        if new_fraud_score > 0.7:  # 70% fraud probability threshold\n            alert = {\n                'user_id': user_id,\n                'transaction': transaction,\n                'fraud_score': new_fraud_score,\n                'indicators': fraud_indicators,\n                'alert_type': 'HIGH_RISK_TRANSACTION',\n                'timestamp': timestamp\n            }\n            yield alert\n        \n        # Set timer for state cleanup (remove old state after 7 days)\n        cleanup_time = timestamp + (7 * 24 * 3600 * 1000)\n        ctx.timer_service().register_event_time_timer(cleanup_time)\n    \n    def calculate_fraud_indicators(self, history: List[Dict], \n                                 current_amount: float, \n                                 current_merchant: str, \n                                 current_location: str) -> Dict[str, float]:\n        \"\"\"\n        Calculate various fraud indicators\n        \"\"\"\n        indicators = {}\n        \n        if len(history) < 2:\n            return {'insufficient_data': 1.0}\n        \n        amounts = [tx['amount'] for tx in history]\n        merchants = [tx['merchant'] for tx in history]\n        locations = [tx['location'] for tx in history]\n        timestamps = [tx['timestamp'] for tx in history]\n        \n        # Amount-based indicators\n        avg_amount = sum(amounts) / len(amounts)\n        amount_deviation = abs(current_amount - avg_amount) / avg_amount if avg_amount > 0 else 0\n        indicators['amount_deviation'] = min(amount_deviation, 1.0)\n        \n        # Velocity indicators\n        recent_1h = [tx for tx in history if tx['timestamp'] > (timestamps[-1] - 3600000)]\n        indicators['velocity_1h'] = min(len(recent_1h) / 10.0, 1.0)  # Normalize to 0-1\n        \n        # Location diversity (potential account takeover)\n        unique_locations = len(set(locations))\n        indicators['location_diversity'] = min(unique_locations / 5.0, 1.0)\n        \n        # Merchant diversity (unusual spending pattern)\n        unique_merchants = len(set(merchants))\n        indicators['merchant_diversity'] = min(unique_merchants / 10.0, 1.0)\n        \n        # Time-based patterns (unusual hours)\n        current_hour = datetime.fromtimestamp(timestamps[-1] / 1000).hour\n        if current_hour < 6 or current_hour > 22:  # Late night transactions\n            indicators['unusual_time'] = 0.8\n        else:\n            indicators['unusual_time'] = 0.1\n        \n        # Round amounts (common in fraud)\n        if current_amount == int(current_amount) and current_amount >= 100:\n            indicators['round_amount'] = 0.6\n        else:\n            indicators['round_amount'] = 0.1\n        \n        return indicators\n    \n    def update_fraud_score(self, current_score: float, \n                          indicators: Dict[str, float]) -> float:\n        \"\"\"\n        Update fraud score using exponential weighted average\n        \"\"\"\n        if 'insufficient_data' in indicators:\n            return current_score\n        \n        # Calculate weighted indicator score\n        weights = {\n            'amount_deviation': 0.3,\n            'velocity_1h': 0.25,\n            'location_diversity': 0.2,\n            'merchant_diversity': 0.1,\n            'unusual_time': 0.1,\n            'round_amount': 0.05\n        }\n        \n        indicator_score = sum(\n            indicators.get(key, 0) * weight \n            for key, weight in weights.items()\n        )\n        \n        # Exponential weighted average (alpha = 0.3)\n        alpha = 0.3\n        new_score = alpha * indicator_score + (1 - alpha) * current_score\n        \n        return min(max(new_score, 0.0), 1.0)  # Clamp between 0 and 1\n    \n    def calculate_velocity(self, history: List[Dict]) -> float:\n        \"\"\"\n        Calculate transaction velocity (transactions per hour)\n        \"\"\"\n        if len(history) < 2:\n            return 0.0\n        \n        # Calculate time span in hours\n        time_span_ms = history[-1]['timestamp'] - history[0]['timestamp']\n        time_span_hours = time_span_ms / (1000 * 3600)\n        \n        if time_span_hours == 0:\n            return 0.0\n        \n        return len(history) / time_span_hours\n    \n    def on_timer(self, timestamp, ctx):\n        # Cleanup old state\n        self.transaction_history_state.clear()\n        self.transaction_count_state.clear()\n        self.velocity_state.clear()\n        self.fraud_score_state.clear()\n\n# Main execution\ndef create_fraud_detection_pipeline():\n    env = StreamExecutionEnvironment.get_execution_environment()\n    env.enable_checkpointing(5000)  # Checkpoint every 5 seconds\n    \n    # Configure Kafka source\n    kafka_consumer = FlinkKafkaConsumer(\n        topics=['transactions'],\n        deserialization_schema=JsonRowDeserializationSchema.builder()\n            .type_info(Types.ROW_NAMED(\n                ['user_id', 'amount', 'timestamp', 'merchant', 'location'],\n                [Types.STRING(), Types.DOUBLE(), Types.LONG(), Types.STRING(), Types.STRING()]\n            ))\n            .build(),\n        properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'fraud-detection'}\n    )\n    \n    # Create processing pipeline\n    alerts = env.add_source(kafka_consumer) \\\n        .key_by(lambda x: x['user_id']) \\\n        .process(FraudDetectionFunction())\n    \n    # Send alerts to downstream systems\n    alerts.add_sink(\n        FlinkKafkaProducer(\n            topic='fraud-alerts',\n            serialization_schema=JsonRowSerializationSchema.builder()\n                .with_type_info(Types.ROW_NAMED(\n                    ['user_id', 'fraud_score', 'alert_type', 'timestamp'],\n                    [Types.STRING(), Types.DOUBLE(), Types.STRING(), Types.LONG()]\n                ))\n                .build(),\n            producer_config={'bootstrap.servers': 'localhost:9092'}\n        )\n    )\n    \n    env.execute('Real-time Fraud Detection Pipeline')\n\nif __name__ == '__main__':\n    create_fraud_detection_pipeline()"
    }
  ]
}