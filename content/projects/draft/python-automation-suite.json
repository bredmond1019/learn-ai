{
  "slug": "python-automation-suite",
  "title": "Python Automation Suite",
  "description": "Comprehensive Python automation framework with 50+ pre-built modules for web scraping, API integration, file processing, and workflow orchestration, reducing manual work by 80% across multiple organizations.",
  "longDescription": "Developed a comprehensive Python automation suite that addresses common business automation needs across various industries. This project leverages Brandon's teaching background to create educational, well-documented automation solutions that are both powerful for experts and accessible to newcomers to automation.\n\nThe suite includes modules for web scraping, API integrations, document processing, email automation, database operations, and cloud service integrations. Each module follows consistent patterns and includes extensive documentation, making it easy for teams to adopt and extend the automation capabilities.\n\nThe framework emphasizes reliability through built-in error handling, retry mechanisms, and comprehensive logging. It has been successfully deployed across multiple organizations, from small businesses automating invoicing to large enterprises streamlining data processing workflows.",
  "tags": ["Python", "Automation", "Web Scraping", "API Integration", "Workflow Orchestration", "Data Processing"],
  "featured": false,
  "githubUrl": "https://github.com/brandonjredmond/python-automation-suite",
  "demoUrl": "https://automation-demo.brandonredmond.dev",
  "techStack": [
    {
      "category": "Core Framework",
      "items": ["Python 3.11", "AsyncIO", "Pydantic", "Click", "Rich", "Loguru"]
    },
    {
      "category": "Web & API",
      "items": ["aiohttp", "httpx", "BeautifulSoup4", "Selenium", "Playwright", "FastAPI"]
    },
    {
      "category": "Data Processing",
      "items": ["Pandas", "Polars", "SQLAlchemy", "Alembic", "PyArrow", "Openpyxl"]
    },
    {
      "category": "Cloud & Services",
      "items": ["boto3", "google-cloud", "azure-sdk", "Docker", "Redis", "Celery"]
    }
  ],
  "features": [
    "50+ pre-built automation modules covering common business tasks",
    "Visual workflow designer for non-technical users",
    "Robust error handling with automatic retries and fallbacks",
    "Comprehensive logging and monitoring capabilities",
    "Plugin architecture for easy extension and customization",
    "Web-based dashboard for monitoring automation status",
    "Integration with popular services (Slack, Teams, Email, etc.)",
    "Scheduling system with cron-like expressions and dependencies"
  ],
  "challenges": [
    "Creating a consistent API across diverse automation domains",
    "Handling rate limiting and anti-bot measures in web scraping",
    "Designing fault-tolerant automation workflows",
    "Balancing performance with resource usage in concurrent operations",
    "Creating intuitive configuration for complex automation scenarios"
  ],
  "outcomes": [
    { "metric": "Manual Work Reduction", "value": "80% average" },
    { "metric": "Organizations Using", "value": "15+" },
    { "metric": "Automation Modules", "value": "50+" },
    { "metric": "Processing Speed", "value": "10x faster than manual" },
    { "metric": "Error Rate Reduction", "value": "95%" }
  ],
  "educational": [
    "Asynchronous programming patterns for high-performance automation",
    "Web scraping techniques and anti-detection strategies",
    "API design principles for automation frameworks",
    "Error handling and retry strategies in automated systems",
    "Workflow orchestration patterns and dependency management",
    "Performance optimization techniques for data processing"
  ],
  "codeSnippets": [
    {
      "title": "Web Scraping Module with Anti-Detection",
      "language": "python",
      "code": "import asyncio\nimport random\nfrom typing import List, Dict, Optional, Any\nfrom dataclasses import dataclass, field\nfrom playwright.async_api import async_playwright, Browser, BrowserContext, Page\nfrom fake_useragent import UserAgent\nimport time\nfrom urllib.parse import urljoin, urlparse\nimport json\n\n@dataclass\nclass ScrapingConfig:\n    \"\"\"Configuration for web scraping operations\"\"\"\n    headless: bool = True\n    timeout: int = 30000\n    delay_range: tuple = (1, 3)\n    max_retries: int = 3\n    respect_robots_txt: bool = True\n    user_agents: List[str] = field(default_factory=lambda: [])\n    proxy_rotation: bool = False\n    proxy_list: List[str] = field(default_factory=lambda: [])\n    \nclass AntiDetectionScraper:\n    \"\"\"Advanced web scraper with anti-detection capabilities\"\"\"\n    \n    def __init__(self, config: ScrapingConfig):\n        self.config = config\n        self.ua = UserAgent()\n        self.browser: Optional[Browser] = None\n        self.context: Optional[BrowserContext] = None\n        self.session_cookies: Dict[str, Any] = {}\n        \n    async def __aenter__(self):\n        await self.start_browser()\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close_browser()\n        \n    async def start_browser(self):\n        \"\"\"Initialize browser with stealth settings\"\"\"\n        self.playwright = await async_playwright().start()\n        \n        # Launch browser with stealth settings\n        self.browser = await self.playwright.chromium.launch(\n            headless=self.config.headless,\n            args=[\n                '--no-sandbox',\n                '--disable-setuid-sandbox',\n                '--disable-dev-shm-usage',\n                '--disable-accelerated-2d-canvas',\n                '--no-first-run',\n                '--no-zygote',\n                '--single-process',\n                '--disable-gpu',\n                '--disable-background-timer-throttling',\n                '--disable-backgrounding-occluded-windows',\n                '--disable-renderer-backgrounding'\n            ]\n        )\n        \n        # Create context with anti-detection settings\n        self.context = await self.browser.new_context(\n            user_agent=self.get_random_user_agent(),\n            viewport={'width': 1920, 'height': 1080},\n            locale='en-US',\n            timezone_id='America/New_York'\n        )\n        \n        # Add stealth scripts\n        await self.context.add_init_script(\"\"\"\n            // Remove webdriver property\n            Object.defineProperty(navigator, 'webdriver', {\n                get: () => undefined,\n            });\n            \n            // Mock chrome property\n            window.chrome = {\n                runtime: {},\n            };\n            \n            // Mock permissions\n            const originalQuery = window.navigator.permissions.query;\n            window.navigator.permissions.query = (parameters) => (\n                parameters.name === 'notifications' ?\n                    Promise.resolve({ state: Notification.permission }) :\n                    originalQuery(parameters)\n            );\n            \n            // Mock plugins\n            Object.defineProperty(navigator, 'plugins', {\n                get: () => [1, 2, 3, 4, 5],\n            });\n            \n            // Mock languages\n            Object.defineProperty(navigator, 'languages', {\n                get: () => ['en-US', 'en'],\n            });\n        \"\"\")\n        \n    async def close_browser(self):\n        \"\"\"Clean up browser resources\"\"\"\n        if self.context:\n            await self.context.close()\n        if self.browser:\n            await self.browser.close()\n        if hasattr(self, 'playwright'):\n            await self.playwright.stop()\n            \n    def get_random_user_agent(self) -> str:\n        \"\"\"Get random user agent for request\"\"\"\n        if self.config.user_agents:\n            return random.choice(self.config.user_agents)\n        return self.ua.random\n    \n    async def scrape_page(self, url: str, selectors: Dict[str, str], \n                         wait_for: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Scrape a single page with anti-detection measures\"\"\"\n        \n        for attempt in range(self.config.max_retries):\n            try:\n                page = await self.context.new_page()\n                \n                # Set random viewport\n                await page.set_viewport_size({\n                    'width': random.randint(1200, 1920),\n                    'height': random.randint(800, 1080)\n                })\n                \n                # Navigate with random delay\n                await self.human_like_delay()\n                response = await page.goto(url, timeout=self.config.timeout)\n                \n                if response.status >= 400:\n                    raise Exception(f\"HTTP {response.status} error\")\n                \n                # Wait for specific element if specified\n                if wait_for:\n                    await page.wait_for_selector(wait_for, timeout=self.config.timeout)\n                \n                # Random scroll to simulate human behavior\n                await self.simulate_human_behavior(page)\n                \n                # Extract data using selectors\n                data = {}\n                for key, selector in selectors.items():\n                    try:\n                        if selector.startswith('attr:'):\n                            # Extract attribute value\n                            attr_name = selector.split(':', 1)[1]\n                            element = await page.query_selector(key)\n                            if element:\n                                data[key] = await element.get_attribute(attr_name)\n                        elif selector == 'text':\n                            # Extract text content\n                            element = await page.query_selector(key)\n                            if element:\n                                data[key] = await element.text_content()\n                        elif selector == 'html':\n                            # Extract HTML content\n                            element = await page.query_selector(key)\n                            if element:\n                                data[key] = await element.inner_html()\n                        else:\n                            # Default to text content\n                            elements = await page.query_selector_all(selector)\n                            if elements:\n                                if len(elements) == 1:\n                                    data[key] = await elements[0].text_content()\n                                else:\n                                    data[key] = [await el.text_content() for el in elements]\n                    except Exception as e:\n                        print(f\"Error extracting {key}: {e}\")\n                        data[key] = None\n                \n                await page.close()\n                return data\n                \n            except Exception as e:\n                print(f\"Attempt {attempt + 1} failed: {e}\")\n                if attempt < self.config.max_retries - 1:\n                    delay = random.uniform(2, 5) * (attempt + 1)\n                    await asyncio.sleep(delay)\n                else:\n                    raise e\n    \n    async def scrape_multiple_pages(self, urls: List[str], \n                                  selectors: Dict[str, str],\n                                  concurrent: int = 3) -> List[Dict[str, Any]]:\n        \"\"\"Scrape multiple pages concurrently with rate limiting\"\"\"\n        \n        semaphore = asyncio.Semaphore(concurrent)\n        \n        async def scrape_with_semaphore(url):\n            async with semaphore:\n                try:\n                    return await self.scrape_page(url, selectors)\n                except Exception as e:\n                    print(f\"Failed to scrape {url}: {e}\")\n                    return {'url': url, 'error': str(e)}\n        \n        # Process URLs in batches to avoid overwhelming the server\n        results = []\n        batch_size = min(concurrent, 10)\n        \n        for i in range(0, len(urls), batch_size):\n            batch = urls[i:i + batch_size]\n            batch_results = await asyncio.gather(\n                *[scrape_with_semaphore(url) for url in batch],\n                return_exceptions=True\n            )\n            results.extend(batch_results)\n            \n            # Add delay between batches\n            if i + batch_size < len(urls):\n                await asyncio.sleep(random.uniform(1, 3))\n        \n        return results\n    \n    async def simulate_human_behavior(self, page: Page):\n        \"\"\"Simulate human-like behavior on the page\"\"\"\n        # Random scrolling\n        viewport_height = await page.evaluate('window.innerHeight')\n        scroll_distance = random.randint(100, viewport_height // 2)\n        \n        await page.mouse.wheel(0, scroll_distance)\n        await asyncio.sleep(random.uniform(0.5, 1.5))\n        \n        # Random scroll back up\n        if random.random() < 0.3:\n            await page.mouse.wheel(0, -scroll_distance // 2)\n            await asyncio.sleep(random.uniform(0.3, 0.8))\n        \n        # Random mouse movements\n        for _ in range(random.randint(1, 3)):\n            x = random.randint(100, 1000)\n            y = random.randint(100, 600)\n            await page.mouse.move(x, y)\n            await asyncio.sleep(random.uniform(0.1, 0.3))\n    \n    async def human_like_delay(self):\n        \"\"\"Add human-like delay between requests\"\"\"\n        min_delay, max_delay = self.config.delay_range\n        delay = random.uniform(min_delay, max_delay)\n        await asyncio.sleep(delay)\n\n# Usage example\nasync def scrape_example():\n    config = ScrapingConfig(\n        headless=True,\n        delay_range=(2, 4),\n        max_retries=3\n    )\n    \n    selectors = {\n        'title': 'h1',\n        'price': '.price',\n        'description': '.description',\n        'image_url': 'img.main-image:attr:src'\n    }\n    \n    urls = [\n        'https://example.com/product/1',\n        'https://example.com/product/2',\n        'https://example.com/product/3'\n    ]\n    \n    async with AntiDetectionScraper(config) as scraper:\n        results = await scraper.scrape_multiple_pages(urls, selectors, concurrent=2)\n        \n        for result in results:\n            print(json.dumps(result, indent=2))\n\nif __name__ == '__main__':\n    asyncio.run(scrape_example())"
    },
    {
      "title": "Workflow Orchestration Engine",
      "language": "python",
      "code": "import asyncio\nfrom typing import Dict, List, Any, Optional, Callable, Union\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport json\nfrom datetime import datetime, timedelta\nimport logging\nfrom abc import ABC, abstractmethod\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n    CANCELLED = \"cancelled\"\n\nclass WorkflowStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n@dataclass\nclass TaskResult:\n    task_id: str\n    status: TaskStatus\n    result: Any = None\n    error: Optional[str] = None\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n    duration: Optional[float] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            'task_id': self.task_id,\n            'status': self.status.value,\n            'result': self.result,\n            'error': self.error,\n            'start_time': self.start_time.isoformat() if self.start_time else None,\n            'end_time': self.end_time.isoformat() if self.end_time else None,\n            'duration': self.duration\n        }\n\nclass BaseTask(ABC):\n    \"\"\"Base class for all automation tasks\"\"\"\n    \n    def __init__(self, task_id: str, name: str, description: str = \"\"):\n        self.task_id = task_id\n        self.name = name\n        self.description = description\n        self.dependencies: List[str] = []\n        self.timeout: Optional[float] = None\n        self.retry_count: int = 0\n        self.max_retries: int = 3\n        self.retry_delay: float = 1.0\n        self.condition: Optional[Callable] = None\n        \n    @abstractmethod\n    async def execute(self, context: Dict[str, Any]) -> Any:\n        \"\"\"Execute the task with given context\"\"\"\n        pass\n    \n    def add_dependency(self, task_id: str):\n        \"\"\"Add a task dependency\"\"\"\n        if task_id not in self.dependencies:\n            self.dependencies.append(task_id)\n            \n    def set_condition(self, condition: Callable[[Dict[str, Any]], bool]):\n        \"\"\"Set a condition for task execution\"\"\"\n        self.condition = condition\n\nclass FunctionTask(BaseTask):\n    \"\"\"Task that executes a function\"\"\"\n    \n    def __init__(self, task_id: str, name: str, func: Callable, \n                 args: tuple = (), kwargs: Dict[str, Any] = None, \n                 description: str = \"\"):\n        super().__init__(task_id, name, description)\n        self.func = func\n        self.args = args\n        self.kwargs = kwargs or {}\n        \n    async def execute(self, context: Dict[str, Any]) -> Any:\n        # Merge context with kwargs\n        merged_kwargs = {**self.kwargs, **context}\n        \n        # Execute function (handle both sync and async)\n        if asyncio.iscoroutinefunction(self.func):\n            return await self.func(*self.args, **merged_kwargs)\n        else:\n            # Run sync function in thread pool\n            loop = asyncio.get_event_loop()\n            with ThreadPoolExecutor() as executor:\n                return await loop.run_in_executor(\n                    executor, \n                    lambda: self.func(*self.args, **merged_kwargs)\n                )\n\nclass HTTPRequestTask(BaseTask):\n    \"\"\"Task for making HTTP requests\"\"\"\n    \n    def __init__(self, task_id: str, name: str, method: str, url: str,\n                 headers: Dict[str, str] = None, params: Dict[str, Any] = None,\n                 json_data: Dict[str, Any] = None, description: str = \"\"):\n        super().__init__(task_id, name, description)\n        self.method = method.upper()\n        self.url = url\n        self.headers = headers or {}\n        self.params = params or {}\n        self.json_data = json_data\n        \n    async def execute(self, context: Dict[str, Any]) -> Any:\n        import aiohttp\n        \n        # Template URL with context values\n        url = self.url.format(**context)\n        \n        # Template headers, params, and json_data with context\n        headers = {k: str(v).format(**context) for k, v in self.headers.items()}\n        params = {k: str(v).format(**context) for k, v in self.params.items()}\n        \n        json_data = None\n        if self.json_data:\n            json_data = json.loads(json.dumps(self.json_data).format(**context))\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.request(\n                method=self.method,\n                url=url,\n                headers=headers,\n                params=params,\n                json=json_data\n            ) as response:\n                response.raise_for_status()\n                \n                if response.content_type == 'application/json':\n                    return await response.json()\n                else:\n                    return await response.text()\n\nclass WorkflowEngine:\n    \"\"\"Orchestrates and executes workflows\"\"\"\n    \n    def __init__(self):\n        self.tasks: Dict[str, BaseTask] = {}\n        self.workflows: Dict[str, 'Workflow'] = {}\n        self.logger = logging.getLogger(__name__)\n        \n    def register_task(self, task: BaseTask):\n        \"\"\"Register a task with the engine\"\"\"\n        self.tasks[task.task_id] = task\n        \n    def create_workflow(self, workflow_id: str, name: str, \n                       description: str = \"\") -> 'Workflow':\n        \"\"\"Create a new workflow\"\"\"\n        workflow = Workflow(workflow_id, name, description, self)\n        self.workflows[workflow_id] = workflow\n        return workflow\n        \n    async def execute_workflow(self, workflow_id: str, \n                             context: Dict[str, Any] = None) -> Dict[str, TaskResult]:\n        \"\"\"Execute a workflow by ID\"\"\"\n        if workflow_id not in self.workflows:\n            raise ValueError(f\"Workflow {workflow_id} not found\")\n            \n        workflow = self.workflows[workflow_id]\n        return await workflow.execute(context or {})\n\nclass Workflow:\n    \"\"\"Represents a workflow of tasks\"\"\"\n    \n    def __init__(self, workflow_id: str, name: str, description: str, \n                 engine: WorkflowEngine):\n        self.workflow_id = workflow_id\n        self.name = name\n        self.description = description\n        self.engine = engine\n        self.task_ids: List[str] = []\n        self.status = WorkflowStatus.PENDING\n        self.start_time: Optional[datetime] = None\n        self.end_time: Optional[datetime] = None\n        self.results: Dict[str, TaskResult] = {}\n        \n    def add_task(self, task: BaseTask):\n        \"\"\"Add a task to the workflow\"\"\"\n        self.engine.register_task(task)\n        if task.task_id not in self.task_ids:\n            self.task_ids.append(task.task_id)\n            \n    async def execute(self, context: Dict[str, Any]) -> Dict[str, TaskResult]:\n        \"\"\"Execute the workflow\"\"\"\n        self.status = WorkflowStatus.RUNNING\n        self.start_time = datetime.now()\n        self.results = {}\n        \n        try:\n            # Build dependency graph\n            dependency_graph = self._build_dependency_graph()\n            \n            # Execute tasks in topological order\n            executed_tasks = set()\n            \n            while len(executed_tasks) < len(self.task_ids):\n                # Find tasks ready to execute (no pending dependencies)\n                ready_tasks = [\n                    task_id for task_id in self.task_ids\n                    if task_id not in executed_tasks\n                    and all(dep in executed_tasks for dep in dependency_graph.get(task_id, []))\n                ]\n                \n                if not ready_tasks:\n                    # Check for circular dependencies\n                    remaining_tasks = set(self.task_ids) - executed_tasks\n                    raise ValueError(f\"Circular dependency detected in tasks: {remaining_tasks}\")\n                \n                # Execute ready tasks in parallel\n                task_futures = []\n                for task_id in ready_tasks:\n                    task = self.engine.tasks[task_id]\n                    future = asyncio.create_task(self._execute_task(task, context))\n                    task_futures.append((task_id, future))\n                \n                # Wait for all ready tasks to complete\n                for task_id, future in task_futures:\n                    try:\n                        result = await future\n                        self.results[task_id] = result\n                        executed_tasks.add(task_id)\n                        \n                        # Add task result to context for subsequent tasks\n                        if result.result is not None:\n                            context[f\"{task_id}_result\"] = result.result\n                            \n                    except Exception as e:\n                        error_result = TaskResult(\n                            task_id=task_id,\n                            status=TaskStatus.FAILED,\n                            error=str(e),\n                            start_time=datetime.now(),\n                            end_time=datetime.now()\n                        )\n                        self.results[task_id] = error_result\n                        executed_tasks.add(task_id)\n                        \n                        # Stop workflow on task failure (configurable)\n                        self.engine.logger.error(f\"Task {task_id} failed: {e}\")\n                        self.status = WorkflowStatus.FAILED\n                        return self.results\n            \n            self.status = WorkflowStatus.COMPLETED\n            \n        except Exception as e:\n            self.engine.logger.error(f\"Workflow {self.workflow_id} failed: {e}\")\n            self.status = WorkflowStatus.FAILED\n            \n        finally:\n            self.end_time = datetime.now()\n            \n        return self.results\n    \n    async def _execute_task(self, task: BaseTask, context: Dict[str, Any]) -> TaskResult:\n        \"\"\"Execute a single task with retries and error handling\"\"\"\n        start_time = datetime.now()\n        \n        # Check task condition\n        if task.condition and not task.condition(context):\n            return TaskResult(\n                task_id=task.task_id,\n                status=TaskStatus.SKIPPED,\n                start_time=start_time,\n                end_time=datetime.now()\n            )\n        \n        # Execute task with retries\n        last_error = None\n        for attempt in range(task.max_retries + 1):\n            try:\n                if task.timeout:\n                    result = await asyncio.wait_for(\n                        task.execute(context), \n                        timeout=task.timeout\n                    )\n                else:\n                    result = await task.execute(context)\n                \n                end_time = datetime.now()\n                duration = (end_time - start_time).total_seconds()\n                \n                return TaskResult(\n                    task_id=task.task_id,\n                    status=TaskStatus.COMPLETED,\n                    result=result,\n                    start_time=start_time,\n                    end_time=end_time,\n                    duration=duration\n                )\n                \n            except Exception as e:\n                last_error = e\n                self.engine.logger.warning(\n                    f\"Task {task.task_id} attempt {attempt + 1} failed: {e}\"\n                )\n                \n                if attempt < task.max_retries:\n                    await asyncio.sleep(task.retry_delay * (2 ** attempt))\n        \n        # All retries failed\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n        \n        return TaskResult(\n            task_id=task.task_id,\n            status=TaskStatus.FAILED,\n            error=str(last_error),\n            start_time=start_time,\n            end_time=end_time,\n            duration=duration\n        )\n    \n    def _build_dependency_graph(self) -> Dict[str, List[str]]:\n        \"\"\"Build dependency graph for task execution order\"\"\"\n        graph = {}\n        for task_id in self.task_ids:\n            task = self.engine.tasks[task_id]\n            graph[task_id] = task.dependencies\n        return graph\n\n# Usage example\nasync def example_workflow():\n    engine = WorkflowEngine()\n    \n    # Create tasks\n    fetch_data_task = HTTPRequestTask(\n        task_id=\"fetch_data\",\n        name=\"Fetch Data\",\n        method=\"GET\",\n        url=\"https://api.example.com/data\",\n        headers={\"Authorization\": \"Bearer {api_token}\"}\n    )\n    \n    async def process_data(data, **kwargs):\n        # Process the fetched data\n        return {\"processed_count\": len(data) if data else 0}\n    \n    process_task = FunctionTask(\n        task_id=\"process_data\",\n        name=\"Process Data\",\n        func=process_data,\n        kwargs={\"data\": \"{fetch_data_result}\"}\n    )\n    process_task.add_dependency(\"fetch_data\")\n    \n    # Create workflow\n    workflow = engine.create_workflow(\n        \"data_processing\",\n        \"Data Processing Workflow\",\n        \"Fetch and process data from API\"\n    )\n    \n    workflow.add_task(fetch_data_task)\n    workflow.add_task(process_task)\n    \n    # Execute workflow\n    context = {\"api_token\": \"your-api-token\"}\n    results = await engine.execute_workflow(\"data_processing\", context)\n    \n    # Print results\n    for task_id, result in results.items():\n        print(f\"Task {task_id}: {result.status.value}\")\n        if result.result:\n            print(f\"  Result: {result.result}\")\n        if result.error:\n            print(f\"  Error: {result.error}\")\n\nif __name__ == '__main__':\n    asyncio.run(example_workflow())"
    }
  ]
}