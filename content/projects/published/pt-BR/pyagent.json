{
  "slug": "pyagent",
  "title": "PyAgent - Biblioteca de Agentes Inteligentes",
  "description": "Biblioteca Python abrangente para construir sistemas de agentes IA prontos para produção com padrões padronizados, suporte ao protocolo MCP e ecossistema extenso de integração para implantações empresariais.",
  "longDescription": "PyAgent representa uma mudança fundamental em como sistemas de agentes inteligentes são construídos e implantados em escala. Esta biblioteca abrangente fornece componentes de nível empresarial para criar agentes alimentados por IA com gerenciamento padronizado de ciclo de vida, orquestração de fluxo de trabalho e extensas capacidades de integração em principais plataformas e serviços.\n\nA biblioteca demonstra domínio de arquitetura de sistemas distribuídos através de seu design orientado a eventos, apresentando um motor completo de fluxo de trabalho baseado em DAG com persistência de estado, capacidades de depuração e roteamento de eventos entre serviços. A implementação do Model Context Protocol (MCP) permite comunicação padronizada entre agentes, enquanto a arquitetura modular suporta tudo, desde tarefas simples de automação até sistemas multi-agente complexos coordenando entre diferentes domínios.\n\nAlém de sua sofisticação técnica, PyAgent serve tanto como ferramenta de produção quanto como recurso educacional, com documentação abrangente, exemplos e um ecossistema crescente de agentes pré-construídos. O impacto da biblioteca se estende por indústrias, permitindo que desenvolvedores prototipem e implantem rapidamente soluções de automação inteligente mantendo confiabilidade e observabilidade de nível empresarial.",
  "tags": ["Python", "Agentes IA", "Protocolo MCP", "Event Sourcing", "Motor de Fluxo de Trabalho", "Sistemas Distribuídos"],
  "featured": true,
  "icon": "Bot",
  "isPrivate": false,
  "githubUrl": "https://github.com/bredmond1019/PyAgent",
  "demoUrl": null,
  "techStack": [
    {
      "category": "Framework Principal",
      "items": ["Python 3.11+", "AsyncIO", "Pydantic", "BaseAgent v2.0.0", "Type Hints"]
    },
    {
      "category": "Infraestrutura de Agentes",
      "items": ["Protocolo MCP (JSON-RPC 2.0)", "Motor de Fluxo de Trabalho DAG", "Event Sourcing", "Padrão CQRS", "Circuit Breakers"]
    },
    {
      "category": "Ecossistema de Integração",
      "items": ["GitHub API", "Slack SDK", "Google Drive", "Notion API", "HelpScout", "Shortcut"]
    },
    {
      "category": "Infraestrutura e Monitoramento",
      "items": ["PostgreSQL", "Redis", "Prometheus", "Grafana", "OpenTelemetry", "Docker"]
    }
  ],
  "features": [
    "Fundação BaseAgent v2.0.0 com gerenciamento abrangente de ciclo de vida e monitoramento de saúde",
    "Implementação completa do protocolo MCP para comunicação padronizada entre agentes",
    "Orquestração de fluxo de trabalho baseada em DAG com gerenciamento de estado e depuração visual",
    "40+ agentes pré-construídos em categorias de conteúdo, controle, integração e gerenciamento de projetos",
    "Arquitetura orientada a eventos com event sourcing respaldado por PostgreSQL e snapshots",
    "Gerenciamento centralizado de configuração com validação Pydantic e hot-reloading",
    "Monitoramento de nível de produção com métricas Prometheus e rastreamento distribuído",
    "Gerenciamento automático de recursos com limites, rastreamento e limpeza",
    "Tratamento abrangente de erros com IDs de correlação e lógica de retry",
    "Suporte multi-tenant com fluxos de eventos isolados e segregação de dados"
  ],
  "challenges": [
    "Projetar uma fundação de agente flexível mas padronizada que suporte diversos casos de uso",
    "Implementar comunicação confiável entre serviços em sistemas de agentes distribuídos",
    "Construir um motor de fluxo de trabalho que lide com execução complexa de DAG com recuperação de erros",
    "Criar uma arquitetura de plugin que mantenha segurança enquanto permite extensibilidade",
    "Garantir performance consistente entre diferentes tipos de agentes e padrões de carga de trabalho"
  ],
  "outcomes": [
    { "metric": "Categorias de Agentes", "value": "5 categorias principais" },
    { "metric": "Agentes Pré-construídos", "value": "40+ prontos para usar" },
    { "metric": "Cobertura de Testes", "value": "10% (melhorando)" },
    { "metric": "Performance", "value": "Overhead sub-ms" },
    { "metric": "Documentação", "value": "Guias abrangentes" }
  ],
  "educational": [
    "Demonstra melhores práticas para construir arquiteturas de agentes escaláveis",
    "Ensina padrões de design orientado a eventos para sistemas distribuídos",
    "Mostra como implementar protocolos de comunicação padronizados",
    "Ilustra orquestração de fluxo de trabalho com gerenciamento de estado",
    "Fornece exemplos de tratamento de erros e monitoramento prontos para produção",
    "Explica padrões de integração para serviços e APIs externas"
  ],
  "globalImpact": {
    "geographicReach": ["América do Norte", "Europa", "Ásia", "Austrália"],
    "usersWorldwide": 500,
    "socialImpact": "Democratiza o desenvolvimento de agentes IA fornecendo ferramentas de nível empresarial como open-source, permitindo que equipes menores construam sistemas de automação sofisticados",
    "environmentalImpact": "Gerenciamento otimizado de recursos e pooling de conexões reduzem overhead computacional, enquanto event sourcing minimiza processamento redundante",
    "accessibilityFeatures": ["Documentação abrangente", "Múltiplas opções de integração", "Curva de aprendizado gradual", "Abordagem orientada por exemplos"],
    "multilingualSupport": false,
    "economicImpact": "Reduz o tempo de desenvolvimento para sistemas de agentes em 70% através de componentes pré-construídos e padrões padronizados, acelerando time-to-market para soluções de IA",
    "knowledgeSharing": "Open-source com licença MIT, documentação extensa e comunidade ativa contribuindo para o ecossistema crescente de agentes"
  },
  "localization": {
    "supportedLanguages": ["Inglês"],
    "culturalAdaptations": ["Comportamentos de agentes configuráveis", "Padrões de integração flexíveis"],
    "timeZoneHandling": true,
    "currencySupport": [],
    "regionalCompliance": ["Arquitetura pronta para GDPR", "Capacidades de isolamento de dados"]
  },
  "codeSnippets": [
    {
      "title": "Implementação BaseAgent com Protocolo MCP",
      "language": "python",
      "code": "import asyncio\nfrom agent_lib.shared.base_agent import BaseAgent\nfrom agent_lib.core.agent_capability import AgentCapability\nfrom agent_lib.core.mcp_protocol import MCPMessage, MCPResponse, MCPMessageFactory\nfrom agent_lib.core.agent_health import HealthStatus\nfrom typing import Optional, Dict, Any\nimport uuid\n\nclass IntelligentAgent(BaseAgent):\n    \"\"\"Production-ready agent with lifecycle management and health monitoring\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        default_config = {\n            \"agent_id\": f\"intelligent-agent-{uuid.uuid4().hex[:8]}\",\n            \"name\": \"Intelligent Agent\",\n            \"description\": \"Advanced agent with full lifecycle support\",\n            \"version\": \"1.0.0\",\n            \"resource_limits\": {\n                \"max_memory_mb\": 512,\n                \"max_cpu_percent\": 80,\n                \"max_concurrent_tasks\": 10\n            }\n        }\n        \n        # Merge with provided config\n        if config:\n            default_config.update(config)\n            \n        super().__init__(\n            config=default_config,\n            capabilities=[\n                AgentCapability.MCP_PROTOCOL,\n                AgentCapability.HEALTH_MONITORING,\n                AgentCapability.EVENT_HANDLING\n            ]\n        )\n        \n        # Internal state management\n        self.active_tasks = {}\n        self.metrics = {\n            \"messages_processed\": 0,\n            \"errors_encountered\": 0,\n            \"average_response_time\": 0.0\n        }\n    \n    async def startup(self) -> bool:\n        \"\"\"Initialize agent resources and connections\"\"\"\n        try:\n            self.logger.info(f\"Starting {self.config['name']}...\")\n            \n            # Initialize connections\n            await self._initialize_connections()\n            \n            # Register with agent registry\n            await self._register_agent()\n            \n            # Start health monitoring\n            self._start_health_monitor()\n            \n            self.logger.info(f\"{self.config['name']} started successfully\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Startup failed: {e}\")\n            await self.handle_error(e)\n            return False\n    \n    async def shutdown(self) -> bool:\n        \"\"\"Gracefully shutdown agent and cleanup resources\"\"\"\n        try:\n            self.logger.info(f\"Shutting down {self.config['name']}...\")\n            \n            # Cancel active tasks\n            for task_id, task in self.active_tasks.items():\n                if not task.done():\n                    task.cancel()\n                    self.logger.info(f\"Cancelled task {task_id}\")\n            \n            # Wait for tasks to complete\n            if self.active_tasks:\n                await asyncio.gather(\n                    *self.active_tasks.values(),\n                    return_exceptions=True\n                )\n            \n            # Cleanup resources\n            await self._cleanup_resources()\n            \n            # Deregister from agent registry\n            await self._deregister_agent()\n            \n            self.logger.info(f\"{self.config['name']} shutdown complete\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Shutdown error: {e}\")\n            return False\n    \n    async def handle_mcp_message(self, message: MCPMessage) -> MCPResponse:\n        \"\"\"Process incoming MCP messages with full error handling\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        \n        try:\n            # Validate message\n            if not self._validate_message(message):\n                return MCPResponse(\n                    id=message.id,\n                    error={\n                        \"code\": -32600,\n                        \"message\": \"Invalid request\"\n                    }\n                )\n            \n            # Check resource limits\n            if len(self.active_tasks) >= self.config[\"resource_limits\"][\"max_concurrent_tasks\"]:\n                return MCPResponse(\n                    id=message.id,\n                    error={\n                        \"code\": -32000,\n                        \"message\": \"Resource limit exceeded\"\n                    }\n                )\n            \n            # Process message based on method\n            if message.method == \"process\":\n                result = await self._process_task(message.params)\n            elif message.method == \"status\":\n                result = await self._get_status()\n            elif message.method == \"configure\":\n                result = await self._update_configuration(message.params)\n            else:\n                return MCPResponse(\n                    id=message.id,\n                    error={\n                        \"code\": -32601,\n                        \"message\": f\"Method not found: {message.method}\"\n                    }\n                )\n            \n            # Update metrics\n            response_time = asyncio.get_event_loop().time() - start_time\n            self._update_metrics(response_time)\n            \n            return MCPResponse(\n                id=message.id,\n                result=result\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Error handling message: {e}\")\n            self.metrics[\"errors_encountered\"] += 1\n            \n            return MCPResponse(\n                id=message.id,\n                error={\n                    \"code\": -32603,\n                    \"message\": \"Internal error\",\n                    \"data\": str(e)\n                }\n            )\n    \n    async def _process_task(self, params: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process a task with resource tracking\"\"\"\n        task_id = str(uuid.uuid4())\n        \n        # Create task coroutine\n        task = asyncio.create_task(\n            self._execute_task(task_id, params)\n        )\n        \n        # Track active task\n        self.active_tasks[task_id] = task\n        \n        try:\n            # Execute with timeout\n            timeout = params.get(\"timeout\", 30)\n            result = await asyncio.wait_for(task, timeout=timeout)\n            \n            return {\n                \"success\": True,\n                \"task_id\": task_id,\n                \"result\": result\n            }\n            \n        except asyncio.TimeoutError:\n            task.cancel()\n            return {\n                \"success\": False,\n                \"task_id\": task_id,\n                \"error\": \"Task timeout\"\n            }\n            \n        finally:\n            # Cleanup\n            self.active_tasks.pop(task_id, None)\n    \n    async def _execute_task(self, task_id: str, params: Dict[str, Any]) -> Any:\n        \"\"\"Execute the actual task logic\"\"\"\n        self.logger.info(f\"Executing task {task_id}: {params}\")\n        \n        # Simulate task processing\n        await asyncio.sleep(1)\n        \n        # Return processed result\n        return {\n            \"processed_data\": params.get(\"data\", \"\"),\n            \"timestamp\": asyncio.get_event_loop().time(),\n            \"agent_id\": self.config[\"agent_id\"]\n        }\n    \n    def _update_metrics(self, response_time: float):\n        \"\"\"Update agent performance metrics\"\"\"\n        self.metrics[\"messages_processed\"] += 1\n        \n        # Calculate rolling average response time\n        total = self.metrics[\"messages_processed\"]\n        avg = self.metrics[\"average_response_time\"]\n        self.metrics[\"average_response_time\"] = (\n            (avg * (total - 1) + response_time) / total\n        )\n    \n    async def get_health(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive health status\"\"\"\n        health = await super().get_health()\n        \n        # Add custom metrics\n        health[\"metrics\"] = self.metrics\n        health[\"active_tasks\"] = len(self.active_tasks)\n        health[\"resource_usage\"] = await self._get_resource_usage()\n        \n        return health"
    },
    {
      "title": "Motor de Fluxo de Trabalho Baseado em DAG com Persistência de Estado",
      "language": "python",
      "code": "from agent_lib.core.workflow_engine import WorkflowDefinition, WorkflowStep, DefaultWorkflowEngine\nfrom agent_lib.core.workflow_debugger import WorkflowDebugger\nfrom agent_lib.core.workflow_state import WorkflowState, StepStatus\nfrom agent_lib.core.exceptions import WorkflowError\nfrom typing import List, Dict, Any, Optional\nimport asyncio\nimport json\nfrom datetime import datetime\n\nclass AdvancedWorkflowEngine(DefaultWorkflowEngine):\n    \"\"\"Production workflow engine with state persistence and debugging\"\"\"\n    \n    def __init__(self, event_store=None, redis_client=None):\n        super().__init__()\n        self.event_store = event_store\n        self.redis_client = redis_client\n        self.debugger = WorkflowDebugger()\n        self.active_workflows = {}\n        \n    async def execute_workflow(\n        self,\n        workflow: WorkflowDefinition,\n        context: Dict[str, Any] = None,\n        debug: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Execute workflow with full state management and debugging\"\"\"\n        \n        workflow_run_id = f\"{workflow.id}-{datetime.utcnow().isoformat()}\"\n        state = WorkflowState(workflow_run_id, workflow.id)\n        \n        try:\n            # Initialize workflow state\n            state.status = \"running\"\n            state.started_at = datetime.utcnow()\n            await self._persist_state(state)\n            \n            # Store in active workflows\n            self.active_workflows[workflow_run_id] = state\n            \n            # Validate workflow DAG\n            if not self._validate_dag(workflow):\n                raise WorkflowError(\"Invalid workflow DAG: contains cycles\")\n            \n            # Build execution plan\n            execution_plan = self._build_execution_plan(workflow)\n            \n            # Execute steps according to plan\n            results = {}\n            for step_batch in execution_plan:\n                batch_results = await self._execute_step_batch(\n                    workflow,\n                    step_batch,\n                    context or {},\n                    state,\n                    debug\n                )\n                results.update(batch_results)\n            \n            # Update final state\n            state.status = \"completed\"\n            state.completed_at = datetime.utcnow()\n            state.results = results\n            await self._persist_state(state)\n            \n            # Emit completion event\n            await self._emit_workflow_event({\n                \"type\": \"workflow_completed\",\n                \"workflow_id\": workflow.id,\n                \"run_id\": workflow_run_id,\n                \"duration\": (state.completed_at - state.started_at).total_seconds()\n            })\n            \n            return {\n                \"success\": True,\n                \"workflow_id\": workflow.id,\n                \"run_id\": workflow_run_id,\n                \"results\": results,\n                \"duration\": (state.completed_at - state.started_at).total_seconds()\n            }\n            \n        except Exception as e:\n            # Handle workflow failure\n            state.status = \"failed\"\n            state.error = str(e)\n            state.completed_at = datetime.utcnow()\n            await self._persist_state(state)\n            \n            # Emit failure event\n            await self._emit_workflow_event({\n                \"type\": \"workflow_failed\",\n                \"workflow_id\": workflow.id,\n                \"run_id\": workflow_run_id,\n                \"error\": str(e)\n            })\n            \n            raise WorkflowError(f\"Workflow execution failed: {e}\")\n            \n        finally:\n            # Cleanup\n            self.active_workflows.pop(workflow_run_id, None)\n    \n    def _build_execution_plan(self, workflow: WorkflowDefinition) -> List[List[str]]:\n        \"\"\"Build parallel execution plan from workflow DAG\"\"\"\n        plan = []\n        completed_steps = set()\n        remaining_steps = set(step.id for step in workflow.steps)\n        \n        while remaining_steps:\n            # Find steps that can be executed in parallel\n            executable_batch = []\n            \n            for step in workflow.steps:\n                if step.id in remaining_steps:\n                    # Check if all dependencies are satisfied\n                    deps_satisfied = all(\n                        dep in completed_steps \n                        for dep in (step.dependencies or [])\n                    )\n                    \n                    if deps_satisfied:\n                        executable_batch.append(step.id)\n            \n            if not executable_batch:\n                raise WorkflowError(\n                    \"Cannot resolve workflow dependencies\"\n                )\n            \n            plan.append(executable_batch)\n            completed_steps.update(executable_batch)\n            remaining_steps.difference_update(executable_batch)\n        \n        return plan\n    \n    async def _execute_step_batch(\n        self,\n        workflow: WorkflowDefinition,\n        step_ids: List[str],\n        context: Dict[str, Any],\n        state: WorkflowState,\n        debug: bool\n    ) -> Dict[str, Any]:\n        \"\"\"Execute a batch of steps in parallel\"\"\"\n        tasks = []\n        \n        for step_id in step_ids:\n            step = next(s for s in workflow.steps if s.id == step_id)\n            \n            # Check debug breakpoint\n            if debug and self.debugger.should_break(step_id, context):\n                await self._handle_breakpoint(step_id, context, state)\n            \n            # Create execution task\n            task = asyncio.create_task(\n                self._execute_single_step(step, context, state)\n            )\n            tasks.append((step_id, task))\n        \n        # Execute all tasks in parallel\n        results = {}\n        for step_id, task in tasks:\n            try:\n                result = await task\n                results[step_id] = result\n                \n                # Update step state\n                state.step_states[step_id] = StepStatus(\n                    status=\"completed\",\n                    result=result,\n                    completed_at=datetime.utcnow()\n                )\n                \n            except Exception as e:\n                # Handle step failure\n                state.step_states[step_id] = StepStatus(\n                    status=\"failed\",\n                    error=str(e),\n                    completed_at=datetime.utcnow()\n                )\n                \n                if not workflow.continue_on_error:\n                    raise\n        \n        # Persist updated state\n        await self._persist_state(state)\n        \n        return results\n    \n    async def _execute_single_step(\n        self,\n        step: WorkflowStep,\n        context: Dict[str, Any],\n        state: WorkflowState\n    ) -> Any:\n        \"\"\"Execute a single workflow step\"\"\"\n        # Record step start\n        state.step_states[step.id] = StepStatus(\n            status=\"running\",\n            started_at=datetime.utcnow()\n        )\n        \n        # Get agent for step\n        agent = await self._get_agent(step.agent_type)\n        \n        # Prepare step context\n        step_context = {\n            **context,\n            \"step_id\": step.id,\n            \"workflow_id\": state.workflow_id,\n            \"run_id\": state.run_id\n        }\n        \n        # Execute step via agent\n        message = MCPMessageFactory.create_request(\n            method=\"process\",\n            params={\n                \"context\": step_context,\n                \"config\": step.config or {}\n            }\n        )\n        \n        response = await agent.handle_mcp_message(message)\n        \n        if response.error:\n            raise WorkflowError(\n                f\"Step {step.id} failed: {response.error['message']}\"\n            )\n        \n        return response.result\n    \n    async def _persist_state(self, state: WorkflowState):\n        \"\"\"Persist workflow state to storage\"\"\"\n        if self.redis_client:\n            # Quick access cache\n            await self.redis_client.setex(\n                f\"workflow:state:{state.run_id}\",\n                3600,  # 1 hour TTL\n                json.dumps(state.to_dict())\n            )\n        \n        if self.event_store:\n            # Permanent event store\n            await self.event_store.append_event({\n                \"aggregate_id\": state.run_id,\n                \"event_type\": \"WorkflowStateUpdated\",\n                \"data\": state.to_dict(),\n                \"version\": state.version\n            })\n            \n            state.version += 1\n    \n    async def replay_workflow(\n        self,\n        run_id: str,\n        until_step: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Replay workflow from event store for debugging\"\"\"\n        if not self.event_store:\n            raise WorkflowError(\"Event store not configured\")\n        \n        # Retrieve all events for workflow\n        events = await self.event_store.get_events_by_aggregate_id(run_id)\n        \n        # Rebuild state from events\n        state = WorkflowState(run_id, \"\")\n        \n        for event in events:\n            if event[\"event_type\"] == \"WorkflowStateUpdated\":\n                state.from_dict(event[\"data\"])\n                \n                if until_step and until_step in state.step_states:\n                    break\n        \n        return state.to_dict()\n    \n    def _validate_dag(self, workflow: WorkflowDefinition) -> bool:\n        \"\"\"Validate workflow DAG for cycles\"\"\"\n        # Build adjacency list\n        graph = {step.id: step.dependencies or [] for step in workflow.steps}\n        \n        # DFS to detect cycles\n        visited = set()\n        rec_stack = set()\n        \n        def has_cycle(node):\n            visited.add(node)\n            rec_stack.add(node)\n            \n            for neighbor in graph.get(node, []):\n                if neighbor not in visited:\n                    if has_cycle(neighbor):\n                        return True\n                elif neighbor in rec_stack:\n                    return True\n            \n            rec_stack.remove(node)\n            return False\n        \n        for node in graph:\n            if node not in visited:\n                if has_cycle(node):\n                    return False\n        \n        return True"
    }
  ]
}