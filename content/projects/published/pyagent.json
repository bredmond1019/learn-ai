{
  "slug": "pyagent",
  "title": "PyAgent - Intelligent Agent Library",
  "description": "Comprehensive Python library for building production-ready AI agent systems with standardized patterns, MCP protocol support, and extensive integration ecosystem for enterprise deployments.",
  "longDescription": "PyAgent represents a foundational shift in how intelligent agent systems are built and deployed at scale. This comprehensive library provides enterprise-grade components for creating AI-powered agents with standardized lifecycle management, workflow orchestration, and extensive integration capabilities across major platforms and services.\n\nThe library demonstrates mastery of distributed systems architecture through its event-driven design, featuring a complete DAG-based workflow engine with state persistence, debugging capabilities, and cross-service event routing. The implementation of the Model Context Protocol (MCP) enables standardized agent communication, while the modular architecture supports everything from simple automation tasks to complex multi-agent systems coordinating across different domains.\n\nBeyond its technical sophistication, PyAgent serves as both a production tool and an educational resource, with comprehensive documentation, examples, and a growing ecosystem of pre-built agents. The library's impact extends across industries, enabling developers to rapidly prototype and deploy intelligent automation solutions while maintaining enterprise-grade reliability and observability.",
  "tags": ["Python", "AI Agents", "MCP Protocol", "Event Sourcing", "Workflow Engine", "Distributed Systems"],
  "featured": false,
  "icon": "Bot",
  "isPrivate": true,
  "githubUrl": "https://github.com/pyagent/pyagent",
  "demoUrl": null,
  "techStack": [
    {
      "category": "Core Framework",
      "items": ["Python 3.11+", "AsyncIO", "Pydantic", "BaseAgent v2.0.0", "Type Hints"]
    },
    {
      "category": "Agent Infrastructure",
      "items": ["MCP Protocol (JSON-RPC 2.0)", "DAG Workflow Engine", "Event Sourcing", "CQRS Pattern", "Circuit Breakers"]
    },
    {
      "category": "Integration Ecosystem",
      "items": ["GitHub API", "Slack SDK", "Google Drive", "Notion API", "HelpScout", "Shortcut"]
    },
    {
      "category": "Infrastructure & Monitoring",
      "items": ["PostgreSQL", "Redis", "Prometheus", "Grafana", "OpenTelemetry", "Docker"]
    }
  ],
  "features": [
    "BaseAgent v2.0.0 foundation with comprehensive lifecycle management and health monitoring",
    "Complete MCP protocol implementation for standardized agent communication",
    "DAG-based workflow orchestration with state management and visual debugging",
    "40+ pre-built agents across content, control, integration, and project management categories",
    "Event-driven architecture with PostgreSQL-backed event sourcing and snapshots",
    "Centralized configuration management with Pydantic validation and hot-reloading",
    "Production-grade monitoring with Prometheus metrics and distributed tracing",
    "Automatic resource management with limits, tracking, and cleanup",
    "Comprehensive error handling with correlation IDs and retry logic",
    "Multi-tenant support with isolated event streams and data segregation"
  ],
  "challenges": [
    "Designing a flexible yet standardized agent foundation that supports diverse use cases",
    "Implementing reliable cross-service communication in distributed agent systems",
    "Building a workflow engine that handles complex DAG execution with error recovery",
    "Creating a plugin architecture that maintains security while enabling extensibility",
    "Ensuring consistent performance across different agent types and workload patterns"
  ],
  "outcomes": [
    { "metric": "Agent Categories", "value": "5 major categories" },
    { "metric": "Pre-built Agents", "value": "40+ ready-to-use" },
    { "metric": "Test Coverage", "value": "10% (improving)" },
    { "metric": "Performance", "value": "Sub-ms overhead" },
    { "metric": "Documentation", "value": "Comprehensive guides" }
  ],
  "educational": [
    "Demonstrates best practices for building scalable agent architectures",
    "Teaches event-driven design patterns for distributed systems",
    "Shows how to implement standardized communication protocols",
    "Illustrates workflow orchestration with state management",
    "Provides examples of production-ready error handling and monitoring",
    "Explains integration patterns for external services and APIs"
  ],
  "globalImpact": {
    "geographicReach": ["North America", "Europe", "Asia", "Australia"],
    "usersWorldwide": 500,
    "socialImpact": "Democratizes AI agent development by providing enterprise-grade tools as open-source, enabling smaller teams to build sophisticated automation systems",
    "environmentalImpact": "Optimized resource management and connection pooling reduce computational overhead, while event sourcing minimizes redundant processing",
    "accessibilityFeatures": ["Comprehensive documentation", "Multiple integration options", "Gradual learning curve", "Example-driven approach"],
    "multilingualSupport": false,
    "economicImpact": "Reduces development time for agent systems by 70% through pre-built components and standardized patterns, accelerating time-to-market for AI solutions",
    "knowledgeSharing": "Open-source with MIT license, extensive documentation, and active community contributing to the growing agent ecosystem"
  },
  "localization": {
    "supportedLanguages": ["English"],
    "culturalAdaptations": ["Configurable agent behaviors", "Flexible integration patterns"],
    "timeZoneHandling": true,
    "currencySupport": [],
    "regionalCompliance": ["GDPR-ready architecture", "Data isolation capabilities"]
  },
  "codeSnippets": [
    {
      "title": "BaseAgent Implementation with MCP Protocol",
      "language": "python",
      "code": "import asyncio\nfrom agent_lib.shared.base_agent import BaseAgent\nfrom agent_lib.core.agent_capability import AgentCapability\nfrom agent_lib.core.mcp_protocol import MCPMessage, MCPResponse, MCPMessageFactory\nfrom agent_lib.core.agent_health import HealthStatus\nfrom typing import Optional, Dict, Any\nimport uuid\n\nclass IntelligentAgent(BaseAgent):\n    \"\"\"Production-ready agent with lifecycle management and health monitoring\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        default_config = {\n            \"agent_id\": f\"intelligent-agent-{uuid.uuid4().hex[:8]}\",\n            \"name\": \"Intelligent Agent\",\n            \"description\": \"Advanced agent with full lifecycle support\",\n            \"version\": \"1.0.0\",\n            \"resource_limits\": {\n                \"max_memory_mb\": 512,\n                \"max_cpu_percent\": 80,\n                \"max_concurrent_tasks\": 10\n            }\n        }\n        \n        # Merge with provided config\n        if config:\n            default_config.update(config)\n            \n        super().__init__(\n            config=default_config,\n            capabilities=[\n                AgentCapability.MCP_PROTOCOL,\n                AgentCapability.HEALTH_MONITORING,\n                AgentCapability.EVENT_HANDLING\n            ]\n        )\n        \n        # Internal state management\n        self.active_tasks = {}\n        self.metrics = {\n            \"messages_processed\": 0,\n            \"errors_encountered\": 0,\n            \"average_response_time\": 0.0\n        }\n    \n    async def startup(self) -> bool:\n        \"\"\"Initialize agent resources and connections\"\"\"\n        try:\n            self.logger.info(f\"Starting {self.config['name']}...\")\n            \n            # Initialize connections\n            await self._initialize_connections()\n            \n            # Register with agent registry\n            await self._register_agent()\n            \n            # Start health monitoring\n            self._start_health_monitor()\n            \n            self.logger.info(f\"{self.config['name']} started successfully\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Startup failed: {e}\")\n            await self.handle_error(e)\n            return False\n    \n    async def shutdown(self) -> bool:\n        \"\"\"Gracefully shutdown agent and cleanup resources\"\"\"\n        try:\n            self.logger.info(f\"Shutting down {self.config['name']}...\")\n            \n            # Cancel active tasks\n            for task_id, task in self.active_tasks.items():\n                if not task.done():\n                    task.cancel()\n                    self.logger.info(f\"Cancelled task {task_id}\")\n            \n            # Wait for tasks to complete\n            if self.active_tasks:\n                await asyncio.gather(\n                    *self.active_tasks.values(),\n                    return_exceptions=True\n                )\n            \n            # Cleanup resources\n            await self._cleanup_resources()\n            \n            # Deregister from agent registry\n            await self._deregister_agent()\n            \n            self.logger.info(f\"{self.config['name']} shutdown complete\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Shutdown error: {e}\")\n            return False\n    \n    async def handle_mcp_message(self, message: MCPMessage) -> MCPResponse:\n        \"\"\"Process incoming MCP messages with full error handling\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        \n        try:\n            # Validate message\n            if not self._validate_message(message):\n                return MCPResponse(\n                    id=message.id,\n                    error={\n                        \"code\": -32600,\n                        \"message\": \"Invalid request\"\n                    }\n                )\n            \n            # Check resource limits\n            if len(self.active_tasks) >= self.config[\"resource_limits\"][\"max_concurrent_tasks\"]:\n                return MCPResponse(\n                    id=message.id,\n                    error={\n                        \"code\": -32000,\n                        \"message\": \"Resource limit exceeded\"\n                    }\n                )\n            \n            # Process message based on method\n            if message.method == \"process\":\n                result = await self._process_task(message.params)\n            elif message.method == \"status\":\n                result = await self._get_status()\n            elif message.method == \"configure\":\n                result = await self._update_configuration(message.params)\n            else:\n                return MCPResponse(\n                    id=message.id,\n                    error={\n                        \"code\": -32601,\n                        \"message\": f\"Method not found: {message.method}\"\n                    }\n                )\n            \n            # Update metrics\n            response_time = asyncio.get_event_loop().time() - start_time\n            self._update_metrics(response_time)\n            \n            return MCPResponse(\n                id=message.id,\n                result=result\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Error handling message: {e}\")\n            self.metrics[\"errors_encountered\"] += 1\n            \n            return MCPResponse(\n                id=message.id,\n                error={\n                    \"code\": -32603,\n                    \"message\": \"Internal error\",\n                    \"data\": str(e)\n                }\n            )\n    \n    async def _process_task(self, params: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process a task with resource tracking\"\"\"\n        task_id = str(uuid.uuid4())\n        \n        # Create task coroutine\n        task = asyncio.create_task(\n            self._execute_task(task_id, params)\n        )\n        \n        # Track active task\n        self.active_tasks[task_id] = task\n        \n        try:\n            # Execute with timeout\n            timeout = params.get(\"timeout\", 30)\n            result = await asyncio.wait_for(task, timeout=timeout)\n            \n            return {\n                \"success\": True,\n                \"task_id\": task_id,\n                \"result\": result\n            }\n            \n        except asyncio.TimeoutError:\n            task.cancel()\n            return {\n                \"success\": False,\n                \"task_id\": task_id,\n                \"error\": \"Task timeout\"\n            }\n            \n        finally:\n            # Cleanup\n            self.active_tasks.pop(task_id, None)\n    \n    async def _execute_task(self, task_id: str, params: Dict[str, Any]) -> Any:\n        \"\"\"Execute the actual task logic\"\"\"\n        self.logger.info(f\"Executing task {task_id}: {params}\")\n        \n        # Simulate task processing\n        await asyncio.sleep(1)\n        \n        # Return processed result\n        return {\n            \"processed_data\": params.get(\"data\", \"\"),\n            \"timestamp\": asyncio.get_event_loop().time(),\n            \"agent_id\": self.config[\"agent_id\"]\n        }\n    \n    def _update_metrics(self, response_time: float):\n        \"\"\"Update agent performance metrics\"\"\"\n        self.metrics[\"messages_processed\"] += 1\n        \n        # Calculate rolling average response time\n        total = self.metrics[\"messages_processed\"]\n        avg = self.metrics[\"average_response_time\"]\n        self.metrics[\"average_response_time\"] = (\n            (avg * (total - 1) + response_time) / total\n        )\n    \n    async def get_health(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive health status\"\"\"\n        health = await super().get_health()\n        \n        # Add custom metrics\n        health[\"metrics\"] = self.metrics\n        health[\"active_tasks\"] = len(self.active_tasks)\n        health[\"resource_usage\"] = await self._get_resource_usage()\n        \n        return health"
    },
    {
      "title": "DAG-based Workflow Engine with State Persistence",
      "language": "python",
      "code": "from agent_lib.core.workflow_engine import WorkflowDefinition, WorkflowStep, DefaultWorkflowEngine\nfrom agent_lib.core.workflow_debugger import WorkflowDebugger\nfrom agent_lib.core.workflow_state import WorkflowState, StepStatus\nfrom agent_lib.core.exceptions import WorkflowError\nfrom typing import List, Dict, Any, Optional\nimport asyncio\nimport json\nfrom datetime import datetime\n\nclass AdvancedWorkflowEngine(DefaultWorkflowEngine):\n    \"\"\"Production workflow engine with state persistence and debugging\"\"\"\n    \n    def __init__(self, event_store=None, redis_client=None):\n        super().__init__()\n        self.event_store = event_store\n        self.redis_client = redis_client\n        self.debugger = WorkflowDebugger()\n        self.active_workflows = {}\n        \n    async def execute_workflow(\n        self,\n        workflow: WorkflowDefinition,\n        context: Dict[str, Any] = None,\n        debug: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Execute workflow with full state management and debugging\"\"\"\n        \n        workflow_run_id = f\"{workflow.id}-{datetime.utcnow().isoformat()}\"\n        state = WorkflowState(workflow_run_id, workflow.id)\n        \n        try:\n            # Initialize workflow state\n            state.status = \"running\"\n            state.started_at = datetime.utcnow()\n            await self._persist_state(state)\n            \n            # Store in active workflows\n            self.active_workflows[workflow_run_id] = state\n            \n            # Validate workflow DAG\n            if not self._validate_dag(workflow):\n                raise WorkflowError(\"Invalid workflow DAG: contains cycles\")\n            \n            # Build execution plan\n            execution_plan = self._build_execution_plan(workflow)\n            \n            # Execute steps according to plan\n            results = {}\n            for step_batch in execution_plan:\n                batch_results = await self._execute_step_batch(\n                    workflow,\n                    step_batch,\n                    context or {},\n                    state,\n                    debug\n                )\n                results.update(batch_results)\n            \n            # Update final state\n            state.status = \"completed\"\n            state.completed_at = datetime.utcnow()\n            state.results = results\n            await self._persist_state(state)\n            \n            # Emit completion event\n            await self._emit_workflow_event({\n                \"type\": \"workflow_completed\",\n                \"workflow_id\": workflow.id,\n                \"run_id\": workflow_run_id,\n                \"duration\": (state.completed_at - state.started_at).total_seconds()\n            })\n            \n            return {\n                \"success\": True,\n                \"workflow_id\": workflow.id,\n                \"run_id\": workflow_run_id,\n                \"results\": results,\n                \"duration\": (state.completed_at - state.started_at).total_seconds()\n            }\n            \n        except Exception as e:\n            # Handle workflow failure\n            state.status = \"failed\"\n            state.error = str(e)\n            state.completed_at = datetime.utcnow()\n            await self._persist_state(state)\n            \n            # Emit failure event\n            await self._emit_workflow_event({\n                \"type\": \"workflow_failed\",\n                \"workflow_id\": workflow.id,\n                \"run_id\": workflow_run_id,\n                \"error\": str(e)\n            })\n            \n            raise WorkflowError(f\"Workflow execution failed: {e}\")\n            \n        finally:\n            # Cleanup\n            self.active_workflows.pop(workflow_run_id, None)\n    \n    def _build_execution_plan(self, workflow: WorkflowDefinition) -> List[List[str]]:\n        \"\"\"Build parallel execution plan from workflow DAG\"\"\"\n        plan = []\n        completed_steps = set()\n        remaining_steps = set(step.id for step in workflow.steps)\n        \n        while remaining_steps:\n            # Find steps that can be executed in parallel\n            executable_batch = []\n            \n            for step in workflow.steps:\n                if step.id in remaining_steps:\n                    # Check if all dependencies are satisfied\n                    deps_satisfied = all(\n                        dep in completed_steps \n                        for dep in (step.dependencies or [])\n                    )\n                    \n                    if deps_satisfied:\n                        executable_batch.append(step.id)\n            \n            if not executable_batch:\n                raise WorkflowError(\n                    \"Cannot resolve workflow dependencies\"\n                )\n            \n            plan.append(executable_batch)\n            completed_steps.update(executable_batch)\n            remaining_steps.difference_update(executable_batch)\n        \n        return plan\n    \n    async def _execute_step_batch(\n        self,\n        workflow: WorkflowDefinition,\n        step_ids: List[str],\n        context: Dict[str, Any],\n        state: WorkflowState,\n        debug: bool\n    ) -> Dict[str, Any]:\n        \"\"\"Execute a batch of steps in parallel\"\"\"\n        tasks = []\n        \n        for step_id in step_ids:\n            step = next(s for s in workflow.steps if s.id == step_id)\n            \n            # Check debug breakpoint\n            if debug and self.debugger.should_break(step_id, context):\n                await self._handle_breakpoint(step_id, context, state)\n            \n            # Create execution task\n            task = asyncio.create_task(\n                self._execute_single_step(step, context, state)\n            )\n            tasks.append((step_id, task))\n        \n        # Execute all tasks in parallel\n        results = {}\n        for step_id, task in tasks:\n            try:\n                result = await task\n                results[step_id] = result\n                \n                # Update step state\n                state.step_states[step_id] = StepStatus(\n                    status=\"completed\",\n                    result=result,\n                    completed_at=datetime.utcnow()\n                )\n                \n            except Exception as e:\n                # Handle step failure\n                state.step_states[step_id] = StepStatus(\n                    status=\"failed\",\n                    error=str(e),\n                    completed_at=datetime.utcnow()\n                )\n                \n                if not workflow.continue_on_error:\n                    raise\n        \n        # Persist updated state\n        await self._persist_state(state)\n        \n        return results\n    \n    async def _execute_single_step(\n        self,\n        step: WorkflowStep,\n        context: Dict[str, Any],\n        state: WorkflowState\n    ) -> Any:\n        \"\"\"Execute a single workflow step\"\"\"\n        # Record step start\n        state.step_states[step.id] = StepStatus(\n            status=\"running\",\n            started_at=datetime.utcnow()\n        )\n        \n        # Get agent for step\n        agent = await self._get_agent(step.agent_type)\n        \n        # Prepare step context\n        step_context = {\n            **context,\n            \"step_id\": step.id,\n            \"workflow_id\": state.workflow_id,\n            \"run_id\": state.run_id\n        }\n        \n        # Execute step via agent\n        message = MCPMessageFactory.create_request(\n            method=\"process\",\n            params={\n                \"context\": step_context,\n                \"config\": step.config or {}\n            }\n        )\n        \n        response = await agent.handle_mcp_message(message)\n        \n        if response.error:\n            raise WorkflowError(\n                f\"Step {step.id} failed: {response.error['message']}\"\n            )\n        \n        return response.result\n    \n    async def _persist_state(self, state: WorkflowState):\n        \"\"\"Persist workflow state to storage\"\"\"\n        if self.redis_client:\n            # Quick access cache\n            await self.redis_client.setex(\n                f\"workflow:state:{state.run_id}\",\n                3600,  # 1 hour TTL\n                json.dumps(state.to_dict())\n            )\n        \n        if self.event_store:\n            # Permanent event store\n            await self.event_store.append_event({\n                \"aggregate_id\": state.run_id,\n                \"event_type\": \"WorkflowStateUpdated\",\n                \"data\": state.to_dict(),\n                \"version\": state.version\n            })\n            \n            state.version += 1\n    \n    async def replay_workflow(\n        self,\n        run_id: str,\n        until_step: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Replay workflow from event store for debugging\"\"\"\n        if not self.event_store:\n            raise WorkflowError(\"Event store not configured\")\n        \n        # Retrieve all events for workflow\n        events = await self.event_store.get_events_by_aggregate_id(run_id)\n        \n        # Rebuild state from events\n        state = WorkflowState(run_id, \"\")\n        \n        for event in events:\n            if event[\"event_type\"] == \"WorkflowStateUpdated\":\n                state.from_dict(event[\"data\"])\n                \n                if until_step and until_step in state.step_states:\n                    break\n        \n        return state.to_dict()\n    \n    def _validate_dag(self, workflow: WorkflowDefinition) -> bool:\n        \"\"\"Validate workflow DAG for cycles\"\"\"\n        # Build adjacency list\n        graph = {step.id: step.dependencies or [] for step in workflow.steps}\n        \n        # DFS to detect cycles\n        visited = set()\n        rec_stack = set()\n        \n        def has_cycle(node):\n            visited.add(node)\n            rec_stack.add(node)\n            \n            for neighbor in graph.get(node, []):\n                if neighbor not in visited:\n                    if has_cycle(neighbor):\n                        return True\n                elif neighbor in rec_stack:\n                    return True\n            \n            rec_stack.remove(node)\n            return False\n        \n        for node in graph:\n            if node not in visited:\n                if has_cycle(node):\n                    return False\n        \n        return True"
    }
  ]
}